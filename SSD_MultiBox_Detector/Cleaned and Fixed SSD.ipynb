{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5747a86d90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ALL IMPORTS FOR A NEW NOTEBOOK\n",
    "__SEED = 0\n",
    "__N_FOLDS = 5\n",
    "__NROWS = None\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os, sys, random, math\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 500)\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 500)\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import itertools as it\n",
    "import scipy\n",
    "import glob\n",
    "import matplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.optim import Optimizer\n",
    "import torchvision.transforms.transforms as txf\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn import model_selection as ms\n",
    "\n",
    "import ml_utils as mu\n",
    "import time\n",
    "import time, datetime, pickle\n",
    "\n",
    "import torchvision.transforms.functional as FT\n",
    "# fold1 = ms.StratifiedKFold(n_splits=__N_FOLDS, shuffle=True, random_state=__SEED)\n",
    "# fold2 = ms.StratifiedKFold(n_splits=__N_FOLDS, shuffle=True, random_state=__SEED+3)\n",
    "# fold3 = ms.StratifiedKFold(n_splits=__N_FOLDS, shuffle=True, random_state=__SEED+5)\n",
    "font = {'size'   : 14}\n",
    "matplotlib.rc('font', **font)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(947)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT PROCESSING and DATASET READING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_labels = (\n",
    "    \"background\",\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
    "    'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "label_map = {k:v for v,k in enumerate(voc_labels)}\n",
    "rev_label_map = {v:k for k,v in label_map.items()}\n",
    "distinct_colors = [\"#FFFFFF\",\n",
    "                   '#e6194B', '#3cb44b', '#ffe119', '#4363d8', '#f58231', \n",
    "                   '#911eb4', '#42d4f4', '#f032e6', '#bfef45', '#fabebe',\n",
    "                   '#469990', '#e6beff', '#9A6324', '#fffac8', '#800000',\n",
    "                   '#aaffc3', '#808000', '#ffd8b1', '#000075', '#a9a9a9']\n",
    "label_color_map = {k:distinct_colors[i] for i,k in enumerate(voc_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "import json\n",
    "\n",
    "### FOR PARSING THE XML ANNOTATIONS OF THE DATASETS ###\n",
    "def parse_annotations(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    \n",
    "    root = tree.getroot()\n",
    "    \n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    difficulties = list()\n",
    "    \n",
    "    for object in root.iter('object'):\n",
    "        difficult = int(object.find(\"difficult\").text==\"1\")\n",
    "        label = object.find(\"name\").text.lower().strip()\n",
    "        \n",
    "        if label not in label_map:\n",
    "            continue\n",
    "        \n",
    "        bbox = object.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) - 1\n",
    "        xmax = int(bbox.find('xmax').text) - 1\n",
    "        ymin = int(bbox.find('ymin').text) - 1\n",
    "        ymax = int(bbox.find('ymax').text) - 1\n",
    "        \n",
    "        boxes.append([xmin,ymin,xmax,ymax])\n",
    "        labels.append(label_map[label])\n",
    "        difficulties.append(difficult)\n",
    "    return {\"boxes\":boxes, \"labels\":labels,\"difficulties\":difficulties}\n",
    "\n",
    "\n",
    "### READ XML DESCRIPTIONS AND SAVE AS JSON FOR FASTER ACCESS ###\n",
    "def create_data_lists(v07, v12, output_folder):\n",
    "    v07_path = os.path.abspath(v07)\n",
    "    v12_path = os.path.abspath(v12)\n",
    "    \n",
    "    ## PARSE TRAIN ##\n",
    "    train_images = list()\n",
    "    train_objects = list()\n",
    "    n_objects = 0\n",
    "    \n",
    "    for path in [v07_path, v12_path]:\n",
    "        with open(os.path.join(path,\"ImageSets/Main/trainval.txt\")) as f:\n",
    "            ids = f.read().splitlines()\n",
    "            \n",
    "            for id in ids:\n",
    "                objects = parse_annotations(os.path.join(path,\"Annotations\",id+\".xml\"))\n",
    "                \n",
    "                if len(objects[\"boxes\"])==0:\n",
    "                    continue\n",
    "                n_objects+=len(objects[\"boxes\"])\n",
    "                \n",
    "                train_objects.append(objects)\n",
    "                train_images.append(os.path.join(path, \"JPEGImages\", id+'.jpg'))\n",
    "                \n",
    "        \n",
    "    print(\"TOTAL TRAIN IMAGES: {} == TOTAL TRAIN OBJECTS {}\".format(len(train_images), len(train_objects)))\n",
    "    print(\"TOTAL OBJECTS IN ALL TRAIN IMAGES: {}\\n\".format(n_objects))\n",
    "    \n",
    "    with open(os.path.join(output_folder, \"TRAIN_images.json\"), 'w') as j:\n",
    "        json.dump(train_images, j)\n",
    "    with open(os.path.join(output_folder, \"TRAIN_objects.json\"), 'w') as j:\n",
    "        json.dump(train_objects, j)\n",
    "    with open(os.path.join(output_folder, \"label_map.json\"), 'w') as j:\n",
    "        json.dump(label_map, j)\n",
    "    \n",
    "    \n",
    "    ## PARSE TEST ##\n",
    "    \n",
    "    test_images = list()\n",
    "    test_objects = list()\n",
    "    n_objects = 0\n",
    "    \n",
    "    with open(os.path.join(v07_path, \"ImageSets/Main/test.txt\")) as f:\n",
    "        ids = f.read().splitlines()\n",
    "        \n",
    "        for id in ids:\n",
    "            objects = parse_annotations(os.path.join(v07_path, \"Annotations\", id+\".xml\"))\n",
    "            \n",
    "            if (len(objects[\"boxes\"]))==0:\n",
    "                continue\n",
    "            test_objects.append(objects)\n",
    "            test_images.append(os.path.join(v07_path,\"JPEGImages\",id+'.jpg'))\n",
    "            n_objects+=len(objects['boxes'])\n",
    "        \n",
    "    print(\"TOTAL TEST IMAGES: {} == TOTAL TEST OBJECTS {}\".format(len(test_images), len(test_objects)))\n",
    "    print(\"TOTAL OBJECTS IN ALL TEST IMAGES: {}\\n\".format(n_objects))\n",
    "    \n",
    "    with open(os.path.join(output_folder, \"TEST_images.json\"), 'w') as j:\n",
    "        json.dump(test_images, j)\n",
    "    with open(os.path.join(output_folder, \"TEST_objects.json\"), 'w') as j:\n",
    "        json.dump(test_objects, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL TRAIN IMAGES: 16551 == TOTAL TRAIN OBJECTS 16551\n",
      "TOTAL OBJECTS IN ALL TRAIN IMAGES: 47223\n",
      "\n",
      "TOTAL TEST IMAGES: 4952 == TOTAL TEST OBJECTS 4952\n",
      "TOTAL OBJECTS IN ALL TEST IMAGES: 14976\n",
      "\n",
      "CPU times: user 2.9 s, sys: 622 ms, total: 3.52 s\n",
      "Wall time: 6.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "create_data_lists(v07='./train/VOC2007/',\n",
    "                      v12='./train/VOC2012',\n",
    "                      output_folder='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as FT\n",
    "def transform(image, boxes, labels, difficulties, split, resize_dims=(300,300)):\n",
    "    assert split in {\"TRAIN\",\"TEST\", \"VALID\"}\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    new_image = image\n",
    "    new_boxes = boxes \n",
    "    new_labels = labels\n",
    "    new_difficulties = difficulties\n",
    "    \n",
    "    if split == \"TRAIN\":\n",
    "        new_image = mu.photometric_distort(new_image)\n",
    "        new_image = FT.to_tensor(new_image)\n",
    "        \n",
    "        if random.random()<0.5:\n",
    "            new_image, new_boxes = mu.box_expand(new_image, boxes, filler=mean)\n",
    "        \n",
    "        new_image, new_boxes, new_labels, new_difficulties = mu.box_random_crop(new_image, new_boxes, new_labels, new_difficulties)\n",
    "        \n",
    "        new_image = FT.to_pil_image(new_image)\n",
    "        \n",
    "        if random.random()<0.5:\n",
    "            new_image, new_boxes = mu.box_flip(new_image, new_boxes)\n",
    "    \n",
    "    new_image, new_boxes = mu.box_resize(new_image, new_boxes, dims=resize_dims)\n",
    "    new_image = FT.to_tensor(new_image)\n",
    "    new_image = FT.normalize(new_image, mean, std)\n",
    "    return new_image, new_boxes, new_labels, new_difficulties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PascalVOCDataset(Dataset):\n",
    "    def __init__(self, data_folder, split, keep_diffcult=False):\n",
    "        self.split = split.upper()\n",
    "        assert self.split in {\"TRAIN\", \"TEST\", \"VALID\"}\n",
    "        \n",
    "        self.data_folder = data_folder\n",
    "        self.keep_difficult = keep_diffcult\n",
    "        \n",
    "        with open(os.path.join(data_folder, self.split+\"_images.json\"),'r') as j:\n",
    "            self.images = json.load(j)\n",
    "        with open(os.path.join(data_folder, self.split+\"_objects.json\"), 'r') as j:\n",
    "            self.objects = json.load(j)\n",
    "        assert len(self.images) == len(self.objects)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = self.images[i] # just the path\n",
    "        objects = self.objects[i]\n",
    "        \n",
    "        return (image, objects)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Each image may have different number of objects and so,\n",
    "        we need to provide a function to combine these tensors of different sizes.\n",
    "        \"\"\"\n",
    "        \n",
    "        def read_and_process_image(image_path):\n",
    "            image = Image.open(image_path, mode='r')\n",
    "            image = image.convert(\"RGB\")\n",
    "            return image\n",
    "        def extract_labels_and_objects(obj_map):\n",
    "            boxes = torch.FloatTensor(obj_map[\"boxes\"]) # n_obs X 4\n",
    "            labels = torch.LongTensor(obj_map[\"labels\"]) # n_obs\n",
    "            difficulties = (torch.ByteTensor(obj_map[\"difficulties\"])==1) # n_obs\n",
    "            if not self.keep_difficult:\n",
    "                boxes = boxes[~difficulties]\n",
    "                labels = labels[~difficulties]\n",
    "                difficulties = difficulties[~difficulties]\n",
    "            return boxes, labels, difficulties\n",
    "        \n",
    "        \n",
    "        batched_images = list()\n",
    "        batched_boxes = list()\n",
    "        batched_labels = list()\n",
    "        batched_difficulties = list()\n",
    "        for b in batch:\n",
    "            image = read_and_process_image(b[0])\n",
    "            boxes, labels, difficulties = extract_labels_and_objects(b[1])\n",
    "            image, boxes, labels, difficulties = transform(image, boxes, labels, difficulties, split=self.split) \n",
    "            \n",
    "            batched_images.append(image)\n",
    "            batched_boxes.append(boxes)\n",
    "            batched_labels.append(labels)\n",
    "            batched_difficulties.append(difficulties)\n",
    "        batched_images = torch.stack(batched_images, dim=0)\n",
    "        \n",
    "        return batched_images, batched_boxes, batched_labels, batched_difficulties # tensor (N, 3, 300, 300) and 3 lists of N tensors each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL and LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "class VGGBase(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGGBase, self).__init__()\n",
    "        \n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv5_1 = nn.Conv2d(512,512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512,512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512,512, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "        \n",
    "        self.load_pretrained_layers()\n",
    "    \n",
    "    def forward(self, image):\n",
    "        out = F.relu(self.conv1_1(image))\n",
    "        out = F.relu(self.conv1_2(out))\n",
    "        out = self.pool1(out)\n",
    "        \n",
    "        out = F.relu(self.conv2_1(out))\n",
    "        out = F.relu(self.conv2_2(out))\n",
    "        out = self.pool2(out)\n",
    "        \n",
    "        out = F.relu(self.conv3_1(out))\n",
    "        out = F.relu(self.conv3_2(out))\n",
    "        out = F.relu(self.conv3_3(out))\n",
    "        out = self.pool3(out)\n",
    "        \n",
    "        out = F.relu(self.conv4_1(out))\n",
    "        out = F.relu(self.conv4_2(out))\n",
    "        out = F.relu(self.conv4_3(out))\n",
    "        \n",
    "        conv_4_3_feats = out\n",
    "        \n",
    "        out = self.pool4(out)\n",
    "        \n",
    "        out = F.relu(self.conv5_1(out))\n",
    "        out = F.relu(self.conv5_2(out))\n",
    "        out = F.relu(self.conv5_3(out))\n",
    "        out = self.pool5(out)\n",
    "        \n",
    "        out = F.relu(self.conv6(out))\n",
    "        out = F.relu(self.conv7(out))\n",
    "        \n",
    "        conv7_feats = out\n",
    "        \n",
    "        return conv_4_3_feats, conv7_feats\n",
    "    \n",
    "    def load_pretrained_layers(self):\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "        \n",
    "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "        \n",
    "        ## LOADING FIRST UNCHANGED LAYERS' WEIGHTS\n",
    "        for i, param in enumerate(param_names[:-4]):\n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "        \n",
    "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)\n",
    "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']\n",
    "        state_dict['conv6.weight'] = mu.decimate(conv_fc6_weight, m=[4,None, 3, 3])\n",
    "        state_dict['conv6.bias'] = mu.decimate(conv_fc6_bias, m=[4])\n",
    "        \n",
    "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096,4096,1,1)\n",
    "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']\n",
    "        state_dict['conv7.weight'] = mu.decimate(conv_fc7_weight, m=[4,4,None,None])\n",
    "        state_dict['conv7.bias'] = mu.decimate(conv_fc7_bias, m=[4])\n",
    "        \n",
    "        self.load_state_dict(state_dict)\n",
    "        \n",
    "        print(\"BASE MODEL LOAD....COMPLETE\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryConvolutions(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxiliaryConvolutions, self).__init__()\n",
    "        \n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2)\n",
    "        \n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2)\n",
    "        \n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
    "        \n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
    "        \n",
    "        \n",
    "        self.init_conv2d()\n",
    "    \n",
    "    def init_conv2d(self):\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.0)\n",
    "    \n",
    "    def forward(self, conv7_feats):\n",
    "        out = F.relu(self.conv8_1(conv7_feats))\n",
    "        out = F.relu(self.conv8_2(out))\n",
    "        conv8_2_feats = out\n",
    "        \n",
    "        out = F.relu(self.conv9_1(out))\n",
    "        out = F.relu(self.conv9_2(out))\n",
    "        conv9_2_feats = out\n",
    "        \n",
    "        out = F.relu(self.conv10_1(out))\n",
    "        out = F.relu(self.conv10_2(out))\n",
    "        conv10_2_feats = out\n",
    "        \n",
    "        out = F.relu(self.conv11_1(out))\n",
    "        out = F.relu(self.conv11_2(out))\n",
    "        conv11_2_feats = out\n",
    "        \n",
    "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionConvolutions(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(PredictionConvolutions, self).__init__()\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        n_boxes = {\n",
    "            'conv4_3':4,\n",
    "            'conv7':6,\n",
    "            'conv8_2':6,\n",
    "            'conv9_2':6,\n",
    "            'conv10_2':4,\n",
    "            'conv11_2':4\n",
    "        }\n",
    "        \n",
    "        \n",
    "        self.loc_conv4_3 = nn.Conv2d(512, 4*n_boxes['conv4_3'], kernel_size=3, padding=1)\n",
    "        self.loc_conv7 = nn.Conv2d(1024, 4*n_boxes['conv7'], kernel_size=3, padding=1)\n",
    "        self.loc_conv8_2 = nn.Conv2d(512, 4*n_boxes['conv8_2'], kernel_size=3, padding=1)\n",
    "        self.loc_conv9_2 = nn.Conv2d(256, 4*n_boxes['conv9_2'], kernel_size=3, padding=1)\n",
    "        self.loc_conv10_2 = nn.Conv2d(256, 4*n_boxes['conv10_2'], kernel_size=3, padding=1)\n",
    "        self.loc_conv11_2 = nn.Conv2d(256, 4*n_boxes['conv11_2'], kernel_size=3, padding=1)\n",
    "        \n",
    "        \n",
    "        self.cl_conv4_3 = nn.Conv2d(512, self.n_classes*n_boxes['conv4_3'], kernel_size=3, padding=1)\n",
    "        self.cl_conv7 = nn.Conv2d(1024, self.n_classes*n_boxes['conv7'], kernel_size=3, padding=1)\n",
    "        self.cl_conv8_2 = nn.Conv2d(512, self.n_classes*n_boxes['conv8_2'], kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2 = nn.Conv2d(256, self.n_classes*n_boxes['conv9_2'], kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256, self.n_classes*n_boxes['conv10_2'], kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256, self.n_classes*n_boxes['conv11_2'], kernel_size=3, padding=1)\n",
    "        \n",
    "        self.init_conv2d()\n",
    "    \n",
    "    def init_conv2d(self):\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.0)\n",
    "                \n",
    "    \n",
    "    \n",
    "    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n",
    "        \n",
    "        batch_size = conv4_3_feats.size(0)\n",
    "        \n",
    "        l_conv4_3 = self.loc_conv4_3(conv4_3_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, 4)\n",
    "        \n",
    "        \n",
    "        l_conv7 = self.loc_conv7(conv7_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, 4)\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, 4)\n",
    "        l_conv9_2 = self.loc_conv9_2(conv9_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, 4)\n",
    "        l_conv10_2 = self.loc_conv10_2(conv10_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, 4)\n",
    "        l_conv11_2 = self.loc_conv11_2(conv11_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, 4)\n",
    "        \n",
    "        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)\n",
    "        \n",
    "        c_conv4_3 = self.cl_conv4_3(conv4_3_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv7 = self.cl_conv7(conv7_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        \n",
    "        class_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2], dim=1)\n",
    "        \n",
    "        return locs, class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "class SSD300(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SSD300, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.base = VGGBase()\n",
    "        self.aux_convs = AuxiliaryConvolutions()\n",
    "        self.pred_convs = PredictionConvolutions(n_classes)\n",
    "        \n",
    "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))\n",
    "        nn.init.constant_(self.rescale_factors, 20)\n",
    "        \n",
    "        self.priors_cxcy = self.create_prior_boxes()\n",
    "    \n",
    "    def create_prior_boxes(self):\n",
    "        fmap_dims = {\n",
    "            \"conv4_3\":38,\n",
    "            \"conv7\":19,\n",
    "            \"conv8_2\":10,\n",
    "            \"conv9_2\":5,\n",
    "            \"conv10_2\":3,\n",
    "            \"conv11_2\":1\n",
    "        }\n",
    "        obj_scales = {\n",
    "            \"conv4_3\":0.1,\n",
    "            \"conv7\":0.2,\n",
    "            \"conv8_2\":0.375,\n",
    "            \"conv9_2\":0.55,\n",
    "            \"conv10_2\":0.725,\n",
    "            \"conv11_2\":0.9\n",
    "        }\n",
    "        aspect_ratios = {\n",
    "            \"conv4_3\": [1.0, 2.0, 0.5],\n",
    "            \"conv7\":   [1.0, 2.0, 0.5, 3.0, 0.333],\n",
    "            \"conv8_2\": [1.0, 2.0, 0.5, 3.0, 0.333],\n",
    "            \"conv9_2\": [1.0, 2.0, 0.5, 3.0, 0.333],\n",
    "            \"conv10_2\":[1.0, 2.0, 0.5],\n",
    "            \"conv11_2\":[1.0, 2.0, 0.5]\n",
    "        }\n",
    "        \n",
    "        fmaps = list(fmap_dims.keys())\n",
    "        \n",
    "        prior_boxes = []\n",
    "        \n",
    "        for k, fmap in enumerate(fmaps):\n",
    "            for i in range(fmap_dims[fmap]):\n",
    "                for j in range(fmap_dims[fmap]):\n",
    "                    cx = (j+0.5)/fmap_dims[fmap]\n",
    "                    cy = (i+0.5)/fmap_dims[fmap]\n",
    "                    \n",
    "                    for ratio in aspect_ratios[fmap]:\n",
    "                        prior_boxes.append(\n",
    "                            [cx, cy, obj_scales[fmap]*sqrt(ratio), obj_scales[fmap]/sqrt(ratio)])\n",
    "                        \n",
    "                        if ratio==1.0:\n",
    "                            try:\n",
    "                                additional_scale = sqrt(obj_scales[fmap]*obj_scales[fmaps[k+1]])\n",
    "                            except IndexError:\n",
    "                                additional_scale = 1.0\n",
    "                            \n",
    "                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
    "        \n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)\n",
    "        prior_boxes.clamp_(0, 1)\n",
    "        \n",
    "        return prior_boxes\n",
    "    \n",
    "    def forward(self, image):\n",
    "        conv4_3_feats, conv7_feats = self.base(image)\n",
    "        \n",
    "        norm = conv4_3_feats.norm(dim=1, keepdim=True)+1e-16\n",
    "        conv4_3_feats = conv4_3_feats/norm\n",
    "        \n",
    "        conv4_3_feats = conv4_3_feats*self.rescale_factors\n",
    "        \n",
    "        \n",
    "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = self.aux_convs(conv7_feats)\n",
    "        \n",
    "        locs, class_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, \n",
    "                                             conv9_2_feats, conv10_2_feats, conv11_2_feats)\n",
    "        \n",
    "        return locs, class_scores\n",
    "    \n",
    "    \n",
    "    def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors = self.priors_cxcy.size(0)\n",
    "        \n",
    "        predicted_scores = F.softmax(predicted_scores, dim=-1)\n",
    "        \n",
    "        all_images_boxes = list()\n",
    "        all_images_labels = list()\n",
    "        all_images_scores = list()\n",
    "        \n",
    "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
    "        \n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            decoded_locs = mu.cxcy_to_xy(mu.gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))\n",
    "            \n",
    "            image_boxes = list()\n",
    "            image_labels = list()\n",
    "            image_scores = list()\n",
    "            \n",
    "#             max_scores, best_label = predicted_scores[i].max(dim=1)\n",
    "            \n",
    "            for c in range(1, self.n_classes):\n",
    "                class_scores  = predicted_scores[i][:, c] # 8732\n",
    "                scores_above_min_score = class_scores > min_score # 8732, True or False\n",
    "                n_above_min_score = scores_above_min_score.sum().item() # 1\n",
    "                \n",
    "                if n_above_min_score == 0:\n",
    "                    continue\n",
    "                \n",
    "                # valid scores\n",
    "                class_scores = class_scores[scores_above_min_score] # N_QLF\n",
    "                # boxes with valid scores\n",
    "                class_decoded_locs = decoded_locs[scores_above_min_score] # N_QLF X 4\n",
    "                \n",
    "                class_scores, sort_ind = class_scores.sort(descending=True)\n",
    "                class_decoded_locs = class_decoded_locs[sort_ind]\n",
    "                \n",
    "                overlap = mu.find_jaccard_overlap(class_decoded_locs, class_decoded_locs)\n",
    "                \n",
    "                suppress = torch.zeros((n_above_min_score), dtype=torch.bool).to(device)\n",
    "                \n",
    "                for box in range(class_decoded_locs.size(0)):\n",
    "                    if suppress[box] == True:\n",
    "                        continue\n",
    "                    suppress = torch.max(suppress, overlap[box]> max_overlap)\n",
    "                    suppress[box] = False\n",
    "                \n",
    "                image_boxes.append(class_decoded_locs[~suppress])\n",
    "                image_labels.append(torch.LongTensor((~suppress).sum().item()*[c]).to(device))\n",
    "                image_scores.append(class_scores[~suppress])\n",
    "            \n",
    "            if len(image_boxes)==0:\n",
    "                image_boxes.append(torch.FloatTensor([[0.0,0.0,1.0,1.0]]).to(device))\n",
    "                image_labels.append(torch.LongTensor([0]).to(device))\n",
    "                image_scores.append(torch.FloatTensor([0.0]).to(device))\n",
    "            \n",
    "            image_boxes = torch.cat(image_boxes, dim=0)\n",
    "            image_labels = torch.cat(image_labels, dim=0)\n",
    "            image_scores = torch.cat(image_scores, dim=0)\n",
    "            \n",
    "            n_objects = image_scores.size(0)\n",
    "            \n",
    "            \n",
    "            if n_objects>top_k:\n",
    "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
    "                image_scores = image_scores[:top_k]\n",
    "                image_boxes = image_boxes[sort_ind][:top_k]\n",
    "                image_labels = image_labels[sort_ind][:top_k]\n",
    "            \n",
    "            all_images_boxes.append(image_boxes)\n",
    "            all_images_labels.append(image_labels)\n",
    "            all_images_scores.append(image_scores)\n",
    "        \n",
    "        return all_images_boxes, all_images_labels, all_images_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.0):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        \n",
    "        self.priors_cxcy = priors_cxcy\n",
    "        self.priors_xy = mu.cxcy_to_xy(priors_cxcy)\n",
    "        self.threshold = threshold\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        ## smooth??\n",
    "        self.smooth_l1 = nn.SmoothL1Loss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
    "        \n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors = self.priors_cxcy.size(0)\n",
    "        n_classes = predicted_scores.size(2)\n",
    "        \n",
    "        assert n_priors == predicted_locs.size(1) == predicted_locs.size(1)\n",
    "        \n",
    "        ## place holders for each image's true labels and boxes\n",
    "        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)\n",
    "        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)\n",
    "        \n",
    "        # for each image, generate ground truth object class and locations using the priors, i.e. wrt of priors\n",
    "        for i in range(batch_size):\n",
    "            n_objects = boxes[i].size(0)\n",
    "            \n",
    "            overlap = mu.find_jaccard_overlap(boxes[i], self.priors_xy)\n",
    "            \n",
    "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)\n",
    "            \n",
    "            overlap_for_each_object, prior_for_each_object = overlap.max(dim=1)\n",
    "            \n",
    "            ## What an engineering solution to solve a corner case!!\n",
    "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n",
    "            overlap_for_each_prior[prior_for_each_object] = 1.0\n",
    "            \n",
    "            ## assign object for each prior from the true labels\n",
    "            label_for_each_prior = labels[i][object_for_each_prior]\n",
    "            ## remove the object assignments for less than threshold overlap\n",
    "            label_for_each_prior[overlap_for_each_prior<self.threshold] = 0.0\n",
    "            \n",
    "            ## save the true classes  and locs\n",
    "            true_classes[i] = label_for_each_prior\n",
    "            true_locs[i] = mu.cxcy_to_gcxgcy(mu.xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)\n",
    "            \n",
    "#             if((boxes[i][object_for_each_prior]!=boxes[i][object_for_each_prior]).any()):\n",
    "#                 print(\"NAN IN BOXES\")\n",
    "#             if((self.priors_cxcy!=self.priors_cxcy).any()):\n",
    "#                 print(\"NAN IN PRIORS CXCY!!\")\n",
    "#                 break\n",
    "            \n",
    "#             if ((true_locs[i]!=true_locs[i]).any()):\n",
    "#                 print(\"NAN IN TRUE_LOCS!!\")\n",
    "#                 break\n",
    "        \n",
    "        \n",
    "        # find the priors with an object for all batches of images\n",
    "        positive_priors = true_classes!=0\n",
    "        \n",
    "        ## localization loss\n",
    "        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])\n",
    "        \n",
    "        ## confidence loss\n",
    "        n_positives = positive_priors.sum(dim=1) # dim=1, because positive_priors is in shape: (batch, 8732)\n",
    "        n_hard_negatives = self.neg_pos_ratio*n_positives\n",
    "        \n",
    "        ## find cross entropy for all predictions\n",
    "        conf_loss_all = self.cross_entropy(\n",
    "            predicted_scores.view(-1, n_classes), \n",
    "            true_classes.view(-1)\n",
    "        ).view(batch_size, n_priors) ## (batch_size, 8732)\n",
    "        \n",
    "        \n",
    "        conf_loss_pos = conf_loss_all[positive_priors]\n",
    "        \n",
    "        conf_loss_neg = conf_loss_all.clone()\n",
    "        conf_loss_neg[positive_priors] = 0\n",
    "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)\n",
    "        \n",
    "        # this is basically masking out the n_hard_negatives from each image in the batch, \n",
    "        # we could've done it in a easier way\n",
    "        hardness_rank = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)\n",
    "        hard_negatives = hardness_rank < n_hard_negatives.unsqueeze(dim=1)\n",
    "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]\n",
    "        \n",
    "        conf_loss = (conf_loss_hard_neg.sum()+conf_loss_pos.sum())/n_positives.sum().float()\n",
    "        \n",
    "        total_loss = conf_loss+self.alpha*loc_loss\n",
    "        \n",
    "#         if (total_loss!=total_loss).any():\n",
    "#             file_name = \"ERR.txt\"\n",
    "# #             print(\"MODEL PARAMS\", file=open(file_name, \"a\"))\n",
    "# #             print(model.named_parameters(), file=open(file_name, \"a\"))\n",
    "#             print(\"LOC LOSS\", file=open(file_name, \"a\"))\n",
    "#             print(loc_loss, file=open(file_name, \"a\"))\n",
    "#             print(\"CONF LOSS ALL\", file=open(file_name, \"a\"))\n",
    "#             print(conf_loss_all, file=open(file_name, \"a\"))\n",
    "#             print(\"CONF LOSS POS\", file=open(file_name, \"a\"))\n",
    "#             print(conf_loss_pos, file=open(file_name, \"a\"))\n",
    "#             print(\"CONF LOSS HARD NEG\", file=open(file_name, \"a\"))\n",
    "#             print(conf_loss_hard_neg, file=open(file_name, \"a\"))\n",
    "#             print(\"TRUE LOCS\", file=open(file_name, \"a\"))\n",
    "#             print(true_locs, file=open(file_name, \"a\"))\n",
    "#             print(total_loss)\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"./\"\n",
    "keep_difficult = True\n",
    "\n",
    "n_classes = len(label_map)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint = None\n",
    "batch_size = 16\n",
    "iterations = 200000\n",
    "workers = 5\n",
    "pin_memory = False\n",
    "\n",
    "model = None\n",
    "\n",
    "lr = 7e-4\n",
    "decay_lr_at = [60000,100000,140000,180000]\n",
    "decay_lr_to = 0.4\n",
    "momentum = 0.95\n",
    "weight_decay = 7e-4\n",
    "\n",
    "grad_clip = 0.5\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "mu.clear_cuda()\n",
    "history = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def train_single_epoch(epoch, train_loader, model, optimizer, criterion):\n",
    "    model = model.train()\n",
    "    l, a = 0.0, 0.0    \n",
    "    for i,(images, boxes, labels,_) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images = images.to(device)\n",
    "        boxes = [b.to(device) for b in boxes]\n",
    "        labels = [l.to(device) for l in labels]\n",
    "        \n",
    "        predicted_locs, predicted_scores = model(images)\n",
    "        \n",
    "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)\n",
    "        \n",
    "        l+=loss.item()\n",
    "        loss.backward()\n",
    "        mu.plot_grad_flow(model.named_parameters())\n",
    "        \n",
    "        if grad_clip is not None:\n",
    "            mu.clip_gradients(model, grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return l/(len(train_loader)), a/(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global n_classes, label_map, checkpoint, decay_lr_at, decay_lr_to, lr, weight_decay, momentum, history\n",
    "    global grad_clip, workers, iterations, batch_size, device, data_folder, keep_difficult, pin_memory, model\n",
    "    \n",
    "    model = SSD300(n_classes)\n",
    "    \n",
    "    if checkpoint is None:\n",
    "        start_epoch = 0\n",
    "        biases = list()\n",
    "        not_biases = list()\n",
    "        \n",
    "        for param_name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if param_name.endswith(\".bias\"):\n",
    "                    biases.append(param)\n",
    "                else:\n",
    "                    not_biases.append(param)\n",
    "        \n",
    "        optimizer = mu.RAdam(\n",
    "            params=[{'params':biases,'lr':2*lr}, {'params':not_biases}],\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch']+1\n",
    "        print(\"\\nResuming Training From Epoch: {}\\n\".format(start_epoch))\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "        model.load_state_dict(state_dict)\n",
    "        optimizer = checkpoint[\"optimizer\"]\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\n",
    "    \n",
    "    train_dataset = PascalVOCDataset(data_folder, split='train', keep_diffcult=keep_difficult)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn,\n",
    "        num_workers=workers, pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    total_epochs = iterations//(len(train_dataset)//batch_size)\n",
    "    decay_lr_at = [it//(len(train_dataset)//batch_size) for it in decay_lr_at]\n",
    "    \n",
    "    curt = time.time()\n",
    "    \n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        if epoch in decay_lr_at:\n",
    "            optimizer = mu.adjust_learning_rate(optimizer, decay_lr_to)\n",
    "        \n",
    "        train_loss, train_acc = train_single_epoch(epoch, train_loader, model, optimizer, criterion)\n",
    "        \n",
    "        mu.print_epoch_stat(epoch, time.time()-curt, history=history, train_loss=train_loss)\n",
    "        curt = time.time()\n",
    "        mu.clear_cuda()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE MODEL LOAD....COMPLETE\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e12c18e5f91485ab76916a26ac789bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 1 Completed, Time Taken: 0:06:50.672544\n",
      "\tTrain Loss \t5.86313271\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e472bbf23ba4a298ea3ff1bc57a2b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 2 Completed, Time Taken: 0:06:49.860741\n",
      "\tTrain Loss \t4.69082259\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c230dd06e36a4201a916cba75aa4050f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 3 Completed, Time Taken: 0:06:50.245378\n",
      "\tTrain Loss \t4.40688999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6e8c537096420ca6ecc11c0d4fa93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 4 Completed, Time Taken: 0:06:48.903173\n",
      "\tTrain Loss \t4.22826002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c685bfd0806459e86f02d12e6767aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 5 Completed, Time Taken: 0:06:49.046901\n",
      "\tTrain Loss \t4.13804521\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71712b6a3ac4dad88baf8b8503ee129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 6 Completed, Time Taken: 0:06:51.140551\n",
      "\tTrain Loss \t4.02524096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8775039e61a45399bbb566847fb9d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 7 Completed, Time Taken: 0:06:50.942126\n",
      "\tTrain Loss \t3.93791597\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622322f52b8746488bc6902991b45aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 8 Completed, Time Taken: 0:06:46.636186\n",
      "\tTrain Loss \t3.89465265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04235d2724e44929a327758a99b35ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 9 Completed, Time Taken: 0:06:47.029177\n",
      "\tTrain Loss \t3.84168841\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948c91fe6374407da1f0a7d26fe7295b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 10 Completed, Time Taken: 0:06:53.680433\n",
      "\tTrain Loss \t3.79936261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a30540505784dcda38385f2c519dd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 11 Completed, Time Taken: 0:06:56.410275\n",
      "\tTrain Loss \t3.77175202\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b192c91db1497a854dad3561cea01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 12 Completed, Time Taken: 0:06:47.291773\n",
      "\tTrain Loss \t3.76801732\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b6674ef74b484fb1b3b9c61718309b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 13 Completed, Time Taken: 0:06:47.828612\n",
      "\tTrain Loss \t3.74044268\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fdf3192c454ccba9cfc14a0fed65fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 14 Completed, Time Taken: 0:06:47.335701\n",
      "\tTrain Loss \t3.71514846\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95eee78dc43f48cf906001a8623803a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 15 Completed, Time Taken: 0:06:48.400446\n",
      "\tTrain Loss \t3.7082793\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f570b0ef5a449138d63bdf821e75668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 16 Completed, Time Taken: 0:06:47.610862\n",
      "\tTrain Loss \t3.68507817\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac131c919d234af6a45438d70c807e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 17 Completed, Time Taken: 0:06:47.353243\n",
      "\tTrain Loss \t3.67913531\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44dfb8fe98c478eac8a42a6b588916d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 18 Completed, Time Taken: 0:06:48.813681\n",
      "\tTrain Loss \t3.68681375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ac67e907cd4976b235940c292bc1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 19 Completed, Time Taken: 0:06:46.531604\n",
      "\tTrain Loss \t3.68381043\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacec2de57074d7a9289f7b794d7b9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 20 Completed, Time Taken: 0:06:45.954384\n",
      "\tTrain Loss \t3.66097912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ec8bb230f247b2b65f7559812905ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 21 Completed, Time Taken: 0:06:56.166275\n",
      "\tTrain Loss \t3.67962239\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c64a771c3a48779a0b7e617bc87384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 22 Completed, Time Taken: 0:06:47.091582\n",
      "\tTrain Loss \t3.66886414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3f4ba062ca4dfa85a4142c20105d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 23 Completed, Time Taken: 0:06:47.572990\n",
      "\tTrain Loss \t3.6801544\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d481adac904321afdc796a6fed5eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-18-263240bbee7e>\", line 1, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-17-d34e01bd4a21>\", line 51, in main\n",
      "    train_loss, train_acc = train_single_epoch(epoch, train_loader, model, optimizer, criterion)\n",
      "  File \"<ipython-input-16-d760ea920a51>\", line 19, in train_single_epoch\n",
      "    mu.clip_gradients(model, grad_clip)\n",
      "  File \"/home/numan947/MyHome/AIMLDL/KAGGLE/SSD_MultiBox_Detector/ml_utils.py\", line 1295, in clip_gradients\n",
      "    nn.utils.clip_grad_norm_(model.parameters(), clip_val)\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\", line 33, in clip_grad_norm_\n",
      "    total_norm += param_norm.item() ** norm_type\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE MODEL LOAD....COMPLETE\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SSD300(n_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"./MODEL.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PascalVOCDataset(data_folder, split='train', keep_diffcult=keep_difficult)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn,\n",
    "    num_workers=workers, pin_memory=pin_memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abd4ef5936d413da4df549949b001bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e7dc199dc828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_single_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMultiBoxLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpriors_cxcy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpriors_cxcy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-23b97ad4e6a8>\u001b[0m in \u001b[0;36mtrain_single_epoch\u001b[0;34m(epoch, train_loader, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_grad_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrad_clip\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyHome/AIMLDL/KAGGLE/SSD_MultiBox_Detector/ml_utils.py\u001b[0m in \u001b[0;36mplot_grad_flow\u001b[0;34m(named_parameters)\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"average gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradient flow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mgrid\u001b[0;34m(b, which, axis, **kwargs)\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2607\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'major'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2608\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhich\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mgrid\u001b[0;34m(self, b, which, axis, **kwargs)\u001b[0m\n\u001b[1;32m   2783\u001b[0m         \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_in_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'both'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'both'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2785\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhich\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'both'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2787\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhich\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mgrid\u001b[0;34m(self, b, which, **kwargs)\u001b[0m\n\u001b[1;32m   1475\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gridOnMajor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m             self.set_tick_params(which='major', gridOn=self._gridOnMajor,\n\u001b[0;32m-> 1477\u001b[0;31m                                  **gridkw)\n\u001b[0m\u001b[1;32m   1478\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mset_tick_params\u001b[0;34m(self, which, reset, **kw)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'major'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'both'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                     \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'minor'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'both'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminorTicks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_apply_params\u001b[0;34m(self, **kw)\u001b[0m\n\u001b[1;32m    371\u001b[0m                     if k in ['labelsize', 'labelcolor']}\n\u001b[1;32m    372\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlabel_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlabel_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_kw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;31m# for labelsize the text objects covert str ('small')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \u001b[0;34m\"\"\"A property batch setter.  Pass *kwargs* to set properties.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         kwargs = cbook.normalize_kwargs(\n\u001b[0;32m-> 1067\u001b[0;31m             kwargs, getattr(type(self), \"_alias_map\", {}))\n\u001b[0m\u001b[1;32m   1068\u001b[0m         props = OrderedDict(\n\u001b[1;32m   1069\u001b[0m             sorted(kwargs.items(), reverse=True,\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mnormalize_kwargs\u001b[0;34m(kw, alias_mapping, required, forbidden, allowed)\u001b[0m\n\u001b[1;32m   1764\u001b[0m                 \u001b[0mseen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1767\u001b[0m         \u001b[0;31m# if canonical is not in the alias_list assume highest priority\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcanonical\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malias_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x7f57c1ff33b0> (for post_execute):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mflush_figures\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# ignore the tracking, just draw and close all figures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;31m# safely show traceback if in IPython, else raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     41\u001b[0m             display(\n\u001b[1;32m     42\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</home/numan947/anaconda3/envs/pytorch/lib/python3.7/site-packages/decorator.py:decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2063\u001b[0m                         \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m                         \u001b[0mdryrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2065\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   2066\u001b[0m                     \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cachedRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2067\u001b[0m                     \u001b[0mbbox_artists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbox_extra_artists\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1709\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mticks_to_draw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m             \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0;31m# scale up the axis label box to also find the neighbors, not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    295\u001b[0m         for artist in [self.gridline, self.tick1line, self.tick2line,\n\u001b[1;32m    296\u001b[0m                        self.label1, self.label2]:\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    716\u001b[0m                     textrenderer.draw_text(gc, x, y, clean_line,\n\u001b[1;32m    717\u001b[0m                                            \u001b[0mtextobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fontproperties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                                            ismath=ismath, mtext=mtext)\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw_text\u001b[0;34m(self, gc, x, y, s, prop, angle, ismath, mtext)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;31m# We pass '0' for angle here, since it will be rotated (in raster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;31m# space) in the following call to draw_text_image).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mfont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mfont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_glyphs_to_bitmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mantialiased\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text.antialiased'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m64.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(20,40))\n",
    "print(train_single_epoch(0,train_loader, model, mu.RAdam(model.parameters(), lr=10e-5, weight_decay=weight_decay),MultiBoxLoss(priors_cxcy=model.priors_cxcy)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
