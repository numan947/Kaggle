{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f347eb11d90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ALL IMPORTS FOR A NEW NOTEBOOK\n",
    "__SEED = 0\n",
    "__N_FOLDS = 5\n",
    "__NROWS = None\n",
    "\n",
    "import os, sys, random, math\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 500)\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 500)\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import itertools as it\n",
    "import scipy\n",
    "import glob\n",
    "import matplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.optim import Optimizer\n",
    "import torchvision.transforms.transforms as txf\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn import model_selection as ms\n",
    "\n",
    "import ml_utils as mu\n",
    "import time\n",
    "import time, datetime, pickle\n",
    "\n",
    "\n",
    "# fold1 = ms.StratifiedKFold(n_splits=__N_FOLDS, shuffle=True, random_state=__SEED)\n",
    "# fold2 = ms.StratifiedKFold(n_splits=__N_FOLDS, shuffle=True, random_state=__SEED+3)\n",
    "# fold3 = ms.StratifiedKFold(n_splits=__N_FOLDS, shuffle=True, random_state=__SEED+5)\n",
    "font = {'size'   : 14}\n",
    "matplotlib.rc('font', **font)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(947)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILS.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## boundary coords: (x_min, y_min, x_max, y_max) --> xy\n",
    "## center-size coords: (c_x, c_y, w, h) --> cxcy\n",
    "## encoded coords: (gc_x, gc_y, gc_w, gc_h) --> gcxgcy\n",
    "\n",
    "\n",
    "def cxcy_to_xy(cxcy):\n",
    "    return torch.cat([\n",
    "        cxcy[:,:2] - (cxcy[:,2:]/2.0),\n",
    "        cxcy[:,:2]+(cxcy[:,2:]/2.0)\n",
    "    ],dim=1)\n",
    "\n",
    "def xy_to_cxcy(xy):\n",
    "    return torch.cat([\n",
    "        (xy[:,:2]+xy[:,:2])/2,\n",
    "        (xy[:,2:]-xy[:,:2])\n",
    "    ],dim=1)\n",
    "\n",
    "def cxcy_to_gcxgcy(cxcy, priors_cxcy):\n",
    "    return torch.cat([\n",
    "        (cxcy[:,:2]-priors_cxcy[:,:2])/(priors_cxcy[:,2:]/10.0),\n",
    "        torch.log((cxcy[:,2:]/priors_cxcy[:,2:])*5.0)\n",
    "    ], dim=1)\n",
    "\n",
    "def gcxgcy_to_cxcy(gcxgcy, priors_cxcy):\n",
    "    return torch.cat([\n",
    "        gcxgcy[:, :2]*priors_cxcy[:,2:]/10.0 + priors_cxcy[:,:2],\n",
    "        torch.exp((gcxgcy[:,2:]/5.0)*priors_cxcy[:,2:])\n",
    "    ],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val*n\n",
    "        self.count += n\n",
    "        self.avg = self.sum/self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(optimizer, grad_clip):\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                nn.utils.clip_grad_norm_(param, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer):\n",
    "    state = {'epoch':epoch, 'model_state':model.state_dict(), 'optimizer':optimizer}\n",
    "    filename = 'checkpoint_ssd300.pth.tar'\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, scale):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr']*scale\n",
    "    print(\"DECAYING LR\\nNew LR is: {}\".format(optimizer.param_groups[1]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimate(tensor, m):\n",
    "    assert tensor.dim() == len(m)\n",
    "    \n",
    "    print(\"Decimating Tensor of shape: \",tensor.shape)\n",
    "    for d in range(tensor.dim()):\n",
    "        if m[d] is not None:\n",
    "            tensor = tensor.index_select(dim=d, index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
    "    print(\"Decimated to Tensor of shape: \",tensor.shape)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties):\n",
    "    assert(len(det_boxes) == len(det_labels) == len(det_scores) == len(true_boxes) == len(true_labels) == len(true_difficulties))\n",
    "    n_classes = len(label_map)\n",
    "    \n",
    "    \n",
    "    true_images = list()\n",
    "    for i in range(len(true_labels)):\n",
    "        true_images.extend([i]*true_labels[i].size(0))\n",
    "    true_images = torch.LongTensor(true_images).to(device)\n",
    "    true_boxes = torch.cat(true_boxes, dim=0)\n",
    "    true_labels = torch.cat(true_labels, dim=0)\n",
    "    true_difficulties = torch.cat(true_difficulties, dim=0)\n",
    "    assert true_images.size(0) == true_labels.size(0) == true_boxes.size(0)\n",
    "    \n",
    "    \n",
    "    det_images = list()\n",
    "    for i in range(len(det_labels)):\n",
    "        det_images.extend([i]*det_labels[i].size(0))\n",
    "    det_images = torch.LongTensor(det_images).to(device)\n",
    "    det_boxes = torch.cat(det_boxes, dim=0)\n",
    "    det_labels = torch.cat(det_labels, dim=0)\n",
    "    det_scores = torch.cat(det_scores, dim=0)\n",
    "    assert det_images.size(0) == det_labels.size(0) == det_scores.size(0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    average_precisions = torch.zeros((n_classes-1), dtype=torch.float)\n",
    "    \n",
    "    for c in tqdm(range(1, n_classes)):\n",
    "        true_class_images = true_images[true_labels == c]\n",
    "        true_class_boxes = true_boxes[true_labels == c]\n",
    "        true_class_difficulties = true_difficulties[true_labels == c]\n",
    "        n_easy_class_objects = (1-true_class_difficulties).sum().item()\n",
    "        \n",
    "        \n",
    "        true_class_boxes_detected = torch.zeros((true_class_difficulties.size(0)), dtype=torch.bool).to(device)\n",
    "        \n",
    "        det_class_images = det_images[det_labels == c]\n",
    "        det_class_boxes = det_boxes[det_labels == c]\n",
    "        det_class_scores = det_scores[det_labels == c]\n",
    "        n_class_detections = det_class_boxes.size(0)\n",
    "        \n",
    "        if n_class_detections == 0:\n",
    "            continue\n",
    "        \n",
    "        det_class_scores, sort_ind = torch.sort(det_class_scores, dim=0, descending=True)\n",
    "        det_class_images = det_class_images[sort_ind]\n",
    "        det_class_boxes = det_class_boxes[sort_ind]\n",
    "        \n",
    "        true_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)\n",
    "        false_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)\n",
    "        \n",
    "        for d in range(n_class_detections):\n",
    "            this_detection_box = det_class_boxes[d].unsqueeze(0)\n",
    "            this_image = det_class_images[d]\n",
    "            \n",
    "            object_boxes = true_class_boxes[true_class_images == this_image]\n",
    "            object_difficulties = true_class_difficulties[true_class_images == this_image]\n",
    "            \n",
    "            if object_boxes.size(0) == 0:\n",
    "                false_positives[d] = 1\n",
    "                continue\n",
    "            \n",
    "            overlaps = find_jaccard_overlap(this_detection_box, object_boxes)\n",
    "            max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)\n",
    "            \n",
    "            original_ind = torch.LongTensor(range(true_class_boxes.size(0)))[true_class_images == this_image][ind]\n",
    "            \n",
    "            if max_overlap.item()>0.5:\n",
    "                if object_difficulties[ind] == 0:\n",
    "                    if true_class_boxes_detected[original_ind] == 0:\n",
    "                        true_positives[d] = 1\n",
    "                        true_class_boxes_detected[original_ind] = 1\n",
    "                    else:\n",
    "                        false_positives[d] = 1\n",
    "            else:\n",
    "                false_positives[d] = 1\n",
    "            \n",
    "        cumulative_tps = torch.cumsum(true_positives, dim=0)\n",
    "        cumulative_fps = torch.cumsum(false_positives, dim=0)\n",
    "        cumulative_precision = cumulative_tps/(cumulative_tps+cumulative_fps+1e-16)\n",
    "        cumulative_recall = cumulative_tps/(n_easy_class_objects+1e-16)\n",
    "        \n",
    "        \n",
    "        recall_thresholds = torch.arange(start=0, end=1.1, step=0.1).tolist()\n",
    "        precisions = torch.zeros((len(recall_thresholds)), dtype=torch.float).to(device)\n",
    "        \n",
    "        for i, t in enumerate(recall_thresholds):\n",
    "            recalls_above_t = cumulative_recall>=t\n",
    "            if recalls_above_t.any():\n",
    "                precisions[i] = cumulative_precision[recalls_above_t].max()\n",
    "            else:\n",
    "                precisions[i] = 0.0\n",
    "        \n",
    "        average_precisions[c-1] = precisions.mean()\n",
    "    \n",
    "    mean_average_precision = average_precisions.mean().item()\n",
    "    \n",
    "    average_precisions = {rev_label_map[c+1]:v for c,v in enumerate(average_precisions.tolist())}\n",
    "    \n",
    "    return average_precisions, mean_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_labels = [\"aeroplane\",\"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \n",
    "              \"diningtable\",\"dog\",\"horse\",\"motorbike\",\"person\", \"train\",\"pottedplant\",\n",
    "              \"sheep\",\"sofa\",\"tvmonitor\"]\n",
    "\n",
    "label_map = {k:v+1 for v,k in enumerate(voc_labels)}\n",
    "label_map[\"background\"] = 0\n",
    "rev_label_map = {v:k for k,v in label_map.items()}\n",
    "\n",
    "distinct_colors = ['#e6194B', '#3cb44b', '#ffe119', '#4363d8', '#f58231', \n",
    "                   '#911eb4', '#42d4f4', '#f032e6', '#bfef45', '#fabebe',\n",
    "                   '#469990', '#e6beff', '#9A6324', '#fffac8', '#800000',\n",
    "                   '#aaffc3', '#808000', '#ffd8b1', '#000075', '#a9a9a9',\"#FFFFFF\"]\n",
    "\n",
    "label_color_map = {k:distinct_colors[i] for i,k in enumerate(label_map.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "import json\n",
    "\n",
    "### FOR PARSING THE XML ANNOTATIONS OF THE DATASETS ###\n",
    "def parse_annotations(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    \n",
    "    root = tree.getroot()\n",
    "    \n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    difficulties = list()\n",
    "    \n",
    "    for object in root.iter('object'):\n",
    "        difficult = int(object.find(\"difficult\").text==\"1\")\n",
    "        label = object.find(\"name\").text.lower().strip()\n",
    "        \n",
    "        if label not in label_map:\n",
    "            continue\n",
    "        \n",
    "        bbox = object.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) - 1\n",
    "        xmax = int(bbox.find('xmax').text) - 1\n",
    "        ymin = int(bbox.find('ymin').text) - 1\n",
    "        ymax = int(bbox.find('ymax').text) - 1\n",
    "        \n",
    "        boxes.append([xmin,ymin,xmax,ymax])\n",
    "        labels.append(label_map[label])\n",
    "        difficulties.append(difficult)\n",
    "    return {\"boxes\":boxes, \"labels\":labels,\"difficulties\":difficulties}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### READ XML DESCRIPTIONS AND SAVE AS JSON FOR FASTER ACCESS ###\n",
    "def create_data_lists(v07, v12, output_folder):\n",
    "    v07_path = os.path.abspath(v07)\n",
    "    v12_path = os.path.abspath(v12)\n",
    "    \n",
    "    ## PARSE TRAIN ##\n",
    "    train_images = list()\n",
    "    train_objects = list()\n",
    "    n_objects = 0\n",
    "    \n",
    "    for path in [v07_path, v12_path]:\n",
    "        with open(os.path.join(path,\"ImageSets/Main/trainval.txt\")) as f:\n",
    "            ids = f.read().splitlines()\n",
    "            \n",
    "            for id in ids:\n",
    "                objects = parse_annotations(os.path.join(path,\"Annotations\",id+\".xml\"))\n",
    "                \n",
    "                if len(objects[\"boxes\"])==0:\n",
    "                    continue\n",
    "                n_objects+=len(objects[\"boxes\"])\n",
    "                \n",
    "                train_objects.append(objects)\n",
    "                train_images.append(os.path.join(path, \"JPEGImages\", id+'.jpg'))\n",
    "                \n",
    "        \n",
    "    print(\"TOTAL TRAIN IMAGES: {} == TOTAL TRAIN OBJECTS {}\".format(len(train_images), len(train_objects)))\n",
    "    print(\"TOTAL OBJECTS IN ALL TRAIN IMAGES: {}\\n\".format(n_objects))\n",
    "    \n",
    "    with open(os.path.join(output_folder, \"TRAIN_images.json\"), 'w') as j:\n",
    "        json.dump(train_images, j)\n",
    "    with open(os.path.join(output_folder, \"TRAIN_objects.json\"), 'w') as j:\n",
    "        json.dump(train_objects, j)\n",
    "    with open(os.path.join(output_folder, \"label_map.json\"), 'w') as j:\n",
    "        json.dump(label_map, j)\n",
    "    \n",
    "    \n",
    "    ## PARSE TEST ##\n",
    "    \n",
    "    test_images = list()\n",
    "    test_objects = list()\n",
    "    n_objects = 0\n",
    "    \n",
    "    with open(os.path.join(v07_path, \"ImageSets/Main/test.txt\")) as f:\n",
    "        ids = f.read().splitlines()\n",
    "        \n",
    "        for id in ids:\n",
    "            objects = parse_annotations(os.path.join(v07_path, \"Annotations\", id+\".xml\"))\n",
    "            \n",
    "            if (len(objects[\"boxes\"]))==0:\n",
    "                continue\n",
    "            test_objects.append(objects)\n",
    "            test_images.append(os.path.join(v07_path,\"JPEGImages\",id+'.jpg'))\n",
    "            n_objects+=len(objects['boxes'])\n",
    "        \n",
    "    print(\"TOTAL TEST IMAGES: {} == TOTAL TEST OBJECTS {}\".format(len(test_images), len(test_objects)))\n",
    "    print(\"TOTAL OBJECTS IN ALL TEST IMAGES: {}\\n\".format(n_objects))\n",
    "    \n",
    "    with open(os.path.join(output_folder, \"TEST_images.json\"), 'w') as j:\n",
    "        json.dump(test_images, j)\n",
    "    with open(os.path.join(output_folder, \"TEST_objects.json\"), 'w') as j:\n",
    "        json.dump(test_objects, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as FT\n",
    "def photometric_distort(image):\n",
    "    \"\"\"\n",
    "    Distort brightness, contrast, saturation and hue, each with a 50% chance in random order\n",
    "    \n",
    "    \"\"\"\n",
    "    new_image = image\n",
    "    distortions = [FT.adjust_brightness, FT.adjust_contrast, FT.adjust_saturation, FT.adjust_hue]\n",
    "    \n",
    "    random.shuffle(distortions)\n",
    "    \n",
    "    for d in distortions:\n",
    "        if random.random()<0.5:\n",
    "            if d.__name__ is \"adjust_hue\":\n",
    "                adjust_factor = random.uniform(-18/255.0, 18/255.0)\n",
    "            else:\n",
    "                adjust_factor = random.uniform(0.5, 1.5)\n",
    "            \n",
    "            new_image = d(new_image, adjust_factor)\n",
    "    \n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(image, boxes, filler):\n",
    "    \"\"\"\n",
    "    Zooming out by placing the image in a larger canvas of filler material.\n",
    "    Helps to detect smaller objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    original_h = image.size(1)\n",
    "    original_w = image.size(2)\n",
    "    \n",
    "    max_scale = 4\n",
    "    scale = random.uniform(1,max_scale)\n",
    "    new_h = int(scale*original_h)\n",
    "    new_w = int(scale*original_w)\n",
    "    \n",
    "    filler = torch.FloatTensor(filler)\n",
    "    \n",
    "    ## elementwise product to initialize the new image with all filler\n",
    "    new_image = torch.ones((3,new_h, new_w), dtype=torch.float)*filler.unsqueeze(1).unsqueeze(1)\n",
    "    \n",
    "    ## place the original image in random coordinate in the new expnded image\n",
    "    left = random.randint(0, new_w-original_w)\n",
    "    right = left+original_w\n",
    "    top = random.randint(0,new_h-original_h)\n",
    "    bottom = top+original_h\n",
    "    new_image[:, top:bottom, left:right] = image\n",
    "    \n",
    "    ## update bbox locations\n",
    "    new_boxes = boxes+torch.FloatTensor([left, top, left, top]).unsqueeze(0)\n",
    "    \n",
    "    return new_image, new_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intersection(set1, set2):\n",
    "    lower_bounds = torch.max(set1[:,:2].unsqueeze(1), set2[:,:2].unsqueeze(0)) # n1 X n2 X 2\n",
    "    upper_bounds = torch.min(set1[:,2:].unsqueeze(1), set2[:,2:].unsqueeze(0)) # n1 X n2 X 2\n",
    "    \n",
    "    intersection_dims = torch.clamp(upper_bounds-lower_bounds, min=0) # just to make sure there're no -ves\n",
    "    \n",
    "    return intersection_dims[:,:,1]*intersection_dims[:,:,0] # one is height, another is width\n",
    "\n",
    "def find_jaccard_overlap(set1, set2):\n",
    "    \"\"\"\n",
    "    set1, set2 --> n1X4, n2X4\n",
    "    output --> n1Xn2\n",
    "    \"\"\"\n",
    "    \n",
    "    intersection = find_intersection(set1, set2) # n1 x n2\n",
    "    \n",
    "    areas_set1 = (set1[:,2]-set1[:,0])*(set1[:,3]-set1[:,1]) # n1\n",
    "    areas_set2 = (set2[:,2]-set2[:,0])*(set2[:,3]-set2[:,1]) # n2\n",
    "    \n",
    "    union = areas_set1.unsqueeze(1)+areas_set2.unsqueeze(0)-intersection # n1 X n2\n",
    "    \n",
    "    return intersection/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image, boxes, labels, difficulties):\n",
    "    \"\"\"\n",
    "    Helps to detect larger and partial objects\n",
    "    \"\"\"\n",
    "    \n",
    "    original_h = image.size(1)\n",
    "    original_w = image.size(2)\n",
    "    \n",
    "    # Keep choosing minimum overlap until a successful crop is made\n",
    "    while True:\n",
    "        min_overlap = random.choice([0.0, 0.1, 0.3, 0.5, 0.7, 0.8, None])\n",
    "        \n",
    "        if min_overlap is None:\n",
    "            return image, boxes, labels, difficulties\n",
    "        \n",
    "        # try up to 50 times for this choice of minimum overlap\n",
    "        \n",
    "        max_trials = 50\n",
    "        for _ in range(max_trials):\n",
    "            min_scale = 0.3 # crop dimension\n",
    "            \n",
    "            scale_h = random.uniform(min_scale, 1)\n",
    "            scale_w = random.uniform(min_scale, 1)\n",
    "            \n",
    "            new_h = int(scale_h*original_h)\n",
    "            new_w = int(scale_w*original_w)\n",
    "            \n",
    "            aspect_ratio = new_h/new_w\n",
    "            \n",
    "            # aspect ratio must be within 0.5 ~ 2 for our implementation\n",
    "            if not 0.5<aspect_ratio<2:\n",
    "                continue\n",
    "            \n",
    "            left = random.randint(0, original_w - new_w)\n",
    "            right = left + new_w\n",
    "            top = random.randint(0, original_h - new_h)\n",
    "            bottom = top + new_h\n",
    "            crop = torch.FloatTensor([left, top, right, bottom])\n",
    "\n",
    "            # Find jaccard overlap between the boxes and crop section, if there're no boxes with minimum jaccard\n",
    "            # overlap needed, try again....50 times\n",
    "            overlap = find_jaccard_overlap(crop.unsqueeze(0), boxes) # make sure intersection dimension is in n1X4 \n",
    "            overlap = overlap.squeeze(0) # for getting the n objects\n",
    "            \n",
    "            if overlap.max().item()<min_overlap:\n",
    "                continue\n",
    "            \n",
    "            new_image = image[:, top:bottom, left:right]\n",
    "            \n",
    "            ## original bb centers\n",
    "            bb_centers = (boxes[:,:2]+boxes[:,2:])/2.0 # nobs X 2\n",
    "            \n",
    "            centers_in_crop = (bb_centers[:,0]>left)*(bb_centers[:,0]<right)*(bb_centers[:,1]>top)*(bb_centers[:,1]<bottom)\n",
    "            \n",
    "            if not centers_in_crop.any():\n",
    "                continue\n",
    "            \n",
    "            new_boxes = boxes[centers_in_crop]\n",
    "            new_labels = labels[centers_in_crop]\n",
    "            new_difficulties = difficulties[centers_in_crop]\n",
    "            \n",
    "            new_boxes[:,:2] = torch.max(new_boxes[:,:2], crop[:2])\n",
    "            new_boxes[:,:2] -= crop[:2] # adjust the new boxes wrt new image\n",
    "            new_boxes[:,2:] = torch.min(new_boxes[:,2:], crop[2:])\n",
    "            new_boxes[:,2:] -= crop[:2]\n",
    "            \n",
    "            return new_image, new_boxes, new_labels, new_difficulties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(image, boxes):\n",
    "    new_image = FT.hflip(image)\n",
    "    \n",
    "    new_boxes = boxes\n",
    "    new_boxes[:,0] = image.width-boxes[:,0]-1\n",
    "    new_boxes[:,2] = image.width-boxes[:,2]-1\n",
    "    new_boxes = new_boxes[:,[2,1,0,3]]\n",
    "    return new_image, new_boxes\n",
    "\n",
    "def resize(image, boxes, dims=(300,300), return_percent_coords=True):\n",
    "    new_image = FT.resize(image, dims)\n",
    "    \n",
    "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n",
    "    new_boxes = boxes/old_dims\n",
    "    \n",
    "    if not return_percent_coords:\n",
    "        new_dims = torch.FloatTensor([dims[1],dims[0],dims[1],dims[0]]).unsqueeze(0)\n",
    "        new_boxes = new_boxes*new_dims\n",
    "    \n",
    "    return new_image, new_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image, boxes, labels, difficulties, split):\n",
    "    assert split in {\"TRAIN\", \"TEST\"}\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    new_image = image\n",
    "    new_boxes = boxes\n",
    "    new_labels = labels\n",
    "    new_difficulties = difficulties\n",
    "    \n",
    "    if split == \"TRAIN\":\n",
    "        new_image = photometric_distort(new_image)\n",
    "        new_image = FT.to_tensor(new_image)\n",
    "        \n",
    "        ## ZOOM OUT\n",
    "        if random.random()<0.5:\n",
    "            new_image, new_boxes = expand(new_image, boxes, filler=mean)\n",
    "        \n",
    "        ## ZOOM IN\n",
    "        new_image, new_boxes, new_labels, new_difficulties = random_crop(new_image, new_boxes, new_labels,\n",
    "                                                                         new_difficulties)\n",
    "        \n",
    "        new_image = FT.to_pil_image(new_image)\n",
    "        \n",
    "        if random.random()<0.5:\n",
    "            new_image, new_boxes = flip(new_image, new_boxes)\n",
    "    print((new_boxes<0.0).any().sum())\n",
    "    new_image, new_boxes = resize(new_image, new_boxes)\n",
    "    new_image = FT.to_tensor(new_image)\n",
    "    \n",
    "    new_image = FT.normalize(new_image, mean=mean, std=std)\n",
    "        \n",
    "    return new_image, new_boxes, new_labels, new_difficulties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# create_data_lists(v07='./train/VOC2007/',\n",
    "#                       v12='./train/VOC2012',\n",
    "#                       output_folder='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PascalVOCDataset(data_folder, split='train', keep_diffcult=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, collate_fn=train_dataset.collate_fn,\n",
    "                          shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for b in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASETS.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class PascalVOCDataset(Dataset):\n",
    "    def __init__(self, data_folder, split, keep_diffcult=False):\n",
    "        self.split = split.upper()\n",
    "        assert self.split in {\"TRAIN\", \"TEST\"}\n",
    "        \n",
    "        self.data_folder = data_folder\n",
    "        self.keep_difficult = keep_diffcult\n",
    "        \n",
    "        with open(os.path.join(data_folder, self.split+\"_images.json\"),'r') as j:\n",
    "            self.images = json.load(j)\n",
    "        with open(os.path.join(data_folder, self.split+\"_objects.json\"), 'r') as j:\n",
    "            self.objects = json.load(j)\n",
    "        assert len(self.images) == len(self.objects)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = Image.open(self.images[i], mode='r')\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "        objects = self.objects[i]\n",
    "        boxes = torch.FloatTensor(objects[\"boxes\"]) # n_obs X 4\n",
    "        labels = torch.LongTensor(objects[\"labels\"]) # n_obs\n",
    "        difficulties = torch.ByteTensor(objects[\"difficulties\"]) # n_obs\n",
    "    \n",
    "        if not self.keep_difficult:\n",
    "            boxes = boxes[1-difficulties]\n",
    "            labels = labels[1-difficulties]\n",
    "            difficulties = difficulties[1-difficulties]\n",
    "        \n",
    "        image, boxes, labels, difficulties = transform(image, boxes, labels, difficulties, split=self.split)\n",
    "        \n",
    "        return image, boxes, labels, difficulties\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Each image may have different number of objects and so,\n",
    "        we need to provide a function to combine these tensors of different sizes.\n",
    "        \"\"\"\n",
    "        \n",
    "        images = list()\n",
    "        boxes = list()\n",
    "        labels = list()\n",
    "        difficulties = list()\n",
    "        \n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "            labels.append(b[2])\n",
    "            difficulties.append(b[3])\n",
    "        images = torch.stack(images, dim=0)\n",
    "        \n",
    "        return images, boxes, labels, difficulties # tensor (N, 3, 300, 300) and 3 lists of N tensors each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "class VGGBase(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGGBase, self).__init__()\n",
    "        \n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv5_1 = nn.Conv2d(512,512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512,512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512,512, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, padding=1, stride=1)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "        \n",
    "        self.load_pretrained_layers()\n",
    "    \n",
    "    def forward(self, image):\n",
    "        out = F.relu(self.conv1_1(image))\n",
    "        out = F.relu(self.conv1_2(out))\n",
    "        out = self.pool1(out)\n",
    "        \n",
    "        out = F.relu(self.conv2_1(out))\n",
    "        out = F.relu(self.conv2_2(out))\n",
    "        out = self.pool2(out)\n",
    "        \n",
    "        out = F.relu(self.conv3_1(out))\n",
    "        out = F.relu(self.conv3_2(out))\n",
    "        out = F.relu(self.conv3_3(out))\n",
    "        out = self.pool3(out)\n",
    "        \n",
    "        out = F.relu(self.conv4_1(out))\n",
    "        out = F.relu(self.conv4_2(out))\n",
    "        out = F.relu(self.conv4_3(out))\n",
    "        conv_4_3_feats = out\n",
    "        out = self.pool4(out)\n",
    "        \n",
    "        out = F.relu(self.conv5_1(out))\n",
    "        out = F.relu(self.conv5_2(out))\n",
    "        out = F.relu(self.conv5_3(out))\n",
    "        out = self.pool5(out)\n",
    "        \n",
    "        out = F.relu(self.conv6(out))\n",
    "        out = F.relu(self.conv7(out))\n",
    "        \n",
    "        conv7_feats = out\n",
    "        \n",
    "        return conv_4_3_feats, conv7_feats\n",
    "    \n",
    "    def load_pretrained_layers(self):\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "        \n",
    "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "        \n",
    "        ## LOADING FIRST UNCHANGED LAYERS' WEIGHTS\n",
    "        for i, param in enumerate(param_names[:-4]):\n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "        \n",
    "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)\n",
    "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']\n",
    "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4,None, 3, 3])\n",
    "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])\n",
    "        \n",
    "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096,4096,1,1)\n",
    "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']\n",
    "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4,4,None,None])\n",
    "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])\n",
    "        \n",
    "        self.load_state_dict(state_dict)\n",
    "        \n",
    "        print(\"BASE MODEL LOAD....COMPLETE\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryConvolutions(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxiliaryConvolutions, self).__init__()\n",
    "        \n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2)\n",
    "        \n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2)\n",
    "        \n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
    "        \n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
    "        \n",
    "        \n",
    "        self.init_conv2d()\n",
    "    \n",
    "    def init_conv2d(self):\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.0)\n",
    "    \n",
    "    def forward(self, conv7_feats):\n",
    "        out = F.relu(self.conv8_1(conv7_feats))\n",
    "        out = F.relu(self.conv8_2(out))\n",
    "        conv8_2_feats = out\n",
    "        \n",
    "        out = F.relu(self.conv9_1(out))\n",
    "        out = F.relu(self.conv9_2(out))\n",
    "        conv9_2_feats = out\n",
    "        \n",
    "        out = F.relu(self.conv10_1(out))\n",
    "        out = F.relu(self.conv10_2(out))\n",
    "        conv10_2_feats = out\n",
    "        \n",
    "        out = F.relu(self.conv11_1(out))\n",
    "        out = F.relu(self.conv11_2(out))\n",
    "        conv11_2_feats = out\n",
    "        \n",
    "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionConvolutions(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(PredictionConvolutions, self).__init__()\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        n_boxes = {\n",
    "            'conv4_3':4,\n",
    "            'conv7':6,\n",
    "            'conv8_2':6,\n",
    "            'conv9_2':6,\n",
    "            'conv10_2':4,\n",
    "            'conv11_2':4\n",
    "        }\n",
    "        \n",
    "        \n",
    "        self.loc_conv4_3 = nn.Conv2d(512, 4*n_boxes['conv4_3'], kernel_size=3, padding=1)\n",
    "        self.loc_conv7 = nn.Conv2d(1024, 4*n_boxes['conv7'], kernel_size=3, padding=1)\n",
    "        self.loc_conv8_2 = nn.Conv2d(512, 4*n_boxes['conv8_2'], kernel_size=3, padding=1)\n",
    "        self.loc_conv9_2 = nn.Conv2d(256, 4*n_boxes['conv9_2'], kernel_size=3, padding=1)\n",
    "        self.loc_conv10_2 = nn.Conv2d(256, 4*n_boxes['conv10_2'], kernel_size=3, padding=1)\n",
    "        self.loc_conv11_2 = nn.Conv2d(256, 4*n_boxes['conv11_2'], kernel_size=3, padding=1)\n",
    "        \n",
    "        \n",
    "        self.cl_conv4_3 = nn.Conv2d(512, self.n_classes*n_boxes['conv4_3'], kernel_size=3, padding=1)\n",
    "        self.cl_conv7 = nn.Conv2d(1024, self.n_classes*n_boxes['conv7'], kernel_size=3, padding=1)\n",
    "        self.cl_conv8_2 = nn.Conv2d(512, self.n_classes*n_boxes['conv8_2'], kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2 = nn.Conv2d(256, self.n_classes*n_boxes['conv9_2'], kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256, self.n_classes*n_boxes['conv10_2'], kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256, self.n_classes*n_boxes['conv11_2'], kernel_size=3, padding=1)\n",
    "        \n",
    "        self.init_conv2d()\n",
    "    \n",
    "    def init_conv2d(self):\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.0)\n",
    "                \n",
    "    \n",
    "    \n",
    "    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n",
    "        \n",
    "        batch_size = conv4_3_feats.size(0)\n",
    "        \n",
    "        l_conv4_3 = self.loc_conv4_3(conv4_3_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, 4)\n",
    "        l_conv7 = self.loc_conv7(conv7_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, 4)\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, 4)\n",
    "        l_conv9_2 = self.loc_conv9_2(conv9_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, 4)\n",
    "        l_conv10_2 = self.loc_conv10_2(conv10_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, 4)\n",
    "        l_conv11_2 = self.loc_conv11_2(conv11_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, 4)\n",
    "        \n",
    "        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)\n",
    "        \n",
    "        c_conv4_3 = self.cl_conv4_3(conv4_3_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv7 = self.cl_conv7(conv7_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats).permute(0,2,3,1).contiguous().view(batch_size, -1, self.n_classes)\n",
    "        \n",
    "        class_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2], dim=1)\n",
    "        \n",
    "        return locs, class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "class SSD300(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SSD300, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.base = VGGBase()\n",
    "        self.aux_convs = AuxiliaryConvolutions()\n",
    "        self.pred_convs = PredictionConvolutions(n_classes)\n",
    "        \n",
    "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))\n",
    "        nn.init.constant_(self.rescale_factors, 20)\n",
    "        \n",
    "        self.priors_cxcy = self.create_prior_boxes()\n",
    "    \n",
    "    def create_prior_boxes(self):\n",
    "        fmap_dims = {\n",
    "            \"conv4_3\":38,\n",
    "            \"conv7\":19,\n",
    "            \"conv8_2\":10,\n",
    "            \"conv9_2\":5,\n",
    "            \"conv10_2\":3,\n",
    "            \"conv11_2\":1\n",
    "        }\n",
    "        obj_scales = {\n",
    "            \"conv4_3\":0.1,\n",
    "            \"conv7\":0.2,\n",
    "            \"conv8_2\":0.375,\n",
    "            \"conv9_2\":0.55,\n",
    "            \"conv10_2\":0.725,\n",
    "            \"conv11_2\":0.9\n",
    "        }\n",
    "        aspect_ratios = {\n",
    "            \"conv4_3\":[1.0,2.0,0.5],\n",
    "            \"conv7\":[1.0,2.0,0.5,3.0,.33],\n",
    "            \"conv8_2\":[1.0,2.0,0.5,3.0,.33],\n",
    "            \"conv9_2\":[1.0,2.0,0.5,3.0,.33],\n",
    "            \"conv10_2\":[1.0,2.0,0.5],\n",
    "            \"conv11_2\":[1.0,2.0,0.5]\n",
    "        }\n",
    "        \n",
    "        fmaps = list(fmap_dims.keys())\n",
    "        \n",
    "        prior_boxes = []\n",
    "        \n",
    "        for k, fmap in enumerate(fmaps):\n",
    "            for i in range(fmap_dims[fmap]):\n",
    "                for j in range(fmap_dims[fmap]):\n",
    "                    cx = (j+0.5)/fmap_dims[fmap]\n",
    "                    cy = (i+0.5)/fmap_dims[fmap]\n",
    "                    \n",
    "                    for ratio in aspect_ratios[fmap]:\n",
    "                        prior_boxes.append([cx,cy,obj_scales[fmap]*sqrt(ratio), obj_scales[fmap]/sqrt(ratio)])\n",
    "                        \n",
    "                        if ratio==1:\n",
    "                            try:\n",
    "                                additional_scale = sqrt(obj_scales[fmap]*obj_scales[fmaps[k+1]])\n",
    "                            except IndexError:\n",
    "                                additional_scale = 1\n",
    "                            \n",
    "                            prior_boxes.append([cx,cy,additional_scale, additional_scale])\n",
    "        \n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)\n",
    "        prior_boxes.clamp_(0, 1)\n",
    "        \n",
    "        return prior_boxes\n",
    "    \n",
    "    def forward(self, image):\n",
    "        conv4_3_feats, conv7_feats = self.base(image)\n",
    "        norm = conv4_3_feats.norm(dim=1, keepdim=True)+1e-16\n",
    "        conv4_3_feats = conv4_3_feats/norm\n",
    "        conv4_3_feats = conv4_3_feats*self.rescale_factors\n",
    "        \n",
    "        \n",
    "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = self.aux_convs(conv7_feats)\n",
    "        \n",
    "        locs, class_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats)\n",
    "        \n",
    "        return locs, class_scores\n",
    "    \n",
    "    \n",
    "    def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors = self.priors_cxcy.size(0)\n",
    "        \n",
    "        predicted_scores = F.softmax(predicted_scores, dim=-1)\n",
    "        \n",
    "        all_images_boxes = list()\n",
    "        all_images_labels = list()\n",
    "        all_images_scores = list()\n",
    "        \n",
    "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
    "        \n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            decoded_locs = cxcy_to_xy(gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))\n",
    "            \n",
    "            image_boxes = list()\n",
    "            image_labels = list()\n",
    "            image_scores = list()\n",
    "            \n",
    "#             max_scores, best_label = predicted_scores[i].max(dim=1)\n",
    "            \n",
    "            for c in range(1, self.n_classes):\n",
    "                class_scores  = predicted_scores[i][:, c] # 8732\n",
    "                scores_above_min_score = class_scores > min_score # 8732, True or False\n",
    "                n_above_min_score = scores_above_min_score.sum().item() # 1\n",
    "                \n",
    "                if n_above_min_score == 0:\n",
    "                    continue\n",
    "                \n",
    "                # valid scores\n",
    "                class_scores = class_scores[scores_above_min_score] # N_QLF\n",
    "                # boxes with valid scores\n",
    "                class_decoded_locs = decoded_locs[scores_above_min_score] # N_QLF X 4\n",
    "                \n",
    "                class_scores, sort_ind = class_scores.sort(descending=True)\n",
    "                class_decoded_locs = class_decoded_locs[sort_ind]\n",
    "                \n",
    "                overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)\n",
    "                \n",
    "                suppress = torch.zeros((n_above_min_score), dtype=torch.bool).to(device)\n",
    "                \n",
    "                for box in range(class_decoded_locs.size(0)):\n",
    "                    if suppress[box] == True:\n",
    "                        continue\n",
    "                    suppress = torch.max(suppress, overlap[box]> max_overlap)\n",
    "                    suppress[box] = False\n",
    "                \n",
    "                image_boxes.append(class_decoded_locs[~suppress])\n",
    "                image_labels.append(torch.LongTensor((~suppress).sum().item()*[c]).to(device))\n",
    "                image_scores.append(class_scores[~suppress])\n",
    "            \n",
    "            if len(image_boxes)==0:\n",
    "                image_boxes.append(torch.FloatTensor([[0.0,0.0,1.0,1.0]]).to(device))\n",
    "                image_labels.append(torch.LongTensor([0]).to(device))\n",
    "                image_scores.append(torch.FloatTensor([0.0]).to(device))\n",
    "            \n",
    "            image_boxes = torch.cat(image_boxes, dim=0)\n",
    "            image_labels = torch.cat(image_labels, dim=0)\n",
    "            image_scores = torch.cat(image_scores, dim=0)\n",
    "            \n",
    "            n_objects = image_scores.size(0)\n",
    "            \n",
    "            \n",
    "            if n_objects>top_k:\n",
    "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
    "                image_scores = image_scores[:top_k]\n",
    "                image_boxes = image_boxes[sort_ind][:top_k]\n",
    "                image_labels = image_labels[sort_ind][:top_k]\n",
    "            \n",
    "            all_images_boxes.append(image_boxes)\n",
    "            all_images_labels.append(image_labels)\n",
    "            all_images_scores.append(image_scores)\n",
    "        \n",
    "        return all_images_boxes, all_images_labels, all_images_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.0):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        \n",
    "        self.priors_cxcy = priors_cxcy\n",
    "        self.priors_xy = cxcy_to_xy(priors_cxcy)\n",
    "        self.threshold = threshold\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        ## smooth??\n",
    "        self.smooth_l1 = nn.SmoothL1Loss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
    "        \n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors = self.priors_cxcy.size(0)\n",
    "        n_classes = predicted_scores.size(2)\n",
    "        \n",
    "        assert n_priors == predicted_locs.size(1) == predicted_locs.size(1)\n",
    "        \n",
    "        ## place holders for each image's true labels and boxes\n",
    "        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)\n",
    "        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)\n",
    "        \n",
    "        # for each image, generate ground truth object class and locations using the priors, i.e. wrt of priors\n",
    "        for i in range(batch_size):\n",
    "            n_objects = boxes[i].size(0)\n",
    "            \n",
    "            overlap = find_jaccard_overlap(boxes[i], self.priors_xy)\n",
    "            \n",
    "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)\n",
    "            \n",
    "            overlap_for_each_object, prior_for_each_object = overlap.max(dim=1)\n",
    "            \n",
    "            ## What an engineering solution to solve a corner case!!\n",
    "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n",
    "            overlap_for_each_prior[prior_for_each_object] = 1.0\n",
    "            \n",
    "            ## assign object for each prior from the true labels\n",
    "            label_for_each_prior = labels[i][object_for_each_prior]\n",
    "            ## remove the object assignments for less than threshold overlap\n",
    "            label_for_each_prior[overlap_for_each_prior<self.threshold] = 0\n",
    "            \n",
    "            ## save the true classes  and locs\n",
    "            true_classes[i] = label_for_each_prior\n",
    "            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)\n",
    "        \n",
    "        \n",
    "        # find the priors with an object for all batches of images\n",
    "        positive_priors = true_classes!=0\n",
    "        \n",
    "        ## localization loss\n",
    "        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])\n",
    "        \n",
    "        \n",
    "        ## confidence loss\n",
    "        n_positives = positive_priors.sum(dim=1) # dim=1, because positive_priors is in shape: (batch, 8732)\n",
    "        n_hard_negatives = self.neg_pos_ratio*n_positives\n",
    "        \n",
    "        ## find cross entropy for all predictions\n",
    "        conf_loss_all = self.cross_entropy(\n",
    "            predicted_scores.view(-1, n_classes), \n",
    "            true_classes.view(-1)\n",
    "        ).view(batch_size, n_priors) ## (batch_size, 8732)\n",
    "        \n",
    "        \n",
    "        conf_loss_pos = conf_loss_all[positive_priors]\n",
    "        \n",
    "        conf_loss_neg = conf_loss_all.clone()\n",
    "        conf_loss_neg[positive_priors] = 0\n",
    "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)\n",
    "        \n",
    "        # this is basically masking out the n_hard_negatives from each image in the batch, \n",
    "        # we could've done it in a easier way\n",
    "        hardness_rank = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)\n",
    "        hard_negatives = hardness_rank < n_hard_negatives.unsqueeze(dim=1)\n",
    "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]\n",
    "        \n",
    "        conf_loss = (conf_loss_hard_neg.sum()+conf_loss_pos.sum())/n_positives.sum().float()\n",
    "        \n",
    "        total_loss = conf_loss+self.alpha*loc_loss\n",
    "        \n",
    "        if (total_loss!=total_loss).any():\n",
    "            file_name = \"ERR.txt\"\n",
    "            print(\"MODEL PARAMS\", file=open(file_name, \"a\"))\n",
    "            print(model.named_parameters(), file=open(file_name, \"a\"))\n",
    "            print(\"LOC LOSS\", file=open(file_name, \"a\"))\n",
    "            print(loc_loss, file=open(file_name, \"a\"))\n",
    "            print(\"CONF LOSS ALL\", file=open(file_name, \"a\"))\n",
    "            print(conf_loss_all, file=open(file_name, \"a\"))\n",
    "            print(\"CONF LOSS POS\", file=open(file_name, \"a\"))\n",
    "            print(conf_loss_pos, file=open(file_name, \"a\"))\n",
    "            print(\"CONF LOSS HARD NEG\", file=open(file_name, \"a\"))\n",
    "            print(conf_loss_hard_neg, file=open(file_name, \"a\"))\n",
    "            print(\"TRUE LOCS\", file=open(file_name, \"a\"))\n",
    "            print(true_locs, file=open(file_name, \"a\"))\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"./\"\n",
    "keep_difficult = True\n",
    "\n",
    "n_classes = len(label_map)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint =\"checkpoint_ssd300.pth.tar\"\n",
    "batch_size = 16\n",
    "iterations = 120000\n",
    "num_workers = 7\n",
    "pin_memory = False\n",
    "print_freq = 200\n",
    "lr = 5e-4\n",
    "decay_lr_at = [40000, 80000,100000]\n",
    "decay_lr_to = 0.5\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "grad_clip = 0.5\n",
    "\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "history = pd.DataFrame()\n",
    "all_epoch_losses = []\n",
    "\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    l=0.0\n",
    "    ll = []\n",
    "    for i, (images, boxes, labels, _) in enumerate(tqdm(train_loader)):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        boxes = [b.to(device) for b in boxes]\n",
    "        labels = [l.to(device) for l in labels]\n",
    "        \n",
    "        predicted_locs, predicted_scores = model(images)\n",
    "        \n",
    "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)\n",
    "        l+=loss.item()        \n",
    "\n",
    "        ll.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        plot_grad_flow(model.named_parameters())\n",
    "        \n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "    del predicted_locs, predicted_scores, images, boxes, labels\n",
    "    plt.savefig(\"./grads/EPOCH_\"+str(epoch)+'.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    all_epoch_losses.append(ll)\n",
    "    return l/len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Training\n",
    "    \"\"\"\n",
    "    \n",
    "    global start_epoch, label_map, epoch, checkpoint, decay_lr_at, history, model\n",
    "    \n",
    "    model = SSD300(n_classes=n_classes)\n",
    "    \n",
    "    if checkpoint is None:\n",
    "        start_epoch = 0    \n",
    "        biases = list()\n",
    "        not_biases = list()\n",
    "        \n",
    "        for param_name, param in model.named_parameters():\n",
    "            if param_name.endswith(\".bias\"):\n",
    "                biases.append(param)\n",
    "            else:\n",
    "                not_biases.append(param)\n",
    "        \n",
    "        optimizer = mu.RAdam(params=[{'params':biases, 'lr':2*lr}, {'params':not_biases}], lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        ## FOR RESUMING TRAINING\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch']+1\n",
    "        print(\"\\nLoaded checkpoint from epoch {}.\\n\".format(start_epoch))\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state'])\n",
    "        optimizer = checkpoint['optimizer']\n",
    "    \n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\n",
    "    \n",
    "    train_dataset = PascalVOCDataset(data_folder, split='train', keep_diffcult=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=train_dataset.collate_fn,\n",
    "                              shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    \n",
    "    epochs = 1+iterations//(len(train_dataset)//batch_size)\n",
    "    decay_lr_at = [it//(len(train_dataset)//batch_size) for it in decay_lr_at]\n",
    "    decay_lr_at.append(86)\n",
    "    \n",
    "    st = time.time()\n",
    "    \n",
    "    print(\"Training For: {} epochs\".format(epochs))\n",
    "    print(\"Decay Learning Rate AT: \", decay_lr_at)\n",
    "    \n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        st = time.time()\n",
    "        if epoch in decay_lr_at:\n",
    "            adjust_learning_rate(optimizer, decay_lr_to)\n",
    "        \n",
    "        loss = train(train_loader=train_loader, model=model, criterion=criterion, optimizer=optimizer, epoch=epoch)\n",
    "        \n",
    "        mu.print_epoch_stat(epoch, time.time()-st, train_loss=loss, history=history)\n",
    "        save_checkpoint(epoch, model, optimizer)\n",
    "        mu.clear_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decimating Tensor of shape:  torch.Size([4096, 512, 7, 7])\n",
      "Decimated to Tensor of shape:  torch.Size([1024, 512, 3, 3])\n",
      "Decimating Tensor of shape:  torch.Size([4096])\n",
      "Decimated to Tensor of shape:  torch.Size([1024])\n",
      "Decimating Tensor of shape:  torch.Size([4096, 4096, 1, 1])\n",
      "Decimated to Tensor of shape:  torch.Size([1024, 1024, 1, 1])\n",
      "Decimating Tensor of shape:  torch.Size([4096])\n",
      "Decimated to Tensor of shape:  torch.Size([1024])\n",
      "BASE MODEL LOAD....COMPLETE\n",
      "\n",
      "\n",
      "Loaded checkpoint from epoch 85.\n",
      "\n",
      "Training For: 117 epochs\n",
      "Decay Learning Rate AT:  [38, 77, 96, 86]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c1ec4502fe48d5a72c88efb530ad21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 86 Completed, Time Taken: 0:06:53.363753\n",
      "\tTrain Loss \t3.4438235\n",
      "DECAYING LR\n",
      "New LR is: 6.25e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a2f6ba9eee4fff84b8f8e79df6d351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 87 Completed, Time Taken: 0:06:48.998646\n",
      "\tTrain Loss \t3.44433369\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e83726eb8574e138b511ccc509c0392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 88 Completed, Time Taken: 0:06:50.583992\n",
      "\tTrain Loss \t3.42336783\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df33730ea3044f83b0c40c01a4f1d795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 89 Completed, Time Taken: 0:06:50.450069\n",
      "\tTrain Loss \t3.40572397\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d51b30b114485990335c5354d05cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 90 Completed, Time Taken: 0:06:49.632272\n",
      "\tTrain Loss \t3.40036188\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a4f15078a846a085d3e4d648e59513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 91 Completed, Time Taken: 0:06:49.604374\n",
      "\tTrain Loss \t3.38883025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d094a146e9ef46e0919e7313d37062bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 92 Completed, Time Taken: 0:06:49.730453\n",
      "\tTrain Loss \t3.50369565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f182eff568b3414f90d02ab755386c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 93 Completed, Time Taken: 0:06:49.901212\n",
      "\tTrain Loss \t3.42834945\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5664b1d30b454b4fb32f65192100f6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 94 Completed, Time Taken: 0:06:49.569247\n",
      "\tTrain Loss \t3.41634442\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5524fc8214cc4966a693b4a61cc410ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EPOCH 95 Completed, Time Taken: 0:06:49.600972\n",
      "\tTrain Loss \t3.3999271\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4c0932212e4460ac6b6708640a8710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1035.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-75:\n",
      "Process Process-74:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f5289bd0ef0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/home/numan947/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-f2d7325adf8d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_lr_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_epoch_stat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-76d3b8a127a9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mplot_grad_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrad_clip\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-826f54520235>\u001b[0m in \u001b[0;36mplot_grad_flow\u001b[0;34m(named_parameters)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mave_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mave_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mave_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"k\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mave_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"vertical\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2794\u001b[0m     return gca().plot(\n\u001b[1;32m   2795\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2796\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1665\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoscale_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36madd_line\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_line%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1922\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m         \"\"\"\n\u001b[0;32m-> 1924\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mget_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \"\"\"\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mrecache\u001b[0;34m(self, always)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0myconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_yorig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import PrettyPrinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decimating Tensor of shape:  torch.Size([4096, 512, 7, 7])\n",
      "Decimated to Tensor of shape:  torch.Size([1024, 512, 3, 3])\n",
      "Decimating Tensor of shape:  torch.Size([4096])\n",
      "Decimated to Tensor of shape:  torch.Size([1024])\n",
      "Decimating Tensor of shape:  torch.Size([4096, 4096, 1, 1])\n",
      "Decimated to Tensor of shape:  torch.Size([1024, 1024, 1, 1])\n",
      "Decimating Tensor of shape:  torch.Size([4096])\n",
      "Decimated to Tensor of shape:  torch.Size([1024])\n",
      "BASE MODEL LOAD....COMPLETE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoint_ssd300.pth.tar\"\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model = SSD300(n_classes)\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PascalVOCDataset(data_folder, split=\"test\", keep_diffcult=keep_difficult)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, \n",
    "                                        shuffle=False, collate_fn=test_dataset.collate_fn,\n",
    "                                          num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader, model):\n",
    "    model.eval()\n",
    "    \n",
    "    det_boxes = list()\n",
    "    det_labels = list()\n",
    "    det_scores = list()\n",
    "    true_boxes = list()\n",
    "    true_labels = list()\n",
    "    true_difficulties = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, boxes, labels, difficulties) in enumerate(tqdm(test_loader, desc='Evaluating')):\n",
    "            images = images.to(device)\n",
    "            predicted_locs, predicted_scores = model(images)\n",
    "            det_boxes_batch, det_labels_batch, det_scores_batch = model.detect_objects(predicted_locs, predicted_scores,\n",
    "                                                                                      min_score=0.01,\n",
    "                                                                                      max_overlap=0.45,\n",
    "                                                                                      top_k=200)\n",
    "            boxes = [b.to(device) for b in boxes]\n",
    "            labels = [l.to(device) for l in labels]\n",
    "            difficulties = [d.to(device) for d in difficulties]\n",
    "            \n",
    "            det_boxes.extend(det_boxes_batch)\n",
    "            det_labels.extend(det_labels_batch)\n",
    "            det_scores.extend(det_scores_batch)\n",
    "            \n",
    "            true_boxes.extend(boxes)\n",
    "            true_labels.extend(labels)\n",
    "            true_difficulties.extend(difficulties)\n",
    "        \n",
    "        APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties)\n",
    "    \n",
    "    pp.pprint(APs)\n",
    "    print(\"\\nMean Average Precision (mAP): {}\".format(mAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6de7ea666546f0bdaaf094f5905231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=310.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detect.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "def detect(model, original_image, min_score, max_overlap, top_k, suppress=None):\n",
    "    resize = transforms.Resize((300,300))\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    normalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    \n",
    "    image = normalize(to_tensor(resize(original_image)))\n",
    "    image = image.to(device)\n",
    "    \n",
    "    predicted_locs, predicted_scores = model(image.unsqueeze(0))\n",
    "    \n",
    "    det_boxes, det_labels, det_scores = model.detect_objects(\n",
    "        predicted_locs, predicted_scores,min_score=min_score, max_overlap=max_overlap, top_k=top_k)\n",
    "    det_boxes = det_boxes[0].to('cpu')\n",
    "    \n",
    "    original_dims = torch.FloatTensor([original_image.width, original_image.height, original_image.width, original_image.height]).unsqueeze(0)\n",
    "    det_boxes = det_boxes * original_dims\n",
    "    \n",
    "    det_labels = [rev_label_map[l] for l in det_labels[0].to('cpu').tolist()]\n",
    "    \n",
    "    if det_labels == ['background']:\n",
    "        return original_image\n",
    "    \n",
    "    annotated_image = original_image\n",
    "    draw = ImageDraw.Draw(annotated_image)\n",
    "    font = ImageFont.truetype(\"./calibri.ttf\", 15)\n",
    "    \n",
    "    \n",
    "    for i in range(det_boxes.size(0)):\n",
    "        if suppress is not None:\n",
    "            if det_labels[i] in suppress:\n",
    "                continue\n",
    "        \n",
    "        box_location = det_boxes[i].tolist()\n",
    "        \n",
    "        draw.rectangle(xy=box_location, outline=label_color_map[det_labels[i]])\n",
    "        draw.rectangle(xy=[l+1.0 for l in box_location], outline=label_color_map[det_labels[i]])\n",
    "        \n",
    "        text_size = font.getsize(det_labels[i].upper())\n",
    "        text_location = [box_location[0]+2., box_location[1]-text_size[1]]\n",
    "        textbox_location = [box_location[0], box_location[1]-text_size[1],\n",
    "                            box_location[0]+text_size[0]+4., box_location[1]]\n",
    "        \n",
    "        draw.rectangle(xy=textbox_location, fill=label_color_map[det_labels[i]])\n",
    "        draw.text(xy=text_location, text=det_labels[i].upper(), fill='white', font=font)\n",
    "        print(det_labels[i])\n",
    "    \n",
    "    del draw\n",
    "    \n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./train/VOC2007/JPEGImages/000012.jpg\"\n",
    "original_image = Image.open(image_path, mode='r')\n",
    "original_image = original_image.convert('RGB')\n",
    "\n",
    "detect(model, original_image, min_score=0.25, max_overlap=0.5, top_k=200).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"A\".upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TESTING FACILITIES###\n",
    "# mu.clear_cuda()\n",
    "# train_dataset = PascalVOCDataset('./', split='train', keep_diffcult=True)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=3, collate_fn=train_dataset.collate_fn, num_workers=4, pin_memory=False)\n",
    "# vggbase = VGGBase()\n",
    "# auxconv = AuxiliaryConvolutions()\n",
    "# prdconv = PredictionConvolutions(len(label_map))\n",
    "# model = SSD300(len(label_map))\n",
    "# model = model.to(device)\n",
    "# criterion = MultiBoxLoss(model.priors_cxcy).to(device)\n",
    "# opt = optim.Adam(model.parameters())\n",
    "# for i, (images, boxes, labels, _) in enumerate(train_loader):\n",
    "#     images = images.to(device)\n",
    "# #     print(images.shape)\n",
    "# #     print(len(boxes))\n",
    "# #     print(len(labels))\n",
    "# #     print(len(_))\n",
    "# #     out4_3, out7 = vggbase(images)\n",
    "# #     print(out4_3.shape)\n",
    "# #     print(out7.shape)\n",
    "# #     out8_2, out9_2, out10_2, out11_2 = auxconv(out7)\n",
    "# #     print(out8_2.shape)\n",
    "# #     print(out9_2.shape)\n",
    "# #     print(out10_2.shape)\n",
    "# #     print(out11_2.shape)\n",
    "#     opt.zero_grad()\n",
    "#     locs, class_scores = model(images)\n",
    "# #     print(locs.shape)\n",
    "# #     print(class_scores.shape)\n",
    "#     boxes = [b.to(device) for b in boxes]\n",
    "#     labels = [l.to(device) for l in labels]\n",
    "#     loss = criterion(locs, class_scores, boxes, labels)\n",
    "#     loss.backward()\n",
    "#     opt.step()\n",
    "#     print(loss.item())\n",
    "    \n",
    "#     if i==3:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
