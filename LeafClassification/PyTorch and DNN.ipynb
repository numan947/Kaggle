{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL IMPORTS FOR A NEW NOTEBOOK\n",
    "\n",
    "import os, sys, random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import itertools as it\n",
    "import scipy\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.optim import Optimizer\n",
    "import torchvision.transforms.transforms as txf\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch_utils\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=\"mle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 3\n",
    "RSZ = 96\n",
    "MULT = 2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_utils.seed_everything(947)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv(\"./train.csv\") \n",
    "raw_test = pd.read_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((990, 194), (594, 193))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train.shape, raw_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.053711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.044922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   margin1   margin2   margin3   margin4   margin5   margin6   margin7  \\\n",
       "0   4  0.019531  0.009766  0.078125  0.011719  0.003906  0.015625  0.005859   \n",
       "1   7  0.007812  0.005859  0.064453  0.009766  0.003906  0.013672  0.007812   \n",
       "2   9  0.000000  0.000000  0.001953  0.021484  0.041016  0.000000  0.023438   \n",
       "3  12  0.000000  0.000000  0.009766  0.011719  0.017578  0.000000  0.003906   \n",
       "4  13  0.001953  0.000000  0.015625  0.009766  0.039062  0.000000  0.009766   \n",
       "\n",
       "   margin8   margin9  ...  texture55  texture56  texture57  texture58  \\\n",
       "0      0.0  0.005859  ...   0.006836   0.000000   0.015625   0.000977   \n",
       "1      0.0  0.033203  ...   0.000000   0.000000   0.006836   0.001953   \n",
       "2      0.0  0.011719  ...   0.128910   0.000000   0.000977   0.000000   \n",
       "3      0.0  0.003906  ...   0.012695   0.015625   0.002930   0.036133   \n",
       "4      0.0  0.005859  ...   0.000000   0.042969   0.016602   0.010742   \n",
       "\n",
       "   texture59  texture60  texture61  texture62  texture63  texture64  \n",
       "0   0.015625        0.0        0.0   0.000000   0.003906   0.053711  \n",
       "1   0.013672        0.0        0.0   0.000977   0.037109   0.044922  \n",
       "2   0.000000        0.0        0.0   0.015625   0.000000   0.000000  \n",
       "3   0.013672        0.0        0.0   0.089844   0.000000   0.008789  \n",
       "4   0.041016        0.0        0.0   0.007812   0.009766   0.007812  \n",
       "\n",
       "[5 rows x 193 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Acer_Opalus</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pterocarya_Stenoptera</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quercus_Hartwissiana</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Tilia_Tomentosa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Quercus_Variabilis</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                species   margin1   margin2   margin3   margin4  \\\n",
       "0   1            Acer_Opalus  0.007812  0.023438  0.023438  0.003906   \n",
       "1   2  Pterocarya_Stenoptera  0.005859  0.000000  0.031250  0.015625   \n",
       "2   3   Quercus_Hartwissiana  0.005859  0.009766  0.019531  0.007812   \n",
       "3   5        Tilia_Tomentosa  0.000000  0.003906  0.023438  0.005859   \n",
       "4   6     Quercus_Variabilis  0.005859  0.003906  0.048828  0.009766   \n",
       "\n",
       "    margin5   margin6   margin7  margin8  ...  texture55  texture56  \\\n",
       "0  0.011719  0.009766  0.027344      0.0  ...   0.007812   0.000000   \n",
       "1  0.025391  0.001953  0.019531      0.0  ...   0.000977   0.000000   \n",
       "2  0.003906  0.005859  0.068359      0.0  ...   0.154300   0.000000   \n",
       "3  0.021484  0.019531  0.023438      0.0  ...   0.000000   0.000977   \n",
       "4  0.013672  0.015625  0.005859      0.0  ...   0.096680   0.000000   \n",
       "\n",
       "   texture57  texture58  texture59  texture60  texture61  texture62  \\\n",
       "0   0.002930   0.002930   0.035156        0.0        0.0   0.004883   \n",
       "1   0.000000   0.000977   0.023438        0.0        0.0   0.000977   \n",
       "2   0.005859   0.000977   0.007812        0.0        0.0   0.000000   \n",
       "3   0.000000   0.000000   0.020508        0.0        0.0   0.017578   \n",
       "4   0.021484   0.000000   0.000000        0.0        0.0   0.000000   \n",
       "\n",
       "   texture63  texture64  \n",
       "0   0.000000   0.025391  \n",
       "1   0.039062   0.022461  \n",
       "2   0.020508   0.002930  \n",
       "3   0.000000   0.047852  \n",
       "4   0.000000   0.031250  \n",
       "\n",
       "[5 rows x 194 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 194), (99, 194))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, valid = train_test_split(raw_train,test_size=0.1, stratify=raw_train.species, random_state=947)\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components='mle', random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(train.drop([\"id\", \"species\"], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "le = LabelEncoder().fit(train.species)\n",
    "\n",
    "train_labels = le.transform(train.species)\n",
    "valid_labels = le.transform(valid.species)\n",
    "\n",
    "train = train.drop([\"species\"], axis=1).values\n",
    "valid = valid.drop([\"species\"], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, X, y=None, tx=None, pca=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tx = tx\n",
    "        self.pca = pca\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        current_image_X = self.X[idx]\n",
    "        \n",
    "        image_path = \"./images/\"+str(int(current_image_X[0]))+\".jpg\"\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.tx is not None:\n",
    "            image = self.tx(image)\n",
    "        \n",
    "#         print(self.pca.transform(current_image_X[1:].reshape(1,-1)).shape)\n",
    "        if self.y is not None:\n",
    "            return (image, torch.Tensor(self.pca.transform(current_image_X[1:].reshape(1,-1)))), self.y[idx]\n",
    "        \n",
    "        return (image, torch.Tensor(self.pca.transform(current_image_X[1:].reshape(1,-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = txf.Compose([\n",
    "    txf.Resize((RSZ, RSZ)),\n",
    "    txf.RandomRotation(10),\n",
    "    txf.RandomAffine(degrees=7, translate=(0.3,0.3), scale=(0.8, 1.2), shear=7,),\n",
    "    txf.ToTensor(),\n",
    "    txf.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = txf.Compose([\n",
    "    txf.Resize((RSZ, RSZ)),\n",
    "    txf.ToTensor(),\n",
    "    txf.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = MyDataSet(train, train_labels, train_transforms, pca)\n",
    "dataset_valid = MyDataSet(valid, valid_labels, test_transforms, pca)\n",
    "dataset_test = MyDataSet(raw_test.values, tx=test_transforms, pca=pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "valid_loader = DataLoader(dataset_valid, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self, n_classes, drop_prob=0.3):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch_utils.CNNLayer(1, MULT*32)\n",
    "        self.conv2 = torch_utils.CNNLayer(MULT*32, MULT*32)\n",
    "        \n",
    "        self.conv3 = torch_utils.CNNLayer(MULT*32, MULT*64)\n",
    "        self.conv4 = torch_utils.CNNLayer(MULT*64, MULT*64)\n",
    "        \n",
    "        self.conv5 = torch_utils.CNNLayer(MULT*64, MULT*128)\n",
    "        self.conv6 = torch_utils.CNNLayer(MULT*128, MULT*128)\n",
    "        \n",
    "        self.conv7 = torch_utils.CNNLayer(MULT*128, MULT*256)\n",
    "        self.conv8 = torch_utils.CNNLayer(MULT*256, MULT*256)\n",
    "        \n",
    "        self.conv9 = torch_utils.CNNLayer(MULT*256, MULT*512)\n",
    "        self.conv10 = torch_utils.CNNLayer(MULT*512, MULT*512)\n",
    "        \n",
    "        self.fc1 = torch_utils.FCLayer(191+(MULT*512*4*4), MULT*512)\n",
    "        self.fc2 = torch_utils.FCLayer(MULT*512, MULT*512)\n",
    "        self.fc3 = torch_utils.FCLayer(MULT*512, MULT*512)\n",
    "        self.fc4 = torch_utils.FCLayer(MULT*512, MULT*512)\n",
    "        \n",
    "        self.out = torch_utils.FCLayer(MULT*512, n_classes)\n",
    "        \n",
    "        self.drop1d = nn.Dropout(p=drop_prob)\n",
    "        self.maxpool = nn.MaxPool2d(2, 2) \n",
    "        self.drop = nn.Dropout2d(p=drop_prob)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, xx = x\n",
    "        xx = xx.squeeze(dim=1)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.conv9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.drop(x)\n",
    "#         print(x.shape)\n",
    "#         print(xx.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.cat([xx, x], dim=1)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, optimizer, criterion, current_epoch, train_loader, device=\"cpu\", print_interval=10, custom_txf=None, one_hot=False):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    train_correct = 0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        len_data = len(data[0])\n",
    "        len_dataset = len(train_loader.dataset)\n",
    "        len_loader = len(train_loader)\n",
    "        \n",
    "        \n",
    "        data, target = (data[0].to(device), data[1].to(device)), target.to(device)\n",
    "        \n",
    "        if custom_txf is not None:\n",
    "            data, target = custom_txf(data, target)\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        if not one_hot:\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            train_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        else:\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            train_correct += pred.eq(target.max(1, keepdim=True)[1].data.view_as(pred)).cpu().sum().numpy()\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        train_loss+= (loss.item() * len_data)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % print_interval == 0:\n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{} ({:.8f}%)]\\tLoss: {:.8f}'.format(\n",
    "                    current_epoch, batch_idx * len_data, len_dataset,100. * batch_idx / len_loader, loss.item()\n",
    "                    )\n",
    "                )\n",
    "    ## This is training, so reduction = mean, i.e. loss.item() already gives the mean of the batch\n",
    "    train_loss/=len(train_loader.dataset)\n",
    "    train_accuracy = 100. * train_correct / len(train_loader.dataset)\n",
    "    print('Train Set: Average loss: {:.8f}, Accuracy: {}/{} ({:.8f}%)'.format(\n",
    "        train_loss, train_correct, len(train_loader.dataset), train_accuracy\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return train_loss, train_accuracy\n",
    "    \n",
    "    \n",
    "    \n",
    "def test_classifier(model, criterion, device, test_loader, one_hot=False, tta=False):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = (data[0].to(device), data[1].to(device)), target.to(device)\n",
    "            if tta:\n",
    "                batch_size, n_crops, c, h, w = data.size()\n",
    "                data = data.view(-1, c, h, w)\n",
    "                output = model(data)\n",
    "                output = output.view(batch_size, n_crops, -1).mean(1)\n",
    "            else:\n",
    "                output = model(data)\n",
    "            \n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            if not one_hot:\n",
    "                pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "                test_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            else:\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                test_correct += pred.eq(target.max(1, keepdim=True)[1].data.view_as(pred)).cpu().sum().numpy()\n",
    "\n",
    "\n",
    "    ## validation/test, so reduction = \"sum\"\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * test_correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.8f}, Accuracy: {}/{} ({:.8f}%)\\n'.format(\n",
    "        test_loss, test_correct, len(test_loader.dataset),test_accuracy\n",
    "        )\n",
    "    )\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQENet(nn.Module):\n",
    "    ## Squeeze and excitation net\n",
    "    def __init__(self, n_classes, drop_prob=0.3):\n",
    "        super(SQENet, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch_utils.CNNLayer(1, MULT*32, 5)\n",
    "        self.conv2 = torch_utils.CNNLayer(MULT*32, MULT*32, 5)\n",
    "        \n",
    "        self.conv3 = torch_utils.CNNLayer(MULT*32, MULT*64,5)\n",
    "        self.conv4 = torch_utils.CNNLayer(MULT*64, MULT*64,5)\n",
    "        \n",
    "        self.conv5 = torch_utils.CNNLayer(MULT*64, MULT*128)\n",
    "        self.conv6 = torch_utils.CNNLayer(MULT*128, MULT*128)\n",
    "        \n",
    "        self.conv7 = torch_utils.CNNLayer(MULT*128, MULT*256)\n",
    "        self.conv8 = torch_utils.CNNLayer(MULT*256, MULT*256)\n",
    "        \n",
    "        self.se3 = torch_utils.Sq_Ex_Block(in_ch=MULT*256,r=8)\n",
    "        \n",
    "        self.fc1 = torch_utils.FCLayer(191+(MULT*256*8*8), 256)\n",
    "        self.fc2 = torch_utils.FCLayer(256, n_classes)\n",
    "        \n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2, 2) \n",
    "        self.drop = nn.Dropout2d(p=drop_prob)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, xx = x\n",
    "        xx = xx.squeeze(dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = self.conv8(x)\n",
    "        \n",
    "        x = self.se3(x)\n",
    "        \n",
    "        x = self.maxpool(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "#         print(xx.shape)\n",
    "#         print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.cat([xx, x], dim=1)\n",
    "#         print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMNet(nn.Module):\n",
    "    ## Squeeze and excitation net\n",
    "    def __init__(self, n_classes, drop_prob=0.3):\n",
    "        super(AMNet, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 7, padding=3),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(8, 32, 5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "#             torch_utils.Sq_Ex_Block(in_ch=32,r=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "#             torch_utils.Sq_Ex_Block(in_ch=64,r=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear((12*12*64)+191, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, n_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, xx = x\n",
    "        xx = xx.squeeze(dim=1)\n",
    "        \n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.cat([xx, x], dim=1)\n",
    "        \n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AMNet(len(le.classes_)).to(device)\n",
    "n_epochs = 500\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch_utils.RAdam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, n_epochs//4, gamma=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01\n",
      "Train Epoch: 1 [0/891 (0.00000000%)]\tLoss: 5.03411865\n",
      "Train Epoch: 1 [320/891 (35.71428571%)]\tLoss: 5.09584045\n",
      "Train Epoch: 1 [640/891 (71.42857143%)]\tLoss: 4.75775957\n",
      "Train Set: Average loss: 4.86870348, Accuracy: 8/891 (0.89786756%)\n",
      "\n",
      "Test set: Average loss: 4.54028101, Accuracy: 1/99 (1.01010101%)\n",
      "\n",
      "Epoch 1 time: 1.0051159858703613 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 2 [0/891 (0.00000000%)]\tLoss: 4.62733459\n",
      "Train Epoch: 2 [320/891 (35.71428571%)]\tLoss: 4.68065548\n",
      "Train Epoch: 2 [640/891 (71.42857143%)]\tLoss: 4.95789289\n",
      "Train Set: Average loss: 4.79036066, Accuracy: 17/891 (1.90796857%)\n",
      "\n",
      "Test set: Average loss: 4.39998027, Accuracy: 3/99 (3.03030303%)\n",
      "\n",
      "Epoch 2 time: 1.0028033256530762 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 3 [0/891 (0.00000000%)]\tLoss: 4.90903568\n",
      "Train Epoch: 3 [320/891 (35.71428571%)]\tLoss: 4.64941740\n",
      "Train Epoch: 3 [640/891 (71.42857143%)]\tLoss: 4.39744854\n",
      "Train Set: Average loss: 4.61015513, Accuracy: 18/891 (2.02020202%)\n",
      "\n",
      "Test set: Average loss: 4.28784997, Accuracy: 2/99 (2.02020202%)\n",
      "\n",
      "Epoch 3 time: 1.0126678943634033 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 4 [0/891 (0.00000000%)]\tLoss: 4.47870064\n",
      "Train Epoch: 4 [320/891 (35.71428571%)]\tLoss: 4.53494787\n",
      "Train Epoch: 4 [640/891 (71.42857143%)]\tLoss: 4.45312262\n",
      "Train Set: Average loss: 4.49279632, Accuracy: 17/891 (1.90796857%)\n",
      "\n",
      "Test set: Average loss: 4.13405556, Accuracy: 1/99 (1.01010101%)\n",
      "\n",
      "Epoch 4 time: 0.9633243083953857 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 5 [0/891 (0.00000000%)]\tLoss: 4.48685551\n",
      "Train Epoch: 5 [320/891 (35.71428571%)]\tLoss: 4.54154587\n",
      "Train Epoch: 5 [640/891 (71.42857143%)]\tLoss: 4.01383400\n",
      "Train Set: Average loss: 4.33875155, Accuracy: 20/891 (2.24466891%)\n",
      "\n",
      "Test set: Average loss: 4.03732214, Accuracy: 5/99 (5.05050505%)\n",
      "\n",
      "Epoch 5 time: 0.9979498386383057 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 6 [0/891 (0.00000000%)]\tLoss: 4.11948729\n",
      "Train Epoch: 6 [320/891 (35.71428571%)]\tLoss: 4.32118940\n",
      "Train Epoch: 6 [640/891 (71.42857143%)]\tLoss: 4.19647837\n",
      "Train Set: Average loss: 4.31849188, Accuracy: 14/891 (1.57126824%)\n",
      "\n",
      "Test set: Average loss: 3.99272843, Accuracy: 3/99 (3.03030303%)\n",
      "\n",
      "Epoch 6 time: 1.0778546333312988 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 7 [0/891 (0.00000000%)]\tLoss: 4.48348141\n",
      "Train Epoch: 7 [320/891 (35.71428571%)]\tLoss: 4.21329784\n",
      "Train Epoch: 7 [640/891 (71.42857143%)]\tLoss: 4.17921162\n",
      "Train Set: Average loss: 4.32635728, Accuracy: 20/891 (2.24466891%)\n",
      "\n",
      "Test set: Average loss: 4.17066699, Accuracy: 3/99 (3.03030303%)\n",
      "\n",
      "Epoch 7 time: 0.9800786972045898 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 8 [0/891 (0.00000000%)]\tLoss: 4.34008408\n",
      "Train Epoch: 8 [320/891 (35.71428571%)]\tLoss: 4.24575758\n",
      "Train Epoch: 8 [640/891 (71.42857143%)]\tLoss: 4.41140795\n",
      "Train Set: Average loss: 4.21974668, Accuracy: 24/891 (2.69360269%)\n",
      "\n",
      "Test set: Average loss: 3.86944808, Accuracy: 2/99 (2.02020202%)\n",
      "\n",
      "Epoch 8 time: 0.9890575408935547 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 9 [0/891 (0.00000000%)]\tLoss: 4.22053719\n",
      "Train Epoch: 9 [320/891 (35.71428571%)]\tLoss: 4.13710546\n",
      "Train Epoch: 9 [640/891 (71.42857143%)]\tLoss: 4.15768385\n",
      "Train Set: Average loss: 4.19930314, Accuracy: 28/891 (3.14253648%)\n",
      "\n",
      "Test set: Average loss: 3.76139610, Accuracy: 7/99 (7.07070707%)\n",
      "\n",
      "Epoch 9 time: 1.0204675197601318 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 10 [0/891 (0.00000000%)]\tLoss: 3.89553809\n",
      "Train Epoch: 10 [320/891 (35.71428571%)]\tLoss: 4.09501457\n",
      "Train Epoch: 10 [640/891 (71.42857143%)]\tLoss: 4.04160738\n",
      "Train Set: Average loss: 4.14008820, Accuracy: 44/891 (4.93827160%)\n",
      "\n",
      "Test set: Average loss: 3.88664211, Accuracy: 8/99 (8.08080808%)\n",
      "\n",
      "Epoch 10 time: 1.037541151046753 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 11 [0/891 (0.00000000%)]\tLoss: 3.70825481\n",
      "Train Epoch: 11 [320/891 (35.71428571%)]\tLoss: 4.24258709\n",
      "Train Epoch: 11 [640/891 (71.42857143%)]\tLoss: 4.23834562\n",
      "Train Set: Average loss: 4.10423734, Accuracy: 42/891 (4.71380471%)\n",
      "\n",
      "Test set: Average loss: 3.65960539, Accuracy: 7/99 (7.07070707%)\n",
      "\n",
      "Epoch 11 time: 1.0059235095977783 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 12 [0/891 (0.00000000%)]\tLoss: 3.98353553\n",
      "Train Epoch: 12 [320/891 (35.71428571%)]\tLoss: 3.83314085\n",
      "Train Epoch: 12 [640/891 (71.42857143%)]\tLoss: 4.08851719\n",
      "Train Set: Average loss: 4.01954315, Accuracy: 49/891 (5.49943883%)\n",
      "\n",
      "Test set: Average loss: 3.70515386, Accuracy: 12/99 (12.12121212%)\n",
      "\n",
      "Epoch 12 time: 1.0079262256622314 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 13 [0/891 (0.00000000%)]\tLoss: 4.00542974\n",
      "Train Epoch: 13 [320/891 (35.71428571%)]\tLoss: 4.36459827\n",
      "Train Epoch: 13 [640/891 (71.42857143%)]\tLoss: 4.04591131\n",
      "Train Set: Average loss: 4.02830465, Accuracy: 44/891 (4.93827160%)\n",
      "\n",
      "Test set: Average loss: 3.70681495, Accuracy: 5/99 (5.05050505%)\n",
      "\n",
      "Epoch 13 time: 0.984304666519165 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 14 [0/891 (0.00000000%)]\tLoss: 4.35307074\n",
      "Train Epoch: 14 [320/891 (35.71428571%)]\tLoss: 3.94569921\n",
      "Train Epoch: 14 [640/891 (71.42857143%)]\tLoss: 4.00874805\n",
      "Train Set: Average loss: 3.95208708, Accuracy: 45/891 (5.05050505%)\n",
      "\n",
      "Test set: Average loss: 3.89581788, Accuracy: 6/99 (6.06060606%)\n",
      "\n",
      "Epoch 14 time: 0.9865555763244629 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 15 [0/891 (0.00000000%)]\tLoss: 4.09492207\n",
      "Train Epoch: 15 [320/891 (35.71428571%)]\tLoss: 3.93612528\n",
      "Train Epoch: 15 [640/891 (71.42857143%)]\tLoss: 3.87321091\n",
      "Train Set: Average loss: 3.90052338, Accuracy: 45/891 (5.05050505%)\n",
      "\n",
      "Test set: Average loss: 3.33640794, Accuracy: 13/99 (13.13131313%)\n",
      "\n",
      "Epoch 15 time: 1.0234887599945068 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 16 [0/891 (0.00000000%)]\tLoss: 3.87948203\n",
      "Train Epoch: 16 [320/891 (35.71428571%)]\tLoss: 3.99463463\n",
      "Train Epoch: 16 [640/891 (71.42857143%)]\tLoss: 3.90237832\n",
      "Train Set: Average loss: 3.87474974, Accuracy: 49/891 (5.49943883%)\n",
      "\n",
      "Test set: Average loss: 3.81260032, Accuracy: 5/99 (5.05050505%)\n",
      "\n",
      "Epoch 16 time: 0.999908447265625 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 17 [0/891 (0.00000000%)]\tLoss: 3.67582107\n",
      "Train Epoch: 17 [320/891 (35.71428571%)]\tLoss: 3.92879915\n",
      "Train Epoch: 17 [640/891 (71.42857143%)]\tLoss: 4.23799372\n",
      "Train Set: Average loss: 3.83053707, Accuracy: 57/891 (6.39730640%)\n",
      "\n",
      "Test set: Average loss: 3.32105492, Accuracy: 14/99 (14.14141414%)\n",
      "\n",
      "Epoch 17 time: 1.001509428024292 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 18 [0/891 (0.00000000%)]\tLoss: 3.85318327\n",
      "Train Epoch: 18 [320/891 (35.71428571%)]\tLoss: 4.02205038\n",
      "Train Epoch: 18 [640/891 (71.42857143%)]\tLoss: 3.58435488\n",
      "Train Set: Average loss: 3.81954835, Accuracy: 58/891 (6.50953984%)\n",
      "\n",
      "Test set: Average loss: 3.20513905, Accuracy: 12/99 (12.12121212%)\n",
      "\n",
      "Epoch 18 time: 0.9994869232177734 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 19 [0/891 (0.00000000%)]\tLoss: 3.55766344\n",
      "Train Epoch: 19 [320/891 (35.71428571%)]\tLoss: 3.61612225\n",
      "Train Epoch: 19 [640/891 (71.42857143%)]\tLoss: 3.71054673\n",
      "Train Set: Average loss: 3.73135004, Accuracy: 59/891 (6.62177329%)\n",
      "\n",
      "Test set: Average loss: 3.22794751, Accuracy: 16/99 (16.16161616%)\n",
      "\n",
      "Epoch 19 time: 1.0126521587371826 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 20 [0/891 (0.00000000%)]\tLoss: 3.62258172\n",
      "Train Epoch: 20 [320/891 (35.71428571%)]\tLoss: 3.75469279\n",
      "Train Epoch: 20 [640/891 (71.42857143%)]\tLoss: 4.02468777\n",
      "Train Set: Average loss: 3.77530537, Accuracy: 73/891 (8.19304153%)\n",
      "\n",
      "Test set: Average loss: 3.99663971, Accuracy: 6/99 (6.06060606%)\n",
      "\n",
      "Epoch 20 time: 1.0290091037750244 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 21 [0/891 (0.00000000%)]\tLoss: 3.68401647\n",
      "Train Epoch: 21 [320/891 (35.71428571%)]\tLoss: 3.80001307\n",
      "Train Epoch: 21 [640/891 (71.42857143%)]\tLoss: 3.63398361\n",
      "Train Set: Average loss: 3.70271840, Accuracy: 58/891 (6.50953984%)\n",
      "\n",
      "Test set: Average loss: 3.09295960, Accuracy: 22/99 (22.22222222%)\n",
      "\n",
      "Epoch 21 time: 1.1030502319335938 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 22 [0/891 (0.00000000%)]\tLoss: 3.55288267\n",
      "Train Epoch: 22 [320/891 (35.71428571%)]\tLoss: 3.60691929\n",
      "Train Epoch: 22 [640/891 (71.42857143%)]\tLoss: 3.69354916\n",
      "Train Set: Average loss: 3.65701329, Accuracy: 73/891 (8.19304153%)\n",
      "\n",
      "Test set: Average loss: 3.22524145, Accuracy: 12/99 (12.12121212%)\n",
      "\n",
      "Epoch 22 time: 1.0186395645141602 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 23 [0/891 (0.00000000%)]\tLoss: 3.46788836\n",
      "Train Epoch: 23 [320/891 (35.71428571%)]\tLoss: 3.49098086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 23 [640/891 (71.42857143%)]\tLoss: 3.54794312\n",
      "Train Set: Average loss: 3.57963038, Accuracy: 76/891 (8.52974186%)\n",
      "\n",
      "Test set: Average loss: 3.69693558, Accuracy: 8/99 (8.08080808%)\n",
      "\n",
      "Epoch 23 time: 1.0375092029571533 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 24 [0/891 (0.00000000%)]\tLoss: 3.46544290\n",
      "Train Epoch: 24 [320/891 (35.71428571%)]\tLoss: 3.84474349\n",
      "Train Epoch: 24 [640/891 (71.42857143%)]\tLoss: 3.90288472\n",
      "Train Set: Average loss: 3.70425775, Accuracy: 70/891 (7.85634119%)\n",
      "\n",
      "Test set: Average loss: 3.18690763, Accuracy: 11/99 (11.11111111%)\n",
      "\n",
      "Epoch 24 time: 1.0370404720306396 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 25 [0/891 (0.00000000%)]\tLoss: 3.43413568\n",
      "Train Epoch: 25 [320/891 (35.71428571%)]\tLoss: 3.48782229\n",
      "Train Epoch: 25 [640/891 (71.42857143%)]\tLoss: 3.53428125\n",
      "Train Set: Average loss: 3.57431205, Accuracy: 73/891 (8.19304153%)\n",
      "\n",
      "Test set: Average loss: 3.48197386, Accuracy: 10/99 (10.10101010%)\n",
      "\n",
      "Epoch 25 time: 1.0589845180511475 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 26 [0/891 (0.00000000%)]\tLoss: 3.57695150\n",
      "Train Epoch: 26 [320/891 (35.71428571%)]\tLoss: 3.59989762\n",
      "Train Epoch: 26 [640/891 (71.42857143%)]\tLoss: 3.32035446\n",
      "Train Set: Average loss: 3.57497247, Accuracy: 73/891 (8.19304153%)\n",
      "\n",
      "Test set: Average loss: 3.12569498, Accuracy: 15/99 (15.15151515%)\n",
      "\n",
      "Epoch 26 time: 1.007416009902954 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 27 [0/891 (0.00000000%)]\tLoss: 3.54100871\n",
      "Train Epoch: 27 [320/891 (35.71428571%)]\tLoss: 3.55609465\n",
      "Train Epoch: 27 [640/891 (71.42857143%)]\tLoss: 3.60664988\n",
      "Train Set: Average loss: 3.57258733, Accuracy: 77/891 (8.64197531%)\n",
      "\n",
      "Test set: Average loss: 3.16304804, Accuracy: 16/99 (16.16161616%)\n",
      "\n",
      "Epoch 27 time: 1.0127441883087158 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 28 [0/891 (0.00000000%)]\tLoss: 3.32246876\n",
      "Train Epoch: 28 [320/891 (35.71428571%)]\tLoss: 3.30509067\n",
      "Train Epoch: 28 [640/891 (71.42857143%)]\tLoss: 3.21268201\n",
      "Train Set: Average loss: 3.51443269, Accuracy: 87/891 (9.76430976%)\n",
      "\n",
      "Test set: Average loss: 3.48065051, Accuracy: 12/99 (12.12121212%)\n",
      "\n",
      "Epoch 28 time: 1.055497646331787 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 29 [0/891 (0.00000000%)]\tLoss: 3.68274760\n",
      "Train Epoch: 29 [320/891 (35.71428571%)]\tLoss: 3.53375196\n",
      "Train Epoch: 29 [640/891 (71.42857143%)]\tLoss: 3.40992332\n",
      "Train Set: Average loss: 3.50774320, Accuracy: 82/891 (9.20314254%)\n",
      "\n",
      "Test set: Average loss: 2.80735287, Accuracy: 25/99 (25.25252525%)\n",
      "\n",
      "Epoch 29 time: 0.9969708919525146 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 30 [0/891 (0.00000000%)]\tLoss: 3.18347764\n",
      "Train Epoch: 30 [320/891 (35.71428571%)]\tLoss: 3.61654902\n",
      "Train Epoch: 30 [640/891 (71.42857143%)]\tLoss: 3.75765252\n",
      "Train Set: Average loss: 3.44568456, Accuracy: 92/891 (10.32547699%)\n",
      "\n",
      "Test set: Average loss: 3.06026345, Accuracy: 14/99 (14.14141414%)\n",
      "\n",
      "Epoch 30 time: 1.001187801361084 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 31 [0/891 (0.00000000%)]\tLoss: 3.37622714\n",
      "Train Epoch: 31 [320/891 (35.71428571%)]\tLoss: 3.85236907\n",
      "Train Epoch: 31 [640/891 (71.42857143%)]\tLoss: 3.64829016\n",
      "Train Set: Average loss: 3.38488870, Accuracy: 104/891 (11.67227834%)\n",
      "\n",
      "Test set: Average loss: 3.42434312, Accuracy: 13/99 (13.13131313%)\n",
      "\n",
      "Epoch 31 time: 0.9722678661346436 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 32 [0/891 (0.00000000%)]\tLoss: 4.01199293\n",
      "Train Epoch: 32 [320/891 (35.71428571%)]\tLoss: 3.31586933\n",
      "Train Epoch: 32 [640/891 (71.42857143%)]\tLoss: 3.26962304\n",
      "Train Set: Average loss: 3.36425733, Accuracy: 117/891 (13.13131313%)\n",
      "\n",
      "Test set: Average loss: 3.28671642, Accuracy: 16/99 (16.16161616%)\n",
      "\n",
      "Epoch 32 time: 1.0108537673950195 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 33 [0/891 (0.00000000%)]\tLoss: 3.50435758\n",
      "Train Epoch: 33 [320/891 (35.71428571%)]\tLoss: 3.30787039\n",
      "Train Epoch: 33 [640/891 (71.42857143%)]\tLoss: 3.05582595\n",
      "Train Set: Average loss: 3.28723983, Accuracy: 124/891 (13.91694725%)\n",
      "\n",
      "Test set: Average loss: 2.72759261, Accuracy: 26/99 (26.26262626%)\n",
      "\n",
      "Epoch 33 time: 1.0095775127410889 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 34 [0/891 (0.00000000%)]\tLoss: 3.15225983\n",
      "Train Epoch: 34 [320/891 (35.71428571%)]\tLoss: 3.28093338\n",
      "Train Epoch: 34 [640/891 (71.42857143%)]\tLoss: 3.58227682\n",
      "Train Set: Average loss: 3.27585542, Accuracy: 108/891 (12.12121212%)\n",
      "\n",
      "Test set: Average loss: 2.67356951, Accuracy: 26/99 (26.26262626%)\n",
      "\n",
      "Epoch 34 time: 1.0151124000549316 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 35 [0/891 (0.00000000%)]\tLoss: 3.20785403\n",
      "Train Epoch: 35 [320/891 (35.71428571%)]\tLoss: 3.05328059\n",
      "Train Epoch: 35 [640/891 (71.42857143%)]\tLoss: 3.42547250\n",
      "Train Set: Average loss: 3.19484916, Accuracy: 129/891 (14.47811448%)\n",
      "\n",
      "Test set: Average loss: 3.37358683, Accuracy: 17/99 (17.17171717%)\n",
      "\n",
      "Epoch 35 time: 1.0032553672790527 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 36 [0/891 (0.00000000%)]\tLoss: 3.17761660\n",
      "Train Epoch: 36 [320/891 (35.71428571%)]\tLoss: 2.80764151\n",
      "Train Epoch: 36 [640/891 (71.42857143%)]\tLoss: 3.14100599\n",
      "Train Set: Average loss: 3.19313670, Accuracy: 128/891 (14.36588103%)\n",
      "\n",
      "Test set: Average loss: 2.86179667, Accuracy: 20/99 (20.20202020%)\n",
      "\n",
      "Epoch 36 time: 0.9807438850402832 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 37 [0/891 (0.00000000%)]\tLoss: 3.06956577\n",
      "Train Epoch: 37 [320/891 (35.71428571%)]\tLoss: 3.02180052\n",
      "Train Epoch: 37 [640/891 (71.42857143%)]\tLoss: 3.30412817\n",
      "Train Set: Average loss: 3.21682613, Accuracy: 141/891 (15.82491582%)\n",
      "\n",
      "Test set: Average loss: 3.12163844, Accuracy: 17/99 (17.17171717%)\n",
      "\n",
      "Epoch 37 time: 1.0065345764160156 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 38 [0/891 (0.00000000%)]\tLoss: 3.55629945\n",
      "Train Epoch: 38 [320/891 (35.71428571%)]\tLoss: 3.10451508\n",
      "Train Epoch: 38 [640/891 (71.42857143%)]\tLoss: 3.03809643\n",
      "Train Set: Average loss: 3.15312275, Accuracy: 148/891 (16.61054994%)\n",
      "\n",
      "Test set: Average loss: 2.86945046, Accuracy: 19/99 (19.19191919%)\n",
      "\n",
      "Epoch 38 time: 0.9803369045257568 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 39 [0/891 (0.00000000%)]\tLoss: 2.84216857\n",
      "Train Epoch: 39 [320/891 (35.71428571%)]\tLoss: 3.28175139\n",
      "Train Epoch: 39 [640/891 (71.42857143%)]\tLoss: 3.39987111\n",
      "Train Set: Average loss: 3.06284606, Accuracy: 153/891 (17.17171717%)\n",
      "\n",
      "Test set: Average loss: 2.36757968, Accuracy: 36/99 (36.36363636%)\n",
      "\n",
      "Epoch 39 time: 0.9777014255523682 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 40 [0/891 (0.00000000%)]\tLoss: 3.17483187\n",
      "Train Epoch: 40 [320/891 (35.71428571%)]\tLoss: 3.15639782\n",
      "Train Epoch: 40 [640/891 (71.42857143%)]\tLoss: 3.37897611\n",
      "Train Set: Average loss: 3.09866244, Accuracy: 146/891 (16.38608305%)\n",
      "\n",
      "Test set: Average loss: 2.68004608, Accuracy: 27/99 (27.27272727%)\n",
      "\n",
      "Epoch 40 time: 0.9894266128540039 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 41 [0/891 (0.00000000%)]\tLoss: 2.77194405\n",
      "Train Epoch: 41 [320/891 (35.71428571%)]\tLoss: 2.85531044\n",
      "Train Epoch: 41 [640/891 (71.42857143%)]\tLoss: 3.31712627\n",
      "Train Set: Average loss: 3.07297983, Accuracy: 160/891 (17.95735129%)\n",
      "\n",
      "Test set: Average loss: 3.21434617, Accuracy: 17/99 (17.17171717%)\n",
      "\n",
      "Epoch 41 time: 1.029921054840088 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 42 [0/891 (0.00000000%)]\tLoss: 3.33831406\n",
      "Train Epoch: 42 [320/891 (35.71428571%)]\tLoss: 2.80031490\n",
      "Train Epoch: 42 [640/891 (71.42857143%)]\tLoss: 2.96861577\n",
      "Train Set: Average loss: 3.08146699, Accuracy: 154/891 (17.28395062%)\n",
      "\n",
      "Test set: Average loss: 2.53234081, Accuracy: 28/99 (28.28282828%)\n",
      "\n",
      "Epoch 42 time: 1.0015628337860107 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 43 [0/891 (0.00000000%)]\tLoss: 2.83393931\n",
      "Train Epoch: 43 [320/891 (35.71428571%)]\tLoss: 3.06891966\n",
      "Train Epoch: 43 [640/891 (71.42857143%)]\tLoss: 3.05290747\n",
      "Train Set: Average loss: 3.05250334, Accuracy: 145/891 (16.27384961%)\n",
      "\n",
      "Test set: Average loss: 2.34395083, Accuracy: 38/99 (38.38383838%)\n",
      "\n",
      "Epoch 43 time: 1.02378511428833 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 44 [0/891 (0.00000000%)]\tLoss: 2.69498730\n",
      "Train Epoch: 44 [320/891 (35.71428571%)]\tLoss: 2.98564577\n",
      "Train Epoch: 44 [640/891 (71.42857143%)]\tLoss: 2.74662375\n",
      "Train Set: Average loss: 2.98986477, Accuracy: 139/891 (15.60044893%)\n",
      "\n",
      "Test set: Average loss: 2.46715015, Accuracy: 30/99 (30.30303030%)\n",
      "\n",
      "Epoch 44 time: 1.0468437671661377 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 45 [0/891 (0.00000000%)]\tLoss: 3.13850355\n",
      "Train Epoch: 45 [320/891 (35.71428571%)]\tLoss: 2.74900556\n",
      "Train Epoch: 45 [640/891 (71.42857143%)]\tLoss: 3.28181005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: Average loss: 3.04259779, Accuracy: 140/891 (15.71268238%)\n",
      "\n",
      "Test set: Average loss: 3.01576548, Accuracy: 15/99 (15.15151515%)\n",
      "\n",
      "Epoch 45 time: 1.0174920558929443 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 46 [0/891 (0.00000000%)]\tLoss: 3.22969437\n",
      "Train Epoch: 46 [320/891 (35.71428571%)]\tLoss: 2.95890427\n",
      "Train Epoch: 46 [640/891 (71.42857143%)]\tLoss: 2.68653774\n",
      "Train Set: Average loss: 3.03869369, Accuracy: 142/891 (15.93714927%)\n",
      "\n",
      "Test set: Average loss: 2.50843364, Accuracy: 25/99 (25.25252525%)\n",
      "\n",
      "Epoch 46 time: 1.0085420608520508 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 47 [0/891 (0.00000000%)]\tLoss: 3.11671758\n",
      "Train Epoch: 47 [320/891 (35.71428571%)]\tLoss: 3.16825342\n",
      "Train Epoch: 47 [640/891 (71.42857143%)]\tLoss: 2.77821183\n",
      "Train Set: Average loss: 2.96895524, Accuracy: 161/891 (18.06958474%)\n",
      "\n",
      "Test set: Average loss: 2.27510684, Accuracy: 40/99 (40.40404040%)\n",
      "\n",
      "Epoch 47 time: 1.0027339458465576 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 48 [0/891 (0.00000000%)]\tLoss: 2.94077802\n",
      "Train Epoch: 48 [320/891 (35.71428571%)]\tLoss: 3.27222276\n",
      "Train Epoch: 48 [640/891 (71.42857143%)]\tLoss: 2.68577480\n",
      "Train Set: Average loss: 3.00475604, Accuracy: 147/891 (16.49831650%)\n",
      "\n",
      "Test set: Average loss: 2.72015114, Accuracy: 26/99 (26.26262626%)\n",
      "\n",
      "Epoch 48 time: 1.0202443599700928 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 49 [0/891 (0.00000000%)]\tLoss: 3.00390792\n",
      "Train Epoch: 49 [320/891 (35.71428571%)]\tLoss: 2.90295744\n",
      "Train Epoch: 49 [640/891 (71.42857143%)]\tLoss: 2.94507527\n",
      "Train Set: Average loss: 2.95903549, Accuracy: 159/891 (17.84511785%)\n",
      "\n",
      "Test set: Average loss: 2.31608028, Accuracy: 37/99 (37.37373737%)\n",
      "\n",
      "Epoch 49 time: 1.0209877490997314 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 50 [0/891 (0.00000000%)]\tLoss: 2.86007857\n",
      "Train Epoch: 50 [320/891 (35.71428571%)]\tLoss: 2.85011148\n",
      "Train Epoch: 50 [640/891 (71.42857143%)]\tLoss: 2.96325755\n",
      "Train Set: Average loss: 2.86144423, Accuracy: 144/891 (16.16161616%)\n",
      "\n",
      "Test set: Average loss: 2.53315303, Accuracy: 30/99 (30.30303030%)\n",
      "\n",
      "Epoch 50 time: 0.9812922477722168 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 51 [0/891 (0.00000000%)]\tLoss: 2.62844253\n",
      "Train Epoch: 51 [320/891 (35.71428571%)]\tLoss: 3.01336646\n",
      "Train Epoch: 51 [640/891 (71.42857143%)]\tLoss: 3.01536584\n",
      "Train Set: Average loss: 2.92045315, Accuracy: 180/891 (20.20202020%)\n",
      "\n",
      "Test set: Average loss: 2.76044351, Accuracy: 25/99 (25.25252525%)\n",
      "\n",
      "Epoch 51 time: 0.9715304374694824 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 52 [0/891 (0.00000000%)]\tLoss: 2.97959590\n",
      "Train Epoch: 52 [320/891 (35.71428571%)]\tLoss: 3.00117254\n",
      "Train Epoch: 52 [640/891 (71.42857143%)]\tLoss: 2.60041785\n",
      "Train Set: Average loss: 2.92535770, Accuracy: 162/891 (18.18181818%)\n",
      "\n",
      "Test set: Average loss: 2.38270283, Accuracy: 30/99 (30.30303030%)\n",
      "\n",
      "Epoch 52 time: 0.9891743659973145 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 53 [0/891 (0.00000000%)]\tLoss: 2.73536181\n",
      "Train Epoch: 53 [320/891 (35.71428571%)]\tLoss: 2.92756438\n",
      "Train Epoch: 53 [640/891 (71.42857143%)]\tLoss: 2.58120894\n",
      "Train Set: Average loss: 2.85012421, Accuracy: 172/891 (19.30415264%)\n",
      "\n",
      "Test set: Average loss: 2.48827342, Accuracy: 32/99 (32.32323232%)\n",
      "\n",
      "Epoch 53 time: 0.9957771301269531 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 54 [0/891 (0.00000000%)]\tLoss: 2.67481828\n",
      "Train Epoch: 54 [320/891 (35.71428571%)]\tLoss: 3.02336216\n",
      "Train Epoch: 54 [640/891 (71.42857143%)]\tLoss: 2.81376243\n",
      "Train Set: Average loss: 2.80832586, Accuracy: 178/891 (19.97755331%)\n",
      "\n",
      "Test set: Average loss: 2.72019900, Accuracy: 25/99 (25.25252525%)\n",
      "\n",
      "Epoch 54 time: 1.015915870666504 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 55 [0/891 (0.00000000%)]\tLoss: 2.98067117\n",
      "Train Epoch: 55 [320/891 (35.71428571%)]\tLoss: 2.90945244\n",
      "Train Epoch: 55 [640/891 (71.42857143%)]\tLoss: 2.75377178\n",
      "Train Set: Average loss: 2.73002479, Accuracy: 204/891 (22.89562290%)\n",
      "\n",
      "Test set: Average loss: 2.43067810, Accuracy: 34/99 (34.34343434%)\n",
      "\n",
      "Epoch 55 time: 0.9949219226837158 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 56 [0/891 (0.00000000%)]\tLoss: 3.09691787\n",
      "Train Epoch: 56 [320/891 (35.71428571%)]\tLoss: 2.42142081\n",
      "Train Epoch: 56 [640/891 (71.42857143%)]\tLoss: 2.87319946\n",
      "Train Set: Average loss: 2.64520177, Accuracy: 200/891 (22.44668911%)\n",
      "\n",
      "Test set: Average loss: 2.42444198, Accuracy: 31/99 (31.31313131%)\n",
      "\n",
      "Epoch 56 time: 0.9938998222351074 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 57 [0/891 (0.00000000%)]\tLoss: 2.51461530\n",
      "Train Epoch: 57 [320/891 (35.71428571%)]\tLoss: 2.63745570\n",
      "Train Epoch: 57 [640/891 (71.42857143%)]\tLoss: 2.17985344\n",
      "Train Set: Average loss: 2.48948525, Accuracy: 216/891 (24.24242424%)\n",
      "\n",
      "Test set: Average loss: 1.88122086, Accuracy: 46/99 (46.46464646%)\n",
      "\n",
      "Epoch 57 time: 0.9789667129516602 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 58 [0/891 (0.00000000%)]\tLoss: 2.50736189\n",
      "Train Epoch: 58 [320/891 (35.71428571%)]\tLoss: 2.06789279\n",
      "Train Epoch: 58 [640/891 (71.42857143%)]\tLoss: 2.46323490\n",
      "Train Set: Average loss: 2.21996960, Accuracy: 293/891 (32.88439955%)\n",
      "\n",
      "Test set: Average loss: 1.78048472, Accuracy: 52/99 (52.52525253%)\n",
      "\n",
      "Epoch 58 time: 0.9793295860290527 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 59 [0/891 (0.00000000%)]\tLoss: 1.84937835\n",
      "Train Epoch: 59 [320/891 (35.71428571%)]\tLoss: 1.99556315\n",
      "Train Epoch: 59 [640/891 (71.42857143%)]\tLoss: 1.86983955\n",
      "Train Set: Average loss: 1.99920633, Accuracy: 325/891 (36.47586981%)\n",
      "\n",
      "Test set: Average loss: 1.60785297, Accuracy: 51/99 (51.51515152%)\n",
      "\n",
      "Epoch 59 time: 0.9883577823638916 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 60 [0/891 (0.00000000%)]\tLoss: 1.83152819\n",
      "Train Epoch: 60 [320/891 (35.71428571%)]\tLoss: 2.12205482\n",
      "Train Epoch: 60 [640/891 (71.42857143%)]\tLoss: 1.99614429\n",
      "Train Set: Average loss: 1.88028640, Accuracy: 343/891 (38.49607183%)\n",
      "\n",
      "Test set: Average loss: 1.51289264, Accuracy: 56/99 (56.56565657%)\n",
      "\n",
      "Epoch 60 time: 0.98960280418396 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 61 [0/891 (0.00000000%)]\tLoss: 1.53491497\n",
      "Train Epoch: 61 [320/891 (35.71428571%)]\tLoss: 1.91908360\n",
      "Train Epoch: 61 [640/891 (71.42857143%)]\tLoss: 1.70978379\n",
      "Train Set: Average loss: 1.77055609, Accuracy: 391/891 (43.88327722%)\n",
      "\n",
      "Test set: Average loss: 1.29248258, Accuracy: 62/99 (62.62626263%)\n",
      "\n",
      "Epoch 61 time: 1.0083484649658203 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 62 [0/891 (0.00000000%)]\tLoss: 1.70786583\n",
      "Train Epoch: 62 [320/891 (35.71428571%)]\tLoss: 1.73755550\n",
      "Train Epoch: 62 [640/891 (71.42857143%)]\tLoss: 1.75434399\n",
      "Train Set: Average loss: 1.63963439, Accuracy: 389/891 (43.65881033%)\n",
      "\n",
      "Test set: Average loss: 1.29382105, Accuracy: 59/99 (59.59595960%)\n",
      "\n",
      "Epoch 62 time: 1.014512062072754 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 63 [0/891 (0.00000000%)]\tLoss: 1.61092603\n",
      "Train Epoch: 63 [320/891 (35.71428571%)]\tLoss: 1.46533430\n",
      "Train Epoch: 63 [640/891 (71.42857143%)]\tLoss: 1.48581123\n",
      "Train Set: Average loss: 1.52664329, Accuracy: 457/891 (51.29068462%)\n",
      "\n",
      "Test set: Average loss: 1.11988289, Accuracy: 63/99 (63.63636364%)\n",
      "\n",
      "Epoch 63 time: 1.0040278434753418 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 64 [0/891 (0.00000000%)]\tLoss: 1.45603073\n",
      "Train Epoch: 64 [320/891 (35.71428571%)]\tLoss: 1.24006689\n",
      "Train Epoch: 64 [640/891 (71.42857143%)]\tLoss: 1.31709695\n",
      "Train Set: Average loss: 1.42707742, Accuracy: 467/891 (52.41301908%)\n",
      "\n",
      "Test set: Average loss: 1.02871480, Accuracy: 69/99 (69.69696970%)\n",
      "\n",
      "Epoch 64 time: 0.9944045543670654 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 65 [0/891 (0.00000000%)]\tLoss: 1.46930933\n",
      "Train Epoch: 65 [320/891 (35.71428571%)]\tLoss: 1.22679901\n",
      "Train Epoch: 65 [640/891 (71.42857143%)]\tLoss: 1.52142060\n",
      "Train Set: Average loss: 1.33065638, Accuracy: 491/891 (55.10662177%)\n",
      "\n",
      "Test set: Average loss: 0.95907250, Accuracy: 70/99 (70.70707071%)\n",
      "\n",
      "Epoch 65 time: 0.9879856109619141 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 66 [0/891 (0.00000000%)]\tLoss: 1.14954901\n",
      "Train Epoch: 66 [320/891 (35.71428571%)]\tLoss: 1.28928220\n",
      "Train Epoch: 66 [640/891 (71.42857143%)]\tLoss: 1.61853087\n",
      "Train Set: Average loss: 1.27019028, Accuracy: 502/891 (56.34118967%)\n",
      "\n",
      "Test set: Average loss: 0.91808082, Accuracy: 69/99 (69.69696970%)\n",
      "\n",
      "Epoch 66 time: 1.0115830898284912 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 67 [0/891 (0.00000000%)]\tLoss: 1.18264079\n",
      "Train Epoch: 67 [320/891 (35.71428571%)]\tLoss: 1.55673039\n",
      "Train Epoch: 67 [640/891 (71.42857143%)]\tLoss: 1.06796837\n",
      "Train Set: Average loss: 1.25118964, Accuracy: 509/891 (57.12682379%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.85586640, Accuracy: 70/99 (70.70707071%)\n",
      "\n",
      "Epoch 67 time: 1.0428221225738525 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 68 [0/891 (0.00000000%)]\tLoss: 1.17774224\n",
      "Train Epoch: 68 [320/891 (35.71428571%)]\tLoss: 1.29494762\n",
      "Train Epoch: 68 [640/891 (71.42857143%)]\tLoss: 1.33954275\n",
      "Train Set: Average loss: 1.19779849, Accuracy: 525/891 (58.92255892%)\n",
      "\n",
      "Test set: Average loss: 0.80736027, Accuracy: 74/99 (74.74747475%)\n",
      "\n",
      "Epoch 68 time: 1.0020792484283447 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 69 [0/891 (0.00000000%)]\tLoss: 0.98952746\n",
      "Train Epoch: 69 [320/891 (35.71428571%)]\tLoss: 1.29932499\n",
      "Train Epoch: 69 [640/891 (71.42857143%)]\tLoss: 0.71988291\n",
      "Train Set: Average loss: 1.26714297, Accuracy: 513/891 (57.57575758%)\n",
      "\n",
      "Test set: Average loss: 0.82911779, Accuracy: 67/99 (67.67676768%)\n",
      "\n",
      "Epoch 69 time: 1.0095868110656738 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 70 [0/891 (0.00000000%)]\tLoss: 1.40966296\n",
      "Train Epoch: 70 [320/891 (35.71428571%)]\tLoss: 1.42597091\n",
      "Train Epoch: 70 [640/891 (71.42857143%)]\tLoss: 1.56018257\n",
      "Train Set: Average loss: 1.12918044, Accuracy: 546/891 (61.27946128%)\n",
      "\n",
      "Test set: Average loss: 0.78884379, Accuracy: 76/99 (76.76767677%)\n",
      "\n",
      "Epoch 70 time: 1.0316591262817383 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 71 [0/891 (0.00000000%)]\tLoss: 1.11420417\n",
      "Train Epoch: 71 [320/891 (35.71428571%)]\tLoss: 1.07342887\n",
      "Train Epoch: 71 [640/891 (71.42857143%)]\tLoss: 1.14665151\n",
      "Train Set: Average loss: 1.09699817, Accuracy: 562/891 (63.07519641%)\n",
      "\n",
      "Test set: Average loss: 0.79043349, Accuracy: 72/99 (72.72727273%)\n",
      "\n",
      "Epoch 71 time: 1.0644774436950684 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 72 [0/891 (0.00000000%)]\tLoss: 1.21419156\n",
      "Train Epoch: 72 [320/891 (35.71428571%)]\tLoss: 0.93669319\n",
      "Train Epoch: 72 [640/891 (71.42857143%)]\tLoss: 1.07570910\n",
      "Train Set: Average loss: 1.06898410, Accuracy: 568/891 (63.74859708%)\n",
      "\n",
      "Test set: Average loss: 0.82655217, Accuracy: 70/99 (70.70707071%)\n",
      "\n",
      "Epoch 72 time: 1.051025390625 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 73 [0/891 (0.00000000%)]\tLoss: 0.94979644\n",
      "Train Epoch: 73 [320/891 (35.71428571%)]\tLoss: 0.94678122\n",
      "Train Epoch: 73 [640/891 (71.42857143%)]\tLoss: 0.86914235\n",
      "Train Set: Average loss: 1.08350213, Accuracy: 563/891 (63.18742985%)\n",
      "\n",
      "Test set: Average loss: 0.72583331, Accuracy: 73/99 (73.73737374%)\n",
      "\n",
      "Epoch 73 time: 1.0350220203399658 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 74 [0/891 (0.00000000%)]\tLoss: 1.07793689\n",
      "Train Epoch: 74 [320/891 (35.71428571%)]\tLoss: 1.14741337\n",
      "Train Epoch: 74 [640/891 (71.42857143%)]\tLoss: 1.11322701\n",
      "Train Set: Average loss: 0.97314637, Accuracy: 605/891 (67.90123457%)\n",
      "\n",
      "Test set: Average loss: 0.74738920, Accuracy: 76/99 (76.76767677%)\n",
      "\n",
      "Epoch 74 time: 1.0317959785461426 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 75 [0/891 (0.00000000%)]\tLoss: 0.96250194\n",
      "Train Epoch: 75 [320/891 (35.71428571%)]\tLoss: 1.13765538\n",
      "Train Epoch: 75 [640/891 (71.42857143%)]\tLoss: 0.94280195\n",
      "Train Set: Average loss: 1.04698849, Accuracy: 550/891 (61.72839506%)\n",
      "\n",
      "Test set: Average loss: 0.81495682, Accuracy: 67/99 (67.67676768%)\n",
      "\n",
      "Epoch 75 time: 1.0256202220916748 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 76 [0/891 (0.00000000%)]\tLoss: 1.59438848\n",
      "Train Epoch: 76 [320/891 (35.71428571%)]\tLoss: 1.45305455\n",
      "Train Epoch: 76 [640/891 (71.42857143%)]\tLoss: 1.34155464\n",
      "Train Set: Average loss: 1.11430333, Accuracy: 554/891 (62.17732884%)\n",
      "\n",
      "Test set: Average loss: 0.70448297, Accuracy: 72/99 (72.72727273%)\n",
      "\n",
      "Epoch 76 time: 1.039323091506958 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 77 [0/891 (0.00000000%)]\tLoss: 0.83145261\n",
      "Train Epoch: 77 [320/891 (35.71428571%)]\tLoss: 1.53865957\n",
      "Train Epoch: 77 [640/891 (71.42857143%)]\tLoss: 0.91868865\n",
      "Train Set: Average loss: 1.03734783, Accuracy: 593/891 (66.55443322%)\n",
      "\n",
      "Test set: Average loss: 0.66947628, Accuracy: 78/99 (78.78787879%)\n",
      "\n",
      "Epoch 77 time: 1.070716381072998 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 78 [0/891 (0.00000000%)]\tLoss: 0.67388165\n",
      "Train Epoch: 78 [320/891 (35.71428571%)]\tLoss: 1.17954493\n",
      "Train Epoch: 78 [640/891 (71.42857143%)]\tLoss: 0.81708610\n",
      "Train Set: Average loss: 1.02562699, Accuracy: 582/891 (65.31986532%)\n",
      "\n",
      "Test set: Average loss: 0.75545504, Accuracy: 75/99 (75.75757576%)\n",
      "\n",
      "Epoch 78 time: 1.0595510005950928 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 79 [0/891 (0.00000000%)]\tLoss: 1.01359797\n",
      "Train Epoch: 79 [320/891 (35.71428571%)]\tLoss: 0.76681161\n",
      "Train Epoch: 79 [640/891 (71.42857143%)]\tLoss: 1.42506719\n",
      "Train Set: Average loss: 0.93804132, Accuracy: 607/891 (68.12570146%)\n",
      "\n",
      "Test set: Average loss: 0.66145611, Accuracy: 78/99 (78.78787879%)\n",
      "\n",
      "Epoch 79 time: 1.0026264190673828 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 80 [0/891 (0.00000000%)]\tLoss: 1.12219250\n",
      "Train Epoch: 80 [320/891 (35.71428571%)]\tLoss: 0.93120897\n",
      "Train Epoch: 80 [640/891 (71.42857143%)]\tLoss: 1.07611442\n",
      "Train Set: Average loss: 0.94107845, Accuracy: 609/891 (68.35016835%)\n",
      "\n",
      "Test set: Average loss: 0.68035382, Accuracy: 75/99 (75.75757576%)\n",
      "\n",
      "Epoch 80 time: 0.979177713394165 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 81 [0/891 (0.00000000%)]\tLoss: 0.95074940\n",
      "Train Epoch: 81 [320/891 (35.71428571%)]\tLoss: 1.22866297\n",
      "Train Epoch: 81 [640/891 (71.42857143%)]\tLoss: 0.71713245\n",
      "Train Set: Average loss: 1.00505475, Accuracy: 574/891 (64.42199776%)\n",
      "\n",
      "Test set: Average loss: 0.68027074, Accuracy: 76/99 (76.76767677%)\n",
      "\n",
      "Epoch 81 time: 1.0446219444274902 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 82 [0/891 (0.00000000%)]\tLoss: 0.78278297\n",
      "Train Epoch: 82 [320/891 (35.71428571%)]\tLoss: 0.81290859\n",
      "Train Epoch: 82 [640/891 (71.42857143%)]\tLoss: 0.89339268\n",
      "Train Set: Average loss: 0.85502282, Accuracy: 637/891 (71.49270483%)\n",
      "\n",
      "Test set: Average loss: 0.69830330, Accuracy: 76/99 (76.76767677%)\n",
      "\n",
      "Epoch 82 time: 1.0372624397277832 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 83 [0/891 (0.00000000%)]\tLoss: 0.99960327\n",
      "Train Epoch: 83 [320/891 (35.71428571%)]\tLoss: 0.85989672\n",
      "Train Epoch: 83 [640/891 (71.42857143%)]\tLoss: 0.61082536\n",
      "Train Set: Average loss: 0.76374156, Accuracy: 657/891 (73.73737374%)\n",
      "\n",
      "Test set: Average loss: 0.63654445, Accuracy: 74/99 (74.74747475%)\n",
      "\n",
      "Epoch 83 time: 0.9579546451568604 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 84 [0/891 (0.00000000%)]\tLoss: 0.97713232\n",
      "Train Epoch: 84 [320/891 (35.71428571%)]\tLoss: 0.85637212\n",
      "Train Epoch: 84 [640/891 (71.42857143%)]\tLoss: 1.19050550\n",
      "Train Set: Average loss: 0.84124132, Accuracy: 626/891 (70.25813692%)\n",
      "\n",
      "Test set: Average loss: 0.57437172, Accuracy: 80/99 (80.80808081%)\n",
      "\n",
      "Epoch 84 time: 1.0066328048706055 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 85 [0/891 (0.00000000%)]\tLoss: 0.96904802\n",
      "Train Epoch: 85 [320/891 (35.71428571%)]\tLoss: 0.83475935\n",
      "Train Epoch: 85 [640/891 (71.42857143%)]\tLoss: 0.76656955\n",
      "Train Set: Average loss: 0.76990409, Accuracy: 657/891 (73.73737374%)\n",
      "\n",
      "Test set: Average loss: 0.58130032, Accuracy: 84/99 (84.84848485%)\n",
      "\n",
      "Epoch 85 time: 1.0515854358673096 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 86 [0/891 (0.00000000%)]\tLoss: 0.52058899\n",
      "Train Epoch: 86 [320/891 (35.71428571%)]\tLoss: 0.78900945\n",
      "Train Epoch: 86 [640/891 (71.42857143%)]\tLoss: 1.01556146\n",
      "Train Set: Average loss: 0.77905809, Accuracy: 659/891 (73.96184063%)\n",
      "\n",
      "Test set: Average loss: 0.65947280, Accuracy: 75/99 (75.75757576%)\n",
      "\n",
      "Epoch 86 time: 0.9803769588470459 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 87 [0/891 (0.00000000%)]\tLoss: 1.12675655\n",
      "Train Epoch: 87 [320/891 (35.71428571%)]\tLoss: 0.84954095\n",
      "Train Epoch: 87 [640/891 (71.42857143%)]\tLoss: 0.65621889\n",
      "Train Set: Average loss: 0.83097066, Accuracy: 631/891 (70.81930415%)\n",
      "\n",
      "Test set: Average loss: 0.60611236, Accuracy: 80/99 (80.80808081%)\n",
      "\n",
      "Epoch 87 time: 0.9985897541046143 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 88 [0/891 (0.00000000%)]\tLoss: 0.52168763\n",
      "Train Epoch: 88 [320/891 (35.71428571%)]\tLoss: 0.49326521\n",
      "Train Epoch: 88 [640/891 (71.42857143%)]\tLoss: 1.35716558\n",
      "Train Set: Average loss: 0.92048475, Accuracy: 620/891 (69.58473625%)\n",
      "\n",
      "Test set: Average loss: 0.60478578, Accuracy: 76/99 (76.76767677%)\n",
      "\n",
      "Epoch 88 time: 1.0039947032928467 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 89 [0/891 (0.00000000%)]\tLoss: 0.61721712\n",
      "Train Epoch: 89 [320/891 (35.71428571%)]\tLoss: 0.62394142\n",
      "Train Epoch: 89 [640/891 (71.42857143%)]\tLoss: 0.42623159\n",
      "Train Set: Average loss: 0.81358821, Accuracy: 658/891 (73.84960718%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.59977477, Accuracy: 80/99 (80.80808081%)\n",
      "\n",
      "Epoch 89 time: 1.0482041835784912 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 90 [0/891 (0.00000000%)]\tLoss: 0.72461832\n",
      "Train Epoch: 90 [320/891 (35.71428571%)]\tLoss: 1.17763424\n",
      "Train Epoch: 90 [640/891 (71.42857143%)]\tLoss: 0.69614315\n",
      "Train Set: Average loss: 0.75461946, Accuracy: 673/891 (75.53310887%)\n",
      "\n",
      "Test set: Average loss: 0.61448493, Accuracy: 76/99 (76.76767677%)\n",
      "\n",
      "Epoch 90 time: 0.9698917865753174 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 91 [0/891 (0.00000000%)]\tLoss: 0.94621676\n",
      "Train Epoch: 91 [320/891 (35.71428571%)]\tLoss: 1.04746771\n",
      "Train Epoch: 91 [640/891 (71.42857143%)]\tLoss: 0.99865252\n",
      "Train Set: Average loss: 0.78891680, Accuracy: 651/891 (73.06397306%)\n",
      "\n",
      "Test set: Average loss: 0.56320274, Accuracy: 80/99 (80.80808081%)\n",
      "\n",
      "Epoch 91 time: 0.9941420555114746 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 92 [0/891 (0.00000000%)]\tLoss: 0.57777053\n",
      "Train Epoch: 92 [320/891 (35.71428571%)]\tLoss: 0.76403850\n",
      "Train Epoch: 92 [640/891 (71.42857143%)]\tLoss: 0.97625780\n",
      "Train Set: Average loss: 0.66476085, Accuracy: 698/891 (78.33894501%)\n",
      "\n",
      "Test set: Average loss: 0.46525975, Accuracy: 81/99 (81.81818182%)\n",
      "\n",
      "Epoch 92 time: 1.0195715427398682 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 93 [0/891 (0.00000000%)]\tLoss: 0.69041449\n",
      "Train Epoch: 93 [320/891 (35.71428571%)]\tLoss: 0.76204073\n",
      "Train Epoch: 93 [640/891 (71.42857143%)]\tLoss: 0.87080681\n",
      "Train Set: Average loss: 0.74260237, Accuracy: 668/891 (74.97194164%)\n",
      "\n",
      "Test set: Average loss: 0.54925783, Accuracy: 82/99 (82.82828283%)\n",
      "\n",
      "Epoch 93 time: 1.0137014389038086 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 94 [0/891 (0.00000000%)]\tLoss: 0.42374822\n",
      "Train Epoch: 94 [320/891 (35.71428571%)]\tLoss: 0.91551328\n",
      "Train Epoch: 94 [640/891 (71.42857143%)]\tLoss: 0.75633526\n",
      "Train Set: Average loss: 0.70524061, Accuracy: 681/891 (76.43097643%)\n",
      "\n",
      "Test set: Average loss: 0.62063791, Accuracy: 78/99 (78.78787879%)\n",
      "\n",
      "Epoch 94 time: 1.0243902206420898 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 95 [0/891 (0.00000000%)]\tLoss: 0.58921385\n",
      "Train Epoch: 95 [320/891 (35.71428571%)]\tLoss: 0.74961054\n",
      "Train Epoch: 95 [640/891 (71.42857143%)]\tLoss: 1.00978017\n",
      "Train Set: Average loss: 0.72603616, Accuracy: 671/891 (75.30864198%)\n",
      "\n",
      "Test set: Average loss: 0.56174714, Accuracy: 79/99 (79.79797980%)\n",
      "\n",
      "Epoch 95 time: 1.0361058712005615 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 96 [0/891 (0.00000000%)]\tLoss: 0.87667924\n",
      "Train Epoch: 96 [320/891 (35.71428571%)]\tLoss: 0.37795562\n",
      "Train Epoch: 96 [640/891 (71.42857143%)]\tLoss: 0.62639648\n",
      "Train Set: Average loss: 0.64835189, Accuracy: 700/891 (78.56341190%)\n",
      "\n",
      "Test set: Average loss: 0.61814896, Accuracy: 76/99 (76.76767677%)\n",
      "\n",
      "Epoch 96 time: 1.0341851711273193 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 97 [0/891 (0.00000000%)]\tLoss: 0.39581218\n",
      "Train Epoch: 97 [320/891 (35.71428571%)]\tLoss: 0.73105365\n",
      "Train Epoch: 97 [640/891 (71.42857143%)]\tLoss: 0.71578735\n",
      "Train Set: Average loss: 0.72653570, Accuracy: 690/891 (77.44107744%)\n",
      "\n",
      "Test set: Average loss: 0.55684570, Accuracy: 77/99 (77.77777778%)\n",
      "\n",
      "Epoch 97 time: 1.0240018367767334 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 98 [0/891 (0.00000000%)]\tLoss: 0.65685242\n",
      "Train Epoch: 98 [320/891 (35.71428571%)]\tLoss: 0.58036560\n",
      "Train Epoch: 98 [640/891 (71.42857143%)]\tLoss: 0.49183774\n",
      "Train Set: Average loss: 0.68268009, Accuracy: 678/891 (76.09427609%)\n",
      "\n",
      "Test set: Average loss: 0.55071760, Accuracy: 78/99 (78.78787879%)\n",
      "\n",
      "Epoch 98 time: 1.0432698726654053 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 99 [0/891 (0.00000000%)]\tLoss: 0.39850548\n",
      "Train Epoch: 99 [320/891 (35.71428571%)]\tLoss: 0.48074660\n",
      "Train Epoch: 99 [640/891 (71.42857143%)]\tLoss: 0.54109937\n",
      "Train Set: Average loss: 0.69644011, Accuracy: 684/891 (76.76767677%)\n",
      "\n",
      "Test set: Average loss: 0.47017818, Accuracy: 83/99 (83.83838384%)\n",
      "\n",
      "Epoch 99 time: 1.1092488765716553 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 100 [0/891 (0.00000000%)]\tLoss: 0.70694458\n",
      "Train Epoch: 100 [320/891 (35.71428571%)]\tLoss: 0.62725222\n",
      "Train Epoch: 100 [640/891 (71.42857143%)]\tLoss: 0.83373910\n",
      "Train Set: Average loss: 0.57816709, Accuracy: 718/891 (80.58361392%)\n",
      "\n",
      "Test set: Average loss: 0.52430931, Accuracy: 80/99 (80.80808081%)\n",
      "\n",
      "Epoch 100 time: 1.048583745956421 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 101 [0/891 (0.00000000%)]\tLoss: 0.50828862\n",
      "Train Epoch: 101 [320/891 (35.71428571%)]\tLoss: 0.48954874\n",
      "Train Epoch: 101 [640/891 (71.42857143%)]\tLoss: 0.93165559\n",
      "Train Set: Average loss: 0.59538726, Accuracy: 725/891 (81.36924804%)\n",
      "\n",
      "Test set: Average loss: 0.45005522, Accuracy: 80/99 (80.80808081%)\n",
      "\n",
      "Epoch 101 time: 1.0438387393951416 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 102 [0/891 (0.00000000%)]\tLoss: 0.40299857\n",
      "Train Epoch: 102 [320/891 (35.71428571%)]\tLoss: 0.29726753\n",
      "Train Epoch: 102 [640/891 (71.42857143%)]\tLoss: 0.35032856\n",
      "Train Set: Average loss: 0.51677349, Accuracy: 733/891 (82.26711560%)\n",
      "\n",
      "Test set: Average loss: 0.48081687, Accuracy: 77/99 (77.77777778%)\n",
      "\n",
      "Epoch 102 time: 1.018510341644287 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 103 [0/891 (0.00000000%)]\tLoss: 0.62353778\n",
      "Train Epoch: 103 [320/891 (35.71428571%)]\tLoss: 0.60903215\n",
      "Train Epoch: 103 [640/891 (71.42857143%)]\tLoss: 0.74725699\n",
      "Train Set: Average loss: 0.50376958, Accuracy: 747/891 (83.83838384%)\n",
      "\n",
      "Test set: Average loss: 0.47729881, Accuracy: 84/99 (84.84848485%)\n",
      "\n",
      "Epoch 103 time: 1.0156605243682861 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 104 [0/891 (0.00000000%)]\tLoss: 0.32974565\n",
      "Train Epoch: 104 [320/891 (35.71428571%)]\tLoss: 0.41780168\n",
      "Train Epoch: 104 [640/891 (71.42857143%)]\tLoss: 0.28800049\n",
      "Train Set: Average loss: 0.51981178, Accuracy: 738/891 (82.82828283%)\n",
      "\n",
      "Test set: Average loss: 0.51723578, Accuracy: 80/99 (80.80808081%)\n",
      "\n",
      "Epoch 104 time: 1.0087158679962158 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 105 [0/891 (0.00000000%)]\tLoss: 0.42051202\n",
      "Train Epoch: 105 [320/891 (35.71428571%)]\tLoss: 0.25740454\n",
      "Train Epoch: 105 [640/891 (71.42857143%)]\tLoss: 0.60582942\n",
      "Train Set: Average loss: 0.51285149, Accuracy: 735/891 (82.49158249%)\n",
      "\n",
      "Test set: Average loss: 0.39282780, Accuracy: 86/99 (86.86868687%)\n",
      "\n",
      "Epoch 105 time: 1.0682947635650635 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 106 [0/891 (0.00000000%)]\tLoss: 0.58633435\n",
      "Train Epoch: 106 [320/891 (35.71428571%)]\tLoss: 0.53305185\n",
      "Train Epoch: 106 [640/891 (71.42857143%)]\tLoss: 0.69431055\n",
      "Train Set: Average loss: 0.50117110, Accuracy: 738/891 (82.82828283%)\n",
      "\n",
      "Test set: Average loss: 0.54455474, Accuracy: 83/99 (83.83838384%)\n",
      "\n",
      "Epoch 106 time: 1.0440938472747803 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 107 [0/891 (0.00000000%)]\tLoss: 0.48221812\n",
      "Train Epoch: 107 [320/891 (35.71428571%)]\tLoss: 0.44170785\n",
      "Train Epoch: 107 [640/891 (71.42857143%)]\tLoss: 0.81521255\n",
      "Train Set: Average loss: 0.45818524, Accuracy: 769/891 (86.30751964%)\n",
      "\n",
      "Test set: Average loss: 0.44305401, Accuracy: 87/99 (87.87878788%)\n",
      "\n",
      "Epoch 107 time: 1.0257501602172852 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 108 [0/891 (0.00000000%)]\tLoss: 0.31000447\n",
      "Train Epoch: 108 [320/891 (35.71428571%)]\tLoss: 0.53858984\n",
      "Train Epoch: 108 [640/891 (71.42857143%)]\tLoss: 0.46568084\n",
      "Train Set: Average loss: 0.46291747, Accuracy: 744/891 (83.50168350%)\n",
      "\n",
      "Test set: Average loss: 0.42339140, Accuracy: 85/99 (85.85858586%)\n",
      "\n",
      "Epoch 108 time: 1.007561206817627 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 109 [0/891 (0.00000000%)]\tLoss: 0.56739682\n",
      "Train Epoch: 109 [320/891 (35.71428571%)]\tLoss: 0.38833603\n",
      "Train Epoch: 109 [640/891 (71.42857143%)]\tLoss: 0.65049106\n",
      "Train Set: Average loss: 0.43997443, Accuracy: 753/891 (84.51178451%)\n",
      "\n",
      "Test set: Average loss: 0.40736621, Accuracy: 86/99 (86.86868687%)\n",
      "\n",
      "Epoch 109 time: 1.0631279945373535 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 110 [0/891 (0.00000000%)]\tLoss: 0.39798990\n",
      "Train Epoch: 110 [320/891 (35.71428571%)]\tLoss: 0.37386492\n",
      "Train Epoch: 110 [640/891 (71.42857143%)]\tLoss: 0.25554579\n",
      "Train Set: Average loss: 0.47220627, Accuracy: 756/891 (84.84848485%)\n",
      "\n",
      "Test set: Average loss: 0.37215640, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 110 time: 0.9762818813323975 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 111 [0/891 (0.00000000%)]\tLoss: 0.45332497\n",
      "Train Epoch: 111 [320/891 (35.71428571%)]\tLoss: 0.34191504\n",
      "Train Epoch: 111 [640/891 (71.42857143%)]\tLoss: 0.33674991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: Average loss: 0.42363032, Accuracy: 756/891 (84.84848485%)\n",
      "\n",
      "Test set: Average loss: 0.42320634, Accuracy: 86/99 (86.86868687%)\n",
      "\n",
      "Epoch 111 time: 1.028745174407959 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 112 [0/891 (0.00000000%)]\tLoss: 0.77153659\n",
      "Train Epoch: 112 [320/891 (35.71428571%)]\tLoss: 0.37965962\n",
      "Train Epoch: 112 [640/891 (71.42857143%)]\tLoss: 0.56503701\n",
      "Train Set: Average loss: 0.47693790, Accuracy: 743/891 (83.38945006%)\n",
      "\n",
      "Test set: Average loss: 0.29029279, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 112 time: 1.0091452598571777 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 113 [0/891 (0.00000000%)]\tLoss: 0.25801417\n",
      "Train Epoch: 113 [320/891 (35.71428571%)]\tLoss: 0.37473190\n",
      "Train Epoch: 113 [640/891 (71.42857143%)]\tLoss: 0.70667279\n",
      "Train Set: Average loss: 0.38770716, Accuracy: 774/891 (86.86868687%)\n",
      "\n",
      "Test set: Average loss: 0.39622101, Accuracy: 83/99 (83.83838384%)\n",
      "\n",
      "Epoch 113 time: 1.0341110229492188 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 114 [0/891 (0.00000000%)]\tLoss: 0.59396642\n",
      "Train Epoch: 114 [320/891 (35.71428571%)]\tLoss: 0.33025825\n",
      "Train Epoch: 114 [640/891 (71.42857143%)]\tLoss: 0.50275099\n",
      "Train Set: Average loss: 0.38215086, Accuracy: 785/891 (88.10325477%)\n",
      "\n",
      "Test set: Average loss: 0.40997159, Accuracy: 87/99 (87.87878788%)\n",
      "\n",
      "Epoch 114 time: 0.9915955066680908 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 115 [0/891 (0.00000000%)]\tLoss: 0.37135008\n",
      "Train Epoch: 115 [320/891 (35.71428571%)]\tLoss: 0.32029819\n",
      "Train Epoch: 115 [640/891 (71.42857143%)]\tLoss: 0.31066349\n",
      "Train Set: Average loss: 0.38227685, Accuracy: 778/891 (87.31762065%)\n",
      "\n",
      "Test set: Average loss: 0.36656201, Accuracy: 84/99 (84.84848485%)\n",
      "\n",
      "Epoch 115 time: 0.9810750484466553 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 116 [0/891 (0.00000000%)]\tLoss: 0.24958295\n",
      "Train Epoch: 116 [320/891 (35.71428571%)]\tLoss: 0.19486302\n",
      "Train Epoch: 116 [640/891 (71.42857143%)]\tLoss: 0.33017188\n",
      "Train Set: Average loss: 0.45101783, Accuracy: 754/891 (84.62401796%)\n",
      "\n",
      "Test set: Average loss: 0.47403854, Accuracy: 85/99 (85.85858586%)\n",
      "\n",
      "Epoch 116 time: 0.9998774528503418 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 117 [0/891 (0.00000000%)]\tLoss: 0.10112917\n",
      "Train Epoch: 117 [320/891 (35.71428571%)]\tLoss: 0.34045789\n",
      "Train Epoch: 117 [640/891 (71.42857143%)]\tLoss: 0.47395748\n",
      "Train Set: Average loss: 0.42936025, Accuracy: 771/891 (86.53198653%)\n",
      "\n",
      "Test set: Average loss: 0.39540011, Accuracy: 85/99 (85.85858586%)\n",
      "\n",
      "Epoch 117 time: 0.990293025970459 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 118 [0/891 (0.00000000%)]\tLoss: 0.40068004\n",
      "Train Epoch: 118 [320/891 (35.71428571%)]\tLoss: 0.48840070\n",
      "Train Epoch: 118 [640/891 (71.42857143%)]\tLoss: 0.43787777\n",
      "Train Set: Average loss: 0.40420602, Accuracy: 772/891 (86.64421998%)\n",
      "\n",
      "Test set: Average loss: 0.34399955, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 118 time: 0.9761543273925781 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 119 [0/891 (0.00000000%)]\tLoss: 0.60688162\n",
      "Train Epoch: 119 [320/891 (35.71428571%)]\tLoss: 0.28119627\n",
      "Train Epoch: 119 [640/891 (71.42857143%)]\tLoss: 0.62772268\n",
      "Train Set: Average loss: 0.45843791, Accuracy: 748/891 (83.95061728%)\n",
      "\n",
      "Test set: Average loss: 0.36382929, Accuracy: 87/99 (87.87878788%)\n",
      "\n",
      "Epoch 119 time: 0.990973949432373 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 120 [0/891 (0.00000000%)]\tLoss: 0.18889198\n",
      "Train Epoch: 120 [320/891 (35.71428571%)]\tLoss: 0.34014061\n",
      "Train Epoch: 120 [640/891 (71.42857143%)]\tLoss: 0.33717993\n",
      "Train Set: Average loss: 0.44101065, Accuracy: 774/891 (86.86868687%)\n",
      "\n",
      "Test set: Average loss: 0.48048192, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 120 time: 1.0071251392364502 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 121 [0/891 (0.00000000%)]\tLoss: 0.67298818\n",
      "Train Epoch: 121 [320/891 (35.71428571%)]\tLoss: 0.64136803\n",
      "Train Epoch: 121 [640/891 (71.42857143%)]\tLoss: 1.18460858\n",
      "Train Set: Average loss: 0.43167716, Accuracy: 765/891 (85.85858586%)\n",
      "\n",
      "Test set: Average loss: 0.33337012, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 121 time: 1.0332050323486328 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 122 [0/891 (0.00000000%)]\tLoss: 0.28629631\n",
      "Train Epoch: 122 [320/891 (35.71428571%)]\tLoss: 0.22747570\n",
      "Train Epoch: 122 [640/891 (71.42857143%)]\tLoss: 0.33311665\n",
      "Train Set: Average loss: 0.34876344, Accuracy: 779/891 (87.42985410%)\n",
      "\n",
      "Test set: Average loss: 0.35946868, Accuracy: 88/99 (88.88888889%)\n",
      "\n",
      "Epoch 122 time: 0.9904270172119141 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 123 [0/891 (0.00000000%)]\tLoss: 0.28327879\n",
      "Train Epoch: 123 [320/891 (35.71428571%)]\tLoss: 0.37183985\n",
      "Train Epoch: 123 [640/891 (71.42857143%)]\tLoss: 0.22108486\n",
      "Train Set: Average loss: 0.38844722, Accuracy: 763/891 (85.63411897%)\n",
      "\n",
      "Test set: Average loss: 0.37265443, Accuracy: 88/99 (88.88888889%)\n",
      "\n",
      "Epoch 123 time: 0.9982609748840332 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 124 [0/891 (0.00000000%)]\tLoss: 0.23323277\n",
      "Train Epoch: 124 [320/891 (35.71428571%)]\tLoss: 0.50801021\n",
      "Train Epoch: 124 [640/891 (71.42857143%)]\tLoss: 0.19659036\n",
      "Train Set: Average loss: 0.35643129, Accuracy: 787/891 (88.32772166%)\n",
      "\n",
      "Test set: Average loss: 0.36397798, Accuracy: 84/99 (84.84848485%)\n",
      "\n",
      "Epoch 124 time: 1.0281941890716553 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "Train Epoch: 125 [0/891 (0.00000000%)]\tLoss: 0.14008665\n",
      "Train Epoch: 125 [320/891 (35.71428571%)]\tLoss: 0.59773785\n",
      "Train Epoch: 125 [640/891 (71.42857143%)]\tLoss: 0.62177938\n",
      "Train Set: Average loss: 0.38945494, Accuracy: 765/891 (85.85858586%)\n",
      "\n",
      "Test set: Average loss: 0.33755807, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 125 time: 0.9855632781982422 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 126 [0/891 (0.00000000%)]\tLoss: 0.13464287\n",
      "Train Epoch: 126 [320/891 (35.71428571%)]\tLoss: 0.57739139\n",
      "Train Epoch: 126 [640/891 (71.42857143%)]\tLoss: 0.33258933\n",
      "Train Set: Average loss: 0.37538853, Accuracy: 781/891 (87.65432099%)\n",
      "\n",
      "Test set: Average loss: 0.34282633, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 126 time: 1.0405733585357666 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 127 [0/891 (0.00000000%)]\tLoss: 0.21813503\n",
      "Train Epoch: 127 [320/891 (35.71428571%)]\tLoss: 0.40556735\n",
      "Train Epoch: 127 [640/891 (71.42857143%)]\tLoss: 0.32889301\n",
      "Train Set: Average loss: 0.29570909, Accuracy: 799/891 (89.67452301%)\n",
      "\n",
      "Test set: Average loss: 0.36630415, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 127 time: 1.080749750137329 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 128 [0/891 (0.00000000%)]\tLoss: 0.09787887\n",
      "Train Epoch: 128 [320/891 (35.71428571%)]\tLoss: 0.36416885\n",
      "Train Epoch: 128 [640/891 (71.42857143%)]\tLoss: 0.13329497\n",
      "Train Set: Average loss: 0.25712829, Accuracy: 811/891 (91.02132435%)\n",
      "\n",
      "Test set: Average loss: 0.33509874, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 128 time: 1.0064949989318848 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 129 [0/891 (0.00000000%)]\tLoss: 0.50576270\n",
      "Train Epoch: 129 [320/891 (35.71428571%)]\tLoss: 0.19535652\n",
      "Train Epoch: 129 [640/891 (71.42857143%)]\tLoss: 0.60483825\n",
      "Train Set: Average loss: 0.34344028, Accuracy: 781/891 (87.65432099%)\n",
      "\n",
      "Test set: Average loss: 0.27642659, Accuracy: 88/99 (88.88888889%)\n",
      "\n",
      "Epoch 129 time: 1.0934648513793945 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 130 [0/891 (0.00000000%)]\tLoss: 0.24074495\n",
      "Train Epoch: 130 [320/891 (35.71428571%)]\tLoss: 0.34259310\n",
      "Train Epoch: 130 [640/891 (71.42857143%)]\tLoss: 0.09952119\n",
      "Train Set: Average loss: 0.32409999, Accuracy: 796/891 (89.33782267%)\n",
      "\n",
      "Test set: Average loss: 0.42392192, Accuracy: 88/99 (88.88888889%)\n",
      "\n",
      "Epoch 130 time: 1.0337092876434326 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 131 [0/891 (0.00000000%)]\tLoss: 0.11848551\n",
      "Train Epoch: 131 [320/891 (35.71428571%)]\tLoss: 0.33631763\n",
      "Train Epoch: 131 [640/891 (71.42857143%)]\tLoss: 0.68044657\n",
      "Train Set: Average loss: 0.29369805, Accuracy: 799/891 (89.67452301%)\n",
      "\n",
      "Test set: Average loss: 0.29461179, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 131 time: 1.0233795642852783 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 132 [0/891 (0.00000000%)]\tLoss: 0.31984234\n",
      "Train Epoch: 132 [320/891 (35.71428571%)]\tLoss: 0.34268653\n",
      "Train Epoch: 132 [640/891 (71.42857143%)]\tLoss: 0.49190181\n",
      "Train Set: Average loss: 0.30763903, Accuracy: 796/891 (89.33782267%)\n",
      "\n",
      "Test set: Average loss: 0.26597171, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 132 time: 1.044203519821167 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 133 [0/891 (0.00000000%)]\tLoss: 0.19239616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 133 [320/891 (35.71428571%)]\tLoss: 0.16434759\n",
      "Train Epoch: 133 [640/891 (71.42857143%)]\tLoss: 0.49228629\n",
      "Train Set: Average loss: 0.28509297, Accuracy: 797/891 (89.45005612%)\n",
      "\n",
      "Test set: Average loss: 0.28693322, Accuracy: 88/99 (88.88888889%)\n",
      "\n",
      "Epoch 133 time: 1.031968116760254 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 134 [0/891 (0.00000000%)]\tLoss: 0.33629259\n",
      "Train Epoch: 134 [320/891 (35.71428571%)]\tLoss: 0.27486169\n",
      "Train Epoch: 134 [640/891 (71.42857143%)]\tLoss: 0.30309629\n",
      "Train Set: Average loss: 0.29549301, Accuracy: 800/891 (89.78675645%)\n",
      "\n",
      "Test set: Average loss: 0.30730179, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 134 time: 1.0592236518859863 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 135 [0/891 (0.00000000%)]\tLoss: 0.47018912\n",
      "Train Epoch: 135 [320/891 (35.71428571%)]\tLoss: 0.22357571\n",
      "Train Epoch: 135 [640/891 (71.42857143%)]\tLoss: 0.34018004\n",
      "Train Set: Average loss: 0.25342262, Accuracy: 822/891 (92.25589226%)\n",
      "\n",
      "Test set: Average loss: 0.32534060, Accuracy: 88/99 (88.88888889%)\n",
      "\n",
      "Epoch 135 time: 1.0827696323394775 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 136 [0/891 (0.00000000%)]\tLoss: 0.37355131\n",
      "Train Epoch: 136 [320/891 (35.71428571%)]\tLoss: 0.14533365\n",
      "Train Epoch: 136 [640/891 (71.42857143%)]\tLoss: 0.68149543\n",
      "Train Set: Average loss: 0.32163906, Accuracy: 784/891 (87.99102132%)\n",
      "\n",
      "Test set: Average loss: 0.36649587, Accuracy: 88/99 (88.88888889%)\n",
      "\n",
      "Epoch 136 time: 1.0406761169433594 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 137 [0/891 (0.00000000%)]\tLoss: 0.11973923\n",
      "Train Epoch: 137 [320/891 (35.71428571%)]\tLoss: 0.23003364\n",
      "Train Epoch: 137 [640/891 (71.42857143%)]\tLoss: 0.29979876\n",
      "Train Set: Average loss: 0.26353407, Accuracy: 815/891 (91.47025814%)\n",
      "\n",
      "Test set: Average loss: 0.25341346, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 137 time: 1.064884901046753 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 138 [0/891 (0.00000000%)]\tLoss: 0.10486543\n",
      "Train Epoch: 138 [320/891 (35.71428571%)]\tLoss: 0.23935056\n",
      "Train Epoch: 138 [640/891 (71.42857143%)]\tLoss: 0.24459347\n",
      "Train Set: Average loss: 0.25295189, Accuracy: 818/891 (91.80695847%)\n",
      "\n",
      "Test set: Average loss: 0.30118897, Accuracy: 88/99 (88.88888889%)\n",
      "\n",
      "Epoch 138 time: 0.9860408306121826 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 139 [0/891 (0.00000000%)]\tLoss: 0.23384124\n",
      "Train Epoch: 139 [320/891 (35.71428571%)]\tLoss: 0.23198217\n",
      "Train Epoch: 139 [640/891 (71.42857143%)]\tLoss: 0.11897728\n",
      "Train Set: Average loss: 0.29453733, Accuracy: 802/891 (90.01122334%)\n",
      "\n",
      "Test set: Average loss: 0.25123221, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 139 time: 1.082653284072876 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 140 [0/891 (0.00000000%)]\tLoss: 0.34106219\n",
      "Train Epoch: 140 [320/891 (35.71428571%)]\tLoss: 0.56974757\n",
      "Train Epoch: 140 [640/891 (71.42857143%)]\tLoss: 0.20902774\n",
      "Train Set: Average loss: 0.32402658, Accuracy: 800/891 (89.78675645%)\n",
      "\n",
      "Test set: Average loss: 0.26491612, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 140 time: 1.0243265628814697 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 141 [0/891 (0.00000000%)]\tLoss: 0.19265160\n",
      "Train Epoch: 141 [320/891 (35.71428571%)]\tLoss: 0.34230119\n",
      "Train Epoch: 141 [640/891 (71.42857143%)]\tLoss: 0.45889127\n",
      "Train Set: Average loss: 0.33278383, Accuracy: 796/891 (89.33782267%)\n",
      "\n",
      "Test set: Average loss: 0.20754137, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 141 time: 1.0165979862213135 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 142 [0/891 (0.00000000%)]\tLoss: 0.08925486\n",
      "Train Epoch: 142 [320/891 (35.71428571%)]\tLoss: 0.23526773\n",
      "Train Epoch: 142 [640/891 (71.42857143%)]\tLoss: 0.13162667\n",
      "Train Set: Average loss: 0.23448838, Accuracy: 820/891 (92.03142536%)\n",
      "\n",
      "Test set: Average loss: 0.26612170, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 142 time: 1.0359344482421875 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 143 [0/891 (0.00000000%)]\tLoss: 0.14276153\n",
      "Train Epoch: 143 [320/891 (35.71428571%)]\tLoss: 0.16116732\n",
      "Train Epoch: 143 [640/891 (71.42857143%)]\tLoss: 0.33170485\n",
      "Train Set: Average loss: 0.21912504, Accuracy: 823/891 (92.36812570%)\n",
      "\n",
      "Test set: Average loss: 0.31405997, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 143 time: 1.0061872005462646 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 144 [0/891 (0.00000000%)]\tLoss: 0.05817893\n",
      "Train Epoch: 144 [320/891 (35.71428571%)]\tLoss: 0.48484164\n",
      "Train Epoch: 144 [640/891 (71.42857143%)]\tLoss: 0.15731514\n",
      "Train Set: Average loss: 0.27290372, Accuracy: 802/891 (90.01122334%)\n",
      "\n",
      "Test set: Average loss: 0.21886092, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 144 time: 1.021723985671997 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 145 [0/891 (0.00000000%)]\tLoss: 0.37514734\n",
      "Train Epoch: 145 [320/891 (35.71428571%)]\tLoss: 0.26179063\n",
      "Train Epoch: 145 [640/891 (71.42857143%)]\tLoss: 0.16351211\n",
      "Train Set: Average loss: 0.27489275, Accuracy: 807/891 (90.57239057%)\n",
      "\n",
      "Test set: Average loss: 0.24149233, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 145 time: 1.0021944046020508 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 146 [0/891 (0.00000000%)]\tLoss: 0.16935179\n",
      "Train Epoch: 146 [320/891 (35.71428571%)]\tLoss: 0.14810693\n",
      "Train Epoch: 146 [640/891 (71.42857143%)]\tLoss: 0.27414447\n",
      "Train Set: Average loss: 0.22673405, Accuracy: 824/891 (92.48035915%)\n",
      "\n",
      "Test set: Average loss: 0.27055457, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 146 time: 1.0207147598266602 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 147 [0/891 (0.00000000%)]\tLoss: 0.27925608\n",
      "Train Epoch: 147 [320/891 (35.71428571%)]\tLoss: 0.36918032\n",
      "Train Epoch: 147 [640/891 (71.42857143%)]\tLoss: 0.21650773\n",
      "Train Set: Average loss: 0.26772387, Accuracy: 804/891 (90.23569024%)\n",
      "\n",
      "Test set: Average loss: 0.27094351, Accuracy: 95/99 (95.95959596%)\n",
      "\n",
      "Epoch 147 time: 1.0278704166412354 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 148 [0/891 (0.00000000%)]\tLoss: 0.07581046\n",
      "Train Epoch: 148 [320/891 (35.71428571%)]\tLoss: 0.21795940\n",
      "Train Epoch: 148 [640/891 (71.42857143%)]\tLoss: 0.14026609\n",
      "Train Set: Average loss: 0.19247528, Accuracy: 830/891 (93.15375982%)\n",
      "\n",
      "Test set: Average loss: 0.28489246, Accuracy: 95/99 (95.95959596%)\n",
      "\n",
      "Epoch 148 time: 1.1159343719482422 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 149 [0/891 (0.00000000%)]\tLoss: 0.20065042\n",
      "Train Epoch: 149 [320/891 (35.71428571%)]\tLoss: 0.07790080\n",
      "Train Epoch: 149 [640/891 (71.42857143%)]\tLoss: 0.17391366\n",
      "Train Set: Average loss: 0.25549263, Accuracy: 820/891 (92.03142536%)\n",
      "\n",
      "Test set: Average loss: 0.33679726, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 149 time: 1.0143346786499023 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 150 [0/891 (0.00000000%)]\tLoss: 0.20851710\n",
      "Train Epoch: 150 [320/891 (35.71428571%)]\tLoss: 0.26319975\n",
      "Train Epoch: 150 [640/891 (71.42857143%)]\tLoss: 0.29757169\n",
      "Train Set: Average loss: 0.25055327, Accuracy: 822/891 (92.25589226%)\n",
      "\n",
      "Test set: Average loss: 0.27571119, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 150 time: 0.9877841472625732 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 151 [0/891 (0.00000000%)]\tLoss: 0.04341620\n",
      "Train Epoch: 151 [320/891 (35.71428571%)]\tLoss: 0.15597630\n",
      "Train Epoch: 151 [640/891 (71.42857143%)]\tLoss: 0.20193344\n",
      "Train Set: Average loss: 0.24576292, Accuracy: 812/891 (91.13355780%)\n",
      "\n",
      "Test set: Average loss: 0.30510377, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 151 time: 0.9932456016540527 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 152 [0/891 (0.00000000%)]\tLoss: 0.12426180\n",
      "Train Epoch: 152 [320/891 (35.71428571%)]\tLoss: 0.17479038\n",
      "Train Epoch: 152 [640/891 (71.42857143%)]\tLoss: 0.19618887\n",
      "Train Set: Average loss: 0.24149997, Accuracy: 826/891 (92.70482604%)\n",
      "\n",
      "Test set: Average loss: 0.29165451, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 152 time: 1.0017240047454834 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 153 [0/891 (0.00000000%)]\tLoss: 0.20282191\n",
      "Train Epoch: 153 [320/891 (35.71428571%)]\tLoss: 0.15374452\n",
      "Train Epoch: 153 [640/891 (71.42857143%)]\tLoss: 0.20404896\n",
      "Train Set: Average loss: 0.21133417, Accuracy: 830/891 (93.15375982%)\n",
      "\n",
      "Test set: Average loss: 0.29353755, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 153 time: 0.9777798652648926 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 154 [0/891 (0.00000000%)]\tLoss: 0.20074245\n",
      "Train Epoch: 154 [320/891 (35.71428571%)]\tLoss: 0.27024758\n",
      "Train Epoch: 154 [640/891 (71.42857143%)]\tLoss: 0.25812107\n",
      "Train Set: Average loss: 0.20766537, Accuracy: 820/891 (92.03142536%)\n",
      "\n",
      "Test set: Average loss: 0.31750054, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 154 time: 1.019378900527954 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 155 [0/891 (0.00000000%)]\tLoss: 0.20395330\n",
      "Train Epoch: 155 [320/891 (35.71428571%)]\tLoss: 0.13839722\n",
      "Train Epoch: 155 [640/891 (71.42857143%)]\tLoss: 0.27533990\n",
      "Train Set: Average loss: 0.19699175, Accuracy: 832/891 (93.37822671%)\n",
      "\n",
      "Test set: Average loss: 0.24456056, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 155 time: 0.9979829788208008 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 156 [0/891 (0.00000000%)]\tLoss: 0.27115822\n",
      "Train Epoch: 156 [320/891 (35.71428571%)]\tLoss: 0.12764829\n",
      "Train Epoch: 156 [640/891 (71.42857143%)]\tLoss: 0.15582925\n",
      "Train Set: Average loss: 0.26060779, Accuracy: 809/891 (90.79685746%)\n",
      "\n",
      "Test set: Average loss: 0.28698573, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 156 time: 0.9826452732086182 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 157 [0/891 (0.00000000%)]\tLoss: 0.07444447\n",
      "Train Epoch: 157 [320/891 (35.71428571%)]\tLoss: 0.24825823\n",
      "Train Epoch: 157 [640/891 (71.42857143%)]\tLoss: 0.11353761\n",
      "Train Set: Average loss: 0.21733265, Accuracy: 829/891 (93.04152637%)\n",
      "\n",
      "Test set: Average loss: 0.37825703, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 157 time: 1.00026273727417 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 158 [0/891 (0.00000000%)]\tLoss: 0.26206911\n",
      "Train Epoch: 158 [320/891 (35.71428571%)]\tLoss: 0.24990863\n",
      "Train Epoch: 158 [640/891 (71.42857143%)]\tLoss: 0.23149216\n",
      "Train Set: Average loss: 0.26971266, Accuracy: 811/891 (91.02132435%)\n",
      "\n",
      "Test set: Average loss: 0.32399884, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 158 time: 0.9823105335235596 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 159 [0/891 (0.00000000%)]\tLoss: 0.11321795\n",
      "Train Epoch: 159 [320/891 (35.71428571%)]\tLoss: 0.17984539\n",
      "Train Epoch: 159 [640/891 (71.42857143%)]\tLoss: 0.16197276\n",
      "Train Set: Average loss: 0.21505402, Accuracy: 827/891 (92.81705948%)\n",
      "\n",
      "Test set: Average loss: 0.29586066, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 159 time: 0.9785008430480957 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 160 [0/891 (0.00000000%)]\tLoss: 0.12404969\n",
      "Train Epoch: 160 [320/891 (35.71428571%)]\tLoss: 0.02313393\n",
      "Train Epoch: 160 [640/891 (71.42857143%)]\tLoss: 0.32231554\n",
      "Train Set: Average loss: 0.22739897, Accuracy: 829/891 (93.04152637%)\n",
      "\n",
      "Test set: Average loss: 0.31867461, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 160 time: 1.0031588077545166 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 161 [0/891 (0.00000000%)]\tLoss: 0.08480978\n",
      "Train Epoch: 161 [320/891 (35.71428571%)]\tLoss: 0.37311962\n",
      "Train Epoch: 161 [640/891 (71.42857143%)]\tLoss: 0.41885465\n",
      "Train Set: Average loss: 0.21125663, Accuracy: 833/891 (93.49046016%)\n",
      "\n",
      "Test set: Average loss: 0.32643274, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 161 time: 1.0025660991668701 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 162 [0/891 (0.00000000%)]\tLoss: 0.13233408\n",
      "Train Epoch: 162 [320/891 (35.71428571%)]\tLoss: 0.32097900\n",
      "Train Epoch: 162 [640/891 (71.42857143%)]\tLoss: 0.23091549\n",
      "Train Set: Average loss: 0.20460768, Accuracy: 833/891 (93.49046016%)\n",
      "\n",
      "Test set: Average loss: 0.29689947, Accuracy: 88/99 (88.88888889%)\n",
      "\n",
      "Epoch 162 time: 0.996570348739624 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 163 [0/891 (0.00000000%)]\tLoss: 0.17964053\n",
      "Train Epoch: 163 [320/891 (35.71428571%)]\tLoss: 0.07862368\n",
      "Train Epoch: 163 [640/891 (71.42857143%)]\tLoss: 0.08845437\n",
      "Train Set: Average loss: 0.19039449, Accuracy: 822/891 (92.25589226%)\n",
      "\n",
      "Test set: Average loss: 0.28165846, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 163 time: 0.9842369556427002 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 164 [0/891 (0.00000000%)]\tLoss: 0.03298753\n",
      "Train Epoch: 164 [320/891 (35.71428571%)]\tLoss: 0.14609200\n",
      "Train Epoch: 164 [640/891 (71.42857143%)]\tLoss: 0.16826785\n",
      "Train Set: Average loss: 0.20569047, Accuracy: 823/891 (92.36812570%)\n",
      "\n",
      "Test set: Average loss: 0.30312722, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 164 time: 0.9879233837127686 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 165 [0/891 (0.00000000%)]\tLoss: 0.12754685\n",
      "Train Epoch: 165 [320/891 (35.71428571%)]\tLoss: 0.08708674\n",
      "Train Epoch: 165 [640/891 (71.42857143%)]\tLoss: 0.31152168\n",
      "Train Set: Average loss: 0.21263330, Accuracy: 827/891 (92.81705948%)\n",
      "\n",
      "Test set: Average loss: 0.28275324, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 165 time: 1.003058910369873 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 166 [0/891 (0.00000000%)]\tLoss: 0.19926298\n",
      "Train Epoch: 166 [320/891 (35.71428571%)]\tLoss: 0.23603648\n",
      "Train Epoch: 166 [640/891 (71.42857143%)]\tLoss: 0.15458664\n",
      "Train Set: Average loss: 0.27684205, Accuracy: 811/891 (91.02132435%)\n",
      "\n",
      "Test set: Average loss: 0.35367488, Accuracy: 88/99 (88.88888889%)\n",
      "\n",
      "Epoch 166 time: 0.9764103889465332 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 167 [0/891 (0.00000000%)]\tLoss: 0.22704482\n",
      "Train Epoch: 167 [320/891 (35.71428571%)]\tLoss: 0.42181295\n",
      "Train Epoch: 167 [640/891 (71.42857143%)]\tLoss: 0.09426951\n",
      "Train Set: Average loss: 0.16550564, Accuracy: 838/891 (94.05162738%)\n",
      "\n",
      "Test set: Average loss: 0.28674251, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 167 time: 1.022648572921753 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 168 [0/891 (0.00000000%)]\tLoss: 0.25936684\n",
      "Train Epoch: 168 [320/891 (35.71428571%)]\tLoss: 0.45997715\n",
      "Train Epoch: 168 [640/891 (71.42857143%)]\tLoss: 0.17925835\n",
      "Train Set: Average loss: 0.20463329, Accuracy: 825/891 (92.59259259%)\n",
      "\n",
      "Test set: Average loss: 0.22326456, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 168 time: 1.014404535293579 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 169 [0/891 (0.00000000%)]\tLoss: 0.34646243\n",
      "Train Epoch: 169 [320/891 (35.71428571%)]\tLoss: 0.16183364\n",
      "Train Epoch: 169 [640/891 (71.42857143%)]\tLoss: 0.57033324\n",
      "Train Set: Average loss: 0.18077110, Accuracy: 830/891 (93.15375982%)\n",
      "\n",
      "Test set: Average loss: 0.27325441, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 169 time: 0.9827675819396973 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 170 [0/891 (0.00000000%)]\tLoss: 0.28517902\n",
      "Train Epoch: 170 [320/891 (35.71428571%)]\tLoss: 0.25649774\n",
      "Train Epoch: 170 [640/891 (71.42857143%)]\tLoss: 0.19424552\n",
      "Train Set: Average loss: 0.20856296, Accuracy: 829/891 (93.04152637%)\n",
      "\n",
      "Test set: Average loss: 0.36453882, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 170 time: 1.003422737121582 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 171 [0/891 (0.00000000%)]\tLoss: 0.31341231\n",
      "Train Epoch: 171 [320/891 (35.71428571%)]\tLoss: 0.13919717\n",
      "Train Epoch: 171 [640/891 (71.42857143%)]\tLoss: 0.16495442\n",
      "Train Set: Average loss: 0.22360704, Accuracy: 824/891 (92.48035915%)\n",
      "\n",
      "Test set: Average loss: 0.25879851, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 171 time: 1.0504825115203857 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 172 [0/891 (0.00000000%)]\tLoss: 0.28691751\n",
      "Train Epoch: 172 [320/891 (35.71428571%)]\tLoss: 0.21824169\n",
      "Train Epoch: 172 [640/891 (71.42857143%)]\tLoss: 0.24808955\n",
      "Train Set: Average loss: 0.19927862, Accuracy: 834/891 (93.60269360%)\n",
      "\n",
      "Test set: Average loss: 0.37363330, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 172 time: 1.0131549835205078 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 173 [0/891 (0.00000000%)]\tLoss: 0.28540337\n",
      "Train Epoch: 173 [320/891 (35.71428571%)]\tLoss: 0.09504545\n",
      "Train Epoch: 173 [640/891 (71.42857143%)]\tLoss: 0.15819782\n",
      "Train Set: Average loss: 0.20266457, Accuracy: 838/891 (94.05162738%)\n",
      "\n",
      "Test set: Average loss: 0.38098353, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 173 time: 1.0251398086547852 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 174 [0/891 (0.00000000%)]\tLoss: 0.16623396\n",
      "Train Epoch: 174 [320/891 (35.71428571%)]\tLoss: 0.02145952\n",
      "Train Epoch: 174 [640/891 (71.42857143%)]\tLoss: 0.10904390\n",
      "Train Set: Average loss: 0.18124221, Accuracy: 842/891 (94.50056117%)\n",
      "\n",
      "Test set: Average loss: 0.35630606, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 174 time: 0.9723191261291504 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 175 [0/891 (0.00000000%)]\tLoss: 0.20966089\n",
      "Train Epoch: 175 [320/891 (35.71428571%)]\tLoss: 0.37965691\n",
      "Train Epoch: 175 [640/891 (71.42857143%)]\tLoss: 0.51816344\n",
      "Train Set: Average loss: 0.26619067, Accuracy: 808/891 (90.68462402%)\n",
      "\n",
      "Test set: Average loss: 0.31461852, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 175 time: 0.9979195594787598 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 176 [0/891 (0.00000000%)]\tLoss: 0.17546308\n",
      "Train Epoch: 176 [320/891 (35.71428571%)]\tLoss: 0.01706034\n",
      "Train Epoch: 176 [640/891 (71.42857143%)]\tLoss: 0.39259446\n",
      "Train Set: Average loss: 0.20502048, Accuracy: 825/891 (92.59259259%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.32699575, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 176 time: 1.0119602680206299 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 177 [0/891 (0.00000000%)]\tLoss: 0.09983659\n",
      "Train Epoch: 177 [320/891 (35.71428571%)]\tLoss: 0.08706081\n",
      "Train Epoch: 177 [640/891 (71.42857143%)]\tLoss: 0.18306294\n",
      "Train Set: Average loss: 0.19716654, Accuracy: 833/891 (93.49046016%)\n",
      "\n",
      "Test set: Average loss: 0.35901158, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 177 time: 1.0177114009857178 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 178 [0/891 (0.00000000%)]\tLoss: 0.16733980\n",
      "Train Epoch: 178 [320/891 (35.71428571%)]\tLoss: 0.13176692\n",
      "Train Epoch: 178 [640/891 (71.42857143%)]\tLoss: 0.31892163\n",
      "Train Set: Average loss: 0.23529341, Accuracy: 827/891 (92.81705948%)\n",
      "\n",
      "Test set: Average loss: 0.20436730, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 178 time: 0.9946529865264893 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 179 [0/891 (0.00000000%)]\tLoss: 0.06914565\n",
      "Train Epoch: 179 [320/891 (35.71428571%)]\tLoss: 0.21822205\n",
      "Train Epoch: 179 [640/891 (71.42857143%)]\tLoss: 0.27187824\n",
      "Train Set: Average loss: 0.21319558, Accuracy: 831/891 (93.26599327%)\n",
      "\n",
      "Test set: Average loss: 0.29255169, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 179 time: 1.0234460830688477 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 180 [0/891 (0.00000000%)]\tLoss: 0.22460532\n",
      "Train Epoch: 180 [320/891 (35.71428571%)]\tLoss: 0.05912852\n",
      "Train Epoch: 180 [640/891 (71.42857143%)]\tLoss: 0.11515111\n",
      "Train Set: Average loss: 0.25111713, Accuracy: 825/891 (92.59259259%)\n",
      "\n",
      "Test set: Average loss: 0.33903599, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 180 time: 1.0635440349578857 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 181 [0/891 (0.00000000%)]\tLoss: 0.17958295\n",
      "Train Epoch: 181 [320/891 (35.71428571%)]\tLoss: 0.09610200\n",
      "Train Epoch: 181 [640/891 (71.42857143%)]\tLoss: 0.13777763\n",
      "Train Set: Average loss: 0.21327993, Accuracy: 826/891 (92.70482604%)\n",
      "\n",
      "Test set: Average loss: 0.30111480, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 181 time: 1.0155916213989258 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 182 [0/891 (0.00000000%)]\tLoss: 0.04545885\n",
      "Train Epoch: 182 [320/891 (35.71428571%)]\tLoss: 0.32221776\n",
      "Train Epoch: 182 [640/891 (71.42857143%)]\tLoss: 0.15261275\n",
      "Train Set: Average loss: 0.20515993, Accuracy: 833/891 (93.49046016%)\n",
      "\n",
      "Test set: Average loss: 0.26573416, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 182 time: 1.0545947551727295 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 183 [0/891 (0.00000000%)]\tLoss: 0.13908738\n",
      "Train Epoch: 183 [320/891 (35.71428571%)]\tLoss: 0.07703030\n",
      "Train Epoch: 183 [640/891 (71.42857143%)]\tLoss: 0.14082295\n",
      "Train Set: Average loss: 0.20597958, Accuracy: 830/891 (93.15375982%)\n",
      "\n",
      "Test set: Average loss: 0.30982239, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 183 time: 0.9849088191986084 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 184 [0/891 (0.00000000%)]\tLoss: 0.16865912\n",
      "Train Epoch: 184 [320/891 (35.71428571%)]\tLoss: 0.14633551\n",
      "Train Epoch: 184 [640/891 (71.42857143%)]\tLoss: 0.23450744\n",
      "Train Set: Average loss: 0.19946972, Accuracy: 827/891 (92.81705948%)\n",
      "\n",
      "Test set: Average loss: 0.29519445, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 184 time: 1.0433881282806396 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 185 [0/891 (0.00000000%)]\tLoss: 0.05034566\n",
      "Train Epoch: 185 [320/891 (35.71428571%)]\tLoss: 0.14142197\n",
      "Train Epoch: 185 [640/891 (71.42857143%)]\tLoss: 0.11325204\n",
      "Train Set: Average loss: 0.21960393, Accuracy: 824/891 (92.48035915%)\n",
      "\n",
      "Test set: Average loss: 0.28153932, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 185 time: 1.1094653606414795 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 186 [0/891 (0.00000000%)]\tLoss: 0.59071940\n",
      "Train Epoch: 186 [320/891 (35.71428571%)]\tLoss: 0.08959198\n",
      "Train Epoch: 186 [640/891 (71.42857143%)]\tLoss: 0.24772912\n",
      "Train Set: Average loss: 0.25531134, Accuracy: 818/891 (91.80695847%)\n",
      "\n",
      "Test set: Average loss: 0.33461390, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 186 time: 1.0125651359558105 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 187 [0/891 (0.00000000%)]\tLoss: 0.14353085\n",
      "Train Epoch: 187 [320/891 (35.71428571%)]\tLoss: 0.17899764\n",
      "Train Epoch: 187 [640/891 (71.42857143%)]\tLoss: 0.26990962\n",
      "Train Set: Average loss: 0.21144127, Accuracy: 820/891 (92.03142536%)\n",
      "\n",
      "Test set: Average loss: 0.30603723, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 187 time: 1.0793328285217285 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 188 [0/891 (0.00000000%)]\tLoss: 0.15903431\n",
      "Train Epoch: 188 [320/891 (35.71428571%)]\tLoss: 0.28881508\n",
      "Train Epoch: 188 [640/891 (71.42857143%)]\tLoss: 0.24350977\n",
      "Train Set: Average loss: 0.23904956, Accuracy: 812/891 (91.13355780%)\n",
      "\n",
      "Test set: Average loss: 0.29007054, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 188 time: 1.0408082008361816 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 189 [0/891 (0.00000000%)]\tLoss: 0.17794177\n",
      "Train Epoch: 189 [320/891 (35.71428571%)]\tLoss: 0.14477611\n",
      "Train Epoch: 189 [640/891 (71.42857143%)]\tLoss: 0.34031767\n",
      "Train Set: Average loss: 0.26497737, Accuracy: 806/891 (90.46015713%)\n",
      "\n",
      "Test set: Average loss: 0.44115560, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 189 time: 1.0162620544433594 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 190 [0/891 (0.00000000%)]\tLoss: 0.09959507\n",
      "Train Epoch: 190 [320/891 (35.71428571%)]\tLoss: 0.16598973\n",
      "Train Epoch: 190 [640/891 (71.42857143%)]\tLoss: 0.14939880\n",
      "Train Set: Average loss: 0.20184325, Accuracy: 827/891 (92.81705948%)\n",
      "\n",
      "Test set: Average loss: 0.38868274, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 190 time: 1.0443341732025146 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 191 [0/891 (0.00000000%)]\tLoss: 0.07866675\n",
      "Train Epoch: 191 [320/891 (35.71428571%)]\tLoss: 0.19251484\n",
      "Train Epoch: 191 [640/891 (71.42857143%)]\tLoss: 0.07835186\n",
      "Train Set: Average loss: 0.18642096, Accuracy: 833/891 (93.49046016%)\n",
      "\n",
      "Test set: Average loss: 0.34950435, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 191 time: 1.028320074081421 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 192 [0/891 (0.00000000%)]\tLoss: 0.61877197\n",
      "Train Epoch: 192 [320/891 (35.71428571%)]\tLoss: 0.09516716\n",
      "Train Epoch: 192 [640/891 (71.42857143%)]\tLoss: 0.22628325\n",
      "Train Set: Average loss: 0.19195652, Accuracy: 844/891 (94.72502806%)\n",
      "\n",
      "Test set: Average loss: 0.32920211, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 192 time: 0.991682767868042 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 193 [0/891 (0.00000000%)]\tLoss: 0.13867521\n",
      "Train Epoch: 193 [320/891 (35.71428571%)]\tLoss: 0.11972481\n",
      "Train Epoch: 193 [640/891 (71.42857143%)]\tLoss: 0.30777958\n",
      "Train Set: Average loss: 0.19980968, Accuracy: 834/891 (93.60269360%)\n",
      "\n",
      "Test set: Average loss: 0.35511103, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 193 time: 1.0351216793060303 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 194 [0/891 (0.00000000%)]\tLoss: 0.07760197\n",
      "Train Epoch: 194 [320/891 (35.71428571%)]\tLoss: 0.23883209\n",
      "Train Epoch: 194 [640/891 (71.42857143%)]\tLoss: 0.07569295\n",
      "Train Set: Average loss: 0.22877841, Accuracy: 821/891 (92.14365881%)\n",
      "\n",
      "Test set: Average loss: 0.35510840, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 194 time: 1.0909242630004883 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 195 [0/891 (0.00000000%)]\tLoss: 0.18703318\n",
      "Train Epoch: 195 [320/891 (35.71428571%)]\tLoss: 0.04342377\n",
      "Train Epoch: 195 [640/891 (71.42857143%)]\tLoss: 0.09016967\n",
      "Train Set: Average loss: 0.20296163, Accuracy: 832/891 (93.37822671%)\n",
      "\n",
      "Test set: Average loss: 0.32672233, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 195 time: 1.056081771850586 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 196 [0/891 (0.00000000%)]\tLoss: 0.19757780\n",
      "Train Epoch: 196 [320/891 (35.71428571%)]\tLoss: 0.09372997\n",
      "Train Epoch: 196 [640/891 (71.42857143%)]\tLoss: 0.12280661\n",
      "Train Set: Average loss: 0.18844710, Accuracy: 836/891 (93.82716049%)\n",
      "\n",
      "Test set: Average loss: 0.36395871, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 196 time: 1.0091934204101562 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 197 [0/891 (0.00000000%)]\tLoss: 0.10099179\n",
      "Train Epoch: 197 [320/891 (35.71428571%)]\tLoss: 0.16874892\n",
      "Train Epoch: 197 [640/891 (71.42857143%)]\tLoss: 0.21789682\n",
      "Train Set: Average loss: 0.19178458, Accuracy: 832/891 (93.37822671%)\n",
      "\n",
      "Test set: Average loss: 0.35806088, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 197 time: 1.0224006175994873 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 198 [0/891 (0.00000000%)]\tLoss: 0.40863419\n",
      "Train Epoch: 198 [320/891 (35.71428571%)]\tLoss: 0.10543275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 198 [640/891 (71.42857143%)]\tLoss: 0.55559337\n",
      "Train Set: Average loss: 0.22803597, Accuracy: 822/891 (92.25589226%)\n",
      "\n",
      "Test set: Average loss: 0.39269116, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 198 time: 1.0237312316894531 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 199 [0/891 (0.00000000%)]\tLoss: 0.54014546\n",
      "Train Epoch: 199 [320/891 (35.71428571%)]\tLoss: 0.14117661\n",
      "Train Epoch: 199 [640/891 (71.42857143%)]\tLoss: 0.14651072\n",
      "Train Set: Average loss: 0.21819508, Accuracy: 827/891 (92.81705948%)\n",
      "\n",
      "Test set: Average loss: 0.36597735, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 199 time: 0.9948790073394775 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 200 [0/891 (0.00000000%)]\tLoss: 0.10285705\n",
      "Train Epoch: 200 [320/891 (35.71428571%)]\tLoss: 0.07665014\n",
      "Train Epoch: 200 [640/891 (71.42857143%)]\tLoss: 0.46933070\n",
      "Train Set: Average loss: 0.16894165, Accuracy: 837/891 (93.93939394%)\n",
      "\n",
      "Test set: Average loss: 0.31393328, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 200 time: 1.0137341022491455 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 201 [0/891 (0.00000000%)]\tLoss: 0.06234977\n",
      "Train Epoch: 201 [320/891 (35.71428571%)]\tLoss: 0.08069745\n",
      "Train Epoch: 201 [640/891 (71.42857143%)]\tLoss: 0.17322460\n",
      "Train Set: Average loss: 0.18338388, Accuracy: 841/891 (94.38832772%)\n",
      "\n",
      "Test set: Average loss: 0.31680228, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 201 time: 0.9826445579528809 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 202 [0/891 (0.00000000%)]\tLoss: 0.17833364\n",
      "Train Epoch: 202 [320/891 (35.71428571%)]\tLoss: 0.26909953\n",
      "Train Epoch: 202 [640/891 (71.42857143%)]\tLoss: 0.22396910\n",
      "Train Set: Average loss: 0.18837778, Accuracy: 839/891 (94.16386083%)\n",
      "\n",
      "Test set: Average loss: 0.26426049, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 202 time: 1.0870270729064941 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 203 [0/891 (0.00000000%)]\tLoss: 0.12630594\n",
      "Train Epoch: 203 [320/891 (35.71428571%)]\tLoss: 0.11368680\n",
      "Train Epoch: 203 [640/891 (71.42857143%)]\tLoss: 0.16252393\n",
      "Train Set: Average loss: 0.19110491, Accuracy: 825/891 (92.59259259%)\n",
      "\n",
      "Test set: Average loss: 0.31044503, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 203 time: 1.0508642196655273 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 204 [0/891 (0.00000000%)]\tLoss: 0.36467582\n",
      "Train Epoch: 204 [320/891 (35.71428571%)]\tLoss: 0.07370812\n",
      "Train Epoch: 204 [640/891 (71.42857143%)]\tLoss: 0.05049312\n",
      "Train Set: Average loss: 0.18602873, Accuracy: 831/891 (93.26599327%)\n",
      "\n",
      "Test set: Average loss: 0.25336780, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 204 time: 1.0906968116760254 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 205 [0/891 (0.00000000%)]\tLoss: 0.11161494\n",
      "Train Epoch: 205 [320/891 (35.71428571%)]\tLoss: 0.04410771\n",
      "Train Epoch: 205 [640/891 (71.42857143%)]\tLoss: 0.01131392\n",
      "Train Set: Average loss: 0.15372707, Accuracy: 851/891 (95.51066218%)\n",
      "\n",
      "Test set: Average loss: 0.23340451, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 205 time: 1.032789945602417 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 206 [0/891 (0.00000000%)]\tLoss: 0.41853908\n",
      "Train Epoch: 206 [320/891 (35.71428571%)]\tLoss: 0.19650587\n",
      "Train Epoch: 206 [640/891 (71.42857143%)]\tLoss: 0.12186557\n",
      "Train Set: Average loss: 0.20991181, Accuracy: 830/891 (93.15375982%)\n",
      "\n",
      "Test set: Average loss: 0.32660696, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 206 time: 1.0397274494171143 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 207 [0/891 (0.00000000%)]\tLoss: 0.05740130\n",
      "Train Epoch: 207 [320/891 (35.71428571%)]\tLoss: 0.27117366\n",
      "Train Epoch: 207 [640/891 (71.42857143%)]\tLoss: 0.13109493\n",
      "Train Set: Average loss: 0.18283248, Accuracy: 827/891 (92.81705948%)\n",
      "\n",
      "Test set: Average loss: 0.27269687, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 207 time: 1.0710718631744385 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 208 [0/891 (0.00000000%)]\tLoss: 0.13741833\n",
      "Train Epoch: 208 [320/891 (35.71428571%)]\tLoss: 0.39847726\n",
      "Train Epoch: 208 [640/891 (71.42857143%)]\tLoss: 0.32587296\n",
      "Train Set: Average loss: 0.24656940, Accuracy: 832/891 (93.37822671%)\n",
      "\n",
      "Test set: Average loss: 0.37006621, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 208 time: 0.9952151775360107 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 209 [0/891 (0.00000000%)]\tLoss: 0.49304041\n",
      "Train Epoch: 209 [320/891 (35.71428571%)]\tLoss: 0.12873513\n",
      "Train Epoch: 209 [640/891 (71.42857143%)]\tLoss: 0.23085159\n",
      "Train Set: Average loss: 0.15536399, Accuracy: 840/891 (94.27609428%)\n",
      "\n",
      "Test set: Average loss: 0.36533583, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 209 time: 0.9910669326782227 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 210 [0/891 (0.00000000%)]\tLoss: 0.07586128\n",
      "Train Epoch: 210 [320/891 (35.71428571%)]\tLoss: 0.24448991\n",
      "Train Epoch: 210 [640/891 (71.42857143%)]\tLoss: 0.53344166\n",
      "Train Set: Average loss: 0.19676536, Accuracy: 833/891 (93.49046016%)\n",
      "\n",
      "Test set: Average loss: 0.39130631, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 210 time: 0.980410099029541 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 211 [0/891 (0.00000000%)]\tLoss: 0.20554888\n",
      "Train Epoch: 211 [320/891 (35.71428571%)]\tLoss: 0.04505908\n",
      "Train Epoch: 211 [640/891 (71.42857143%)]\tLoss: 0.46826646\n",
      "Train Set: Average loss: 0.17907195, Accuracy: 839/891 (94.16386083%)\n",
      "\n",
      "Test set: Average loss: 0.37735803, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 211 time: 1.0171654224395752 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 212 [0/891 (0.00000000%)]\tLoss: 0.30998588\n",
      "Train Epoch: 212 [320/891 (35.71428571%)]\tLoss: 0.22166705\n",
      "Train Epoch: 212 [640/891 (71.42857143%)]\tLoss: 0.31845385\n",
      "Train Set: Average loss: 0.19419679, Accuracy: 831/891 (93.26599327%)\n",
      "\n",
      "Test set: Average loss: 0.33200715, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 212 time: 0.9925234317779541 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 213 [0/891 (0.00000000%)]\tLoss: 0.05466092\n",
      "Train Epoch: 213 [320/891 (35.71428571%)]\tLoss: 0.21870381\n",
      "Train Epoch: 213 [640/891 (71.42857143%)]\tLoss: 0.19299799\n",
      "Train Set: Average loss: 0.24755022, Accuracy: 819/891 (91.91919192%)\n",
      "\n",
      "Test set: Average loss: 0.34369101, Accuracy: 88/99 (88.88888889%)\n",
      "\n",
      "Epoch 213 time: 1.0171942710876465 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 214 [0/891 (0.00000000%)]\tLoss: 0.07822293\n",
      "Train Epoch: 214 [320/891 (35.71428571%)]\tLoss: 0.05568838\n",
      "Train Epoch: 214 [640/891 (71.42857143%)]\tLoss: 0.22305900\n",
      "Train Set: Average loss: 0.17486886, Accuracy: 833/891 (93.49046016%)\n",
      "\n",
      "Test set: Average loss: 0.32425275, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 214 time: 1.020782232284546 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 215 [0/891 (0.00000000%)]\tLoss: 0.24009466\n",
      "Train Epoch: 215 [320/891 (35.71428571%)]\tLoss: 0.15608114\n",
      "Train Epoch: 215 [640/891 (71.42857143%)]\tLoss: 0.19675618\n",
      "Train Set: Average loss: 0.20151248, Accuracy: 833/891 (93.49046016%)\n",
      "\n",
      "Test set: Average loss: 0.31442355, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 215 time: 1.0885963439941406 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 216 [0/891 (0.00000000%)]\tLoss: 0.07120645\n",
      "Train Epoch: 216 [320/891 (35.71428571%)]\tLoss: 0.11399484\n",
      "Train Epoch: 216 [640/891 (71.42857143%)]\tLoss: 0.37329930\n",
      "Train Set: Average loss: 0.13854274, Accuracy: 848/891 (95.17396184%)\n",
      "\n",
      "Test set: Average loss: 0.30150947, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 216 time: 1.0627763271331787 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 217 [0/891 (0.00000000%)]\tLoss: 0.23912343\n",
      "Train Epoch: 217 [320/891 (35.71428571%)]\tLoss: 0.24289116\n",
      "Train Epoch: 217 [640/891 (71.42857143%)]\tLoss: 0.11047038\n",
      "Train Set: Average loss: 0.16448906, Accuracy: 848/891 (95.17396184%)\n",
      "\n",
      "Test set: Average loss: 0.25830352, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 217 time: 0.9946675300598145 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 218 [0/891 (0.00000000%)]\tLoss: 0.07241583\n",
      "Train Epoch: 218 [320/891 (35.71428571%)]\tLoss: 0.19273776\n",
      "Train Epoch: 218 [640/891 (71.42857143%)]\tLoss: 0.26442736\n",
      "Train Set: Average loss: 0.17726289, Accuracy: 839/891 (94.16386083%)\n",
      "\n",
      "Test set: Average loss: 0.28329477, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 218 time: 1.025672435760498 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 219 [0/891 (0.00000000%)]\tLoss: 0.10114908\n",
      "Train Epoch: 219 [320/891 (35.71428571%)]\tLoss: 0.38987547\n",
      "Train Epoch: 219 [640/891 (71.42857143%)]\tLoss: 0.13289702\n",
      "Train Set: Average loss: 0.19515819, Accuracy: 838/891 (94.05162738%)\n",
      "\n",
      "Test set: Average loss: 0.29678711, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 219 time: 0.9536709785461426 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 220 [0/891 (0.00000000%)]\tLoss: 0.04600012\n",
      "Train Epoch: 220 [320/891 (35.71428571%)]\tLoss: 0.18040186\n",
      "Train Epoch: 220 [640/891 (71.42857143%)]\tLoss: 0.10739914\n",
      "Train Set: Average loss: 0.19602213, Accuracy: 837/891 (93.93939394%)\n",
      "\n",
      "Test set: Average loss: 0.22145832, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 220 time: 1.0125458240509033 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 221 [0/891 (0.00000000%)]\tLoss: 0.19757754\n",
      "Train Epoch: 221 [320/891 (35.71428571%)]\tLoss: 0.16402841\n",
      "Train Epoch: 221 [640/891 (71.42857143%)]\tLoss: 0.22960103\n",
      "Train Set: Average loss: 0.22106212, Accuracy: 820/891 (92.03142536%)\n",
      "\n",
      "Test set: Average loss: 0.29604752, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 221 time: 1.056227445602417 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 222 [0/891 (0.00000000%)]\tLoss: 0.38312396\n",
      "Train Epoch: 222 [320/891 (35.71428571%)]\tLoss: 0.22019333\n",
      "Train Epoch: 222 [640/891 (71.42857143%)]\tLoss: 0.15321529\n",
      "Train Set: Average loss: 0.18827382, Accuracy: 831/891 (93.26599327%)\n",
      "\n",
      "Test set: Average loss: 0.23948621, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 222 time: 1.0513951778411865 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 223 [0/891 (0.00000000%)]\tLoss: 0.43119538\n",
      "Train Epoch: 223 [320/891 (35.71428571%)]\tLoss: 0.16994548\n",
      "Train Epoch: 223 [640/891 (71.42857143%)]\tLoss: 0.14410853\n",
      "Train Set: Average loss: 0.17022867, Accuracy: 839/891 (94.16386083%)\n",
      "\n",
      "Test set: Average loss: 0.21083601, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 223 time: 1.0832064151763916 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 224 [0/891 (0.00000000%)]\tLoss: 0.05505842\n",
      "Train Epoch: 224 [320/891 (35.71428571%)]\tLoss: 0.23703998\n",
      "Train Epoch: 224 [640/891 (71.42857143%)]\tLoss: 0.26275098\n",
      "Train Set: Average loss: 0.19728915, Accuracy: 830/891 (93.15375982%)\n",
      "\n",
      "Test set: Average loss: 0.21174974, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 224 time: 1.001082420349121 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 225 [0/891 (0.00000000%)]\tLoss: 0.10878050\n",
      "Train Epoch: 225 [320/891 (35.71428571%)]\tLoss: 0.14924920\n",
      "Train Epoch: 225 [640/891 (71.42857143%)]\tLoss: 0.15366179\n",
      "Train Set: Average loss: 0.17012892, Accuracy: 838/891 (94.05162738%)\n",
      "\n",
      "Test set: Average loss: 0.20744975, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 225 time: 1.025259017944336 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 226 [0/891 (0.00000000%)]\tLoss: 0.11783797\n",
      "Train Epoch: 226 [320/891 (35.71428571%)]\tLoss: 0.20089626\n",
      "Train Epoch: 226 [640/891 (71.42857143%)]\tLoss: 0.43249249\n",
      "Train Set: Average loss: 0.17525835, Accuracy: 843/891 (94.61279461%)\n",
      "\n",
      "Test set: Average loss: 0.21632464, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 226 time: 1.1932227611541748 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 227 [0/891 (0.00000000%)]\tLoss: 0.19023728\n",
      "Train Epoch: 227 [320/891 (35.71428571%)]\tLoss: 0.02179080\n",
      "Train Epoch: 227 [640/891 (71.42857143%)]\tLoss: 0.14227337\n",
      "Train Set: Average loss: 0.18068198, Accuracy: 847/891 (95.06172840%)\n",
      "\n",
      "Test set: Average loss: 0.24374056, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 227 time: 1.166461706161499 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 228 [0/891 (0.00000000%)]\tLoss: 0.09100667\n",
      "Train Epoch: 228 [320/891 (35.71428571%)]\tLoss: 0.24583018\n",
      "Train Epoch: 228 [640/891 (71.42857143%)]\tLoss: 0.06098777\n",
      "Train Set: Average loss: 0.15247649, Accuracy: 845/891 (94.83726150%)\n",
      "\n",
      "Test set: Average loss: 0.21943495, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 228 time: 1.0531504154205322 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 229 [0/891 (0.00000000%)]\tLoss: 0.10713908\n",
      "Train Epoch: 229 [320/891 (35.71428571%)]\tLoss: 0.26064038\n",
      "Train Epoch: 229 [640/891 (71.42857143%)]\tLoss: 0.11304343\n",
      "Train Set: Average loss: 0.18270466, Accuracy: 837/891 (93.93939394%)\n",
      "\n",
      "Test set: Average loss: 0.21787934, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 229 time: 1.0316205024719238 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 230 [0/891 (0.00000000%)]\tLoss: 0.02667719\n",
      "Train Epoch: 230 [320/891 (35.71428571%)]\tLoss: 0.55960619\n",
      "Train Epoch: 230 [640/891 (71.42857143%)]\tLoss: 0.03413498\n",
      "Train Set: Average loss: 0.20578399, Accuracy: 839/891 (94.16386083%)\n",
      "\n",
      "Test set: Average loss: 0.22631177, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 230 time: 1.0440237522125244 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 231 [0/891 (0.00000000%)]\tLoss: 0.19241613\n",
      "Train Epoch: 231 [320/891 (35.71428571%)]\tLoss: 0.03497589\n",
      "Train Epoch: 231 [640/891 (71.42857143%)]\tLoss: 0.09790581\n",
      "Train Set: Average loss: 0.13301890, Accuracy: 846/891 (94.94949495%)\n",
      "\n",
      "Test set: Average loss: 0.21208382, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 231 time: 1.0407235622406006 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 232 [0/891 (0.00000000%)]\tLoss: 0.13075608\n",
      "Train Epoch: 232 [320/891 (35.71428571%)]\tLoss: 0.19823301\n",
      "Train Epoch: 232 [640/891 (71.42857143%)]\tLoss: 0.13887200\n",
      "Train Set: Average loss: 0.09760539, Accuracy: 853/891 (95.73512907%)\n",
      "\n",
      "Test set: Average loss: 0.23790874, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 232 time: 1.0146193504333496 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 233 [0/891 (0.00000000%)]\tLoss: 0.05612054\n",
      "Train Epoch: 233 [320/891 (35.71428571%)]\tLoss: 0.37800342\n",
      "Train Epoch: 233 [640/891 (71.42857143%)]\tLoss: 0.07734290\n",
      "Train Set: Average loss: 0.18851846, Accuracy: 829/891 (93.04152637%)\n",
      "\n",
      "Test set: Average loss: 0.21316921, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 233 time: 0.9854717254638672 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 234 [0/891 (0.00000000%)]\tLoss: 0.00997311\n",
      "Train Epoch: 234 [320/891 (35.71428571%)]\tLoss: 0.06564483\n",
      "Train Epoch: 234 [640/891 (71.42857143%)]\tLoss: 0.26362801\n",
      "Train Set: Average loss: 0.15123924, Accuracy: 844/891 (94.72502806%)\n",
      "\n",
      "Test set: Average loss: 0.15013406, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 234 time: 1.0001204013824463 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 235 [0/891 (0.00000000%)]\tLoss: 0.10952133\n",
      "Train Epoch: 235 [320/891 (35.71428571%)]\tLoss: 0.06101054\n",
      "Train Epoch: 235 [640/891 (71.42857143%)]\tLoss: 0.11579490\n",
      "Train Set: Average loss: 0.16843373, Accuracy: 832/891 (93.37822671%)\n",
      "\n",
      "Test set: Average loss: 0.24086372, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 235 time: 1.011420726776123 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 236 [0/891 (0.00000000%)]\tLoss: 0.23940042\n",
      "Train Epoch: 236 [320/891 (35.71428571%)]\tLoss: 0.11046219\n",
      "Train Epoch: 236 [640/891 (71.42857143%)]\tLoss: 0.07063693\n",
      "Train Set: Average loss: 0.15726041, Accuracy: 847/891 (95.06172840%)\n",
      "\n",
      "Test set: Average loss: 0.26511555, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 236 time: 1.0330471992492676 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 237 [0/891 (0.00000000%)]\tLoss: 0.04208937\n",
      "Train Epoch: 237 [320/891 (35.71428571%)]\tLoss: 0.13470632\n",
      "Train Epoch: 237 [640/891 (71.42857143%)]\tLoss: 0.05584395\n",
      "Train Set: Average loss: 0.11107580, Accuracy: 861/891 (96.63299663%)\n",
      "\n",
      "Test set: Average loss: 0.33825670, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 237 time: 1.036273717880249 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 238 [0/891 (0.00000000%)]\tLoss: 0.03796339\n",
      "Train Epoch: 238 [320/891 (35.71428571%)]\tLoss: 0.10903543\n",
      "Train Epoch: 238 [640/891 (71.42857143%)]\tLoss: 0.19362181\n",
      "Train Set: Average loss: 0.17996332, Accuracy: 833/891 (93.49046016%)\n",
      "\n",
      "Test set: Average loss: 0.27691855, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 238 time: 0.9942712783813477 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 239 [0/891 (0.00000000%)]\tLoss: 0.10438654\n",
      "Train Epoch: 239 [320/891 (35.71428571%)]\tLoss: 0.17612743\n",
      "Train Epoch: 239 [640/891 (71.42857143%)]\tLoss: 0.28662658\n",
      "Train Set: Average loss: 0.11167687, Accuracy: 863/891 (96.85746352%)\n",
      "\n",
      "Test set: Average loss: 0.30419542, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 239 time: 0.9806976318359375 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 240 [0/891 (0.00000000%)]\tLoss: 0.15455884\n",
      "Train Epoch: 240 [320/891 (35.71428571%)]\tLoss: 0.19174945\n",
      "Train Epoch: 240 [640/891 (71.42857143%)]\tLoss: 0.30342126\n",
      "Train Set: Average loss: 0.17621156, Accuracy: 839/891 (94.16386083%)\n",
      "\n",
      "Test set: Average loss: 0.32406217, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 240 time: 1.0122723579406738 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 241 [0/891 (0.00000000%)]\tLoss: 0.09688079\n",
      "Train Epoch: 241 [320/891 (35.71428571%)]\tLoss: 0.16898113\n",
      "Train Epoch: 241 [640/891 (71.42857143%)]\tLoss: 0.01494145\n",
      "Train Set: Average loss: 0.17161532, Accuracy: 842/891 (94.50056117%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.37419417, Accuracy: 88/99 (88.88888889%)\n",
      "\n",
      "Epoch 241 time: 1.011678695678711 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 242 [0/891 (0.00000000%)]\tLoss: 0.14746594\n",
      "Train Epoch: 242 [320/891 (35.71428571%)]\tLoss: 0.03875464\n",
      "Train Epoch: 242 [640/891 (71.42857143%)]\tLoss: 0.27047896\n",
      "Train Set: Average loss: 0.15448872, Accuracy: 843/891 (94.61279461%)\n",
      "\n",
      "Test set: Average loss: 0.37224708, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 242 time: 0.9987256526947021 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 243 [0/891 (0.00000000%)]\tLoss: 0.05509371\n",
      "Train Epoch: 243 [320/891 (35.71428571%)]\tLoss: 0.28373480\n",
      "Train Epoch: 243 [640/891 (71.42857143%)]\tLoss: 0.20098639\n",
      "Train Set: Average loss: 0.15459278, Accuracy: 850/891 (95.39842873%)\n",
      "\n",
      "Test set: Average loss: 0.26730006, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 243 time: 1.0149829387664795 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 244 [0/891 (0.00000000%)]\tLoss: 0.15675223\n",
      "Train Epoch: 244 [320/891 (35.71428571%)]\tLoss: 0.17710346\n",
      "Train Epoch: 244 [640/891 (71.42857143%)]\tLoss: 0.04673010\n",
      "Train Set: Average loss: 0.14948995, Accuracy: 849/891 (95.28619529%)\n",
      "\n",
      "Test set: Average loss: 0.30646419, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 244 time: 1.0181045532226562 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 245 [0/891 (0.00000000%)]\tLoss: 0.07999578\n",
      "Train Epoch: 245 [320/891 (35.71428571%)]\tLoss: 0.10645580\n",
      "Train Epoch: 245 [640/891 (71.42857143%)]\tLoss: 0.15823787\n",
      "Train Set: Average loss: 0.14553399, Accuracy: 853/891 (95.73512907%)\n",
      "\n",
      "Test set: Average loss: 0.32043176, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 245 time: 0.9948227405548096 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 246 [0/891 (0.00000000%)]\tLoss: 0.01564693\n",
      "Train Epoch: 246 [320/891 (35.71428571%)]\tLoss: 0.06425989\n",
      "Train Epoch: 246 [640/891 (71.42857143%)]\tLoss: 0.07167989\n",
      "Train Set: Average loss: 0.12772322, Accuracy: 855/891 (95.95959596%)\n",
      "\n",
      "Test set: Average loss: 0.25308158, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 246 time: 1.006389856338501 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 247 [0/891 (0.00000000%)]\tLoss: 0.14546937\n",
      "Train Epoch: 247 [320/891 (35.71428571%)]\tLoss: 0.12391090\n",
      "Train Epoch: 247 [640/891 (71.42857143%)]\tLoss: 0.08063823\n",
      "Train Set: Average loss: 0.13315062, Accuracy: 848/891 (95.17396184%)\n",
      "\n",
      "Test set: Average loss: 0.32316888, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 247 time: 1.0338752269744873 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 248 [0/891 (0.00000000%)]\tLoss: 0.16371483\n",
      "Train Epoch: 248 [320/891 (35.71428571%)]\tLoss: 0.13602382\n",
      "Train Epoch: 248 [640/891 (71.42857143%)]\tLoss: 0.24092317\n",
      "Train Set: Average loss: 0.16967397, Accuracy: 840/891 (94.27609428%)\n",
      "\n",
      "Test set: Average loss: 0.35216322, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 248 time: 0.9892885684967041 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 249 [0/891 (0.00000000%)]\tLoss: 0.02231723\n",
      "Train Epoch: 249 [320/891 (35.71428571%)]\tLoss: 0.05367976\n",
      "Train Epoch: 249 [640/891 (71.42857143%)]\tLoss: 0.22126722\n",
      "Train Set: Average loss: 0.14543051, Accuracy: 847/891 (95.06172840%)\n",
      "\n",
      "Test set: Average loss: 0.32780445, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 249 time: 0.9977056980133057 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.0075\n",
      "Train Epoch: 250 [0/891 (0.00000000%)]\tLoss: 0.15737009\n",
      "Train Epoch: 250 [320/891 (35.71428571%)]\tLoss: 0.29625261\n",
      "Train Epoch: 250 [640/891 (71.42857143%)]\tLoss: 0.02575254\n",
      "Train Set: Average loss: 0.13728203, Accuracy: 844/891 (94.72502806%)\n",
      "\n",
      "Test set: Average loss: 0.47566639, Accuracy: 88/99 (88.88888889%)\n",
      "\n",
      "Epoch 250 time: 1.0042672157287598 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 251 [0/891 (0.00000000%)]\tLoss: 0.58203280\n",
      "Train Epoch: 251 [320/891 (35.71428571%)]\tLoss: 0.12772846\n",
      "Train Epoch: 251 [640/891 (71.42857143%)]\tLoss: 0.37367243\n",
      "Train Set: Average loss: 0.18721647, Accuracy: 840/891 (94.27609428%)\n",
      "\n",
      "Test set: Average loss: 0.29160723, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 251 time: 1.009338140487671 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 252 [0/891 (0.00000000%)]\tLoss: 0.08529806\n",
      "Train Epoch: 252 [320/891 (35.71428571%)]\tLoss: 0.29560533\n",
      "Train Epoch: 252 [640/891 (71.42857143%)]\tLoss: 0.15522993\n",
      "Train Set: Average loss: 0.11257091, Accuracy: 859/891 (96.40852974%)\n",
      "\n",
      "Test set: Average loss: 0.30379908, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 252 time: 1.0307514667510986 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 253 [0/891 (0.00000000%)]\tLoss: 0.05280572\n",
      "Train Epoch: 253 [320/891 (35.71428571%)]\tLoss: 0.06581616\n",
      "Train Epoch: 253 [640/891 (71.42857143%)]\tLoss: 0.09328854\n",
      "Train Set: Average loss: 0.13469298, Accuracy: 853/891 (95.73512907%)\n",
      "\n",
      "Test set: Average loss: 0.34399587, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 253 time: 0.9983615875244141 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 254 [0/891 (0.00000000%)]\tLoss: 0.12181205\n",
      "Train Epoch: 254 [320/891 (35.71428571%)]\tLoss: 0.08935341\n",
      "Train Epoch: 254 [640/891 (71.42857143%)]\tLoss: 0.21513081\n",
      "Train Set: Average loss: 0.11658934, Accuracy: 853/891 (95.73512907%)\n",
      "\n",
      "Test set: Average loss: 0.34623724, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 254 time: 1.018476963043213 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 255 [0/891 (0.00000000%)]\tLoss: 0.07065624\n",
      "Train Epoch: 255 [320/891 (35.71428571%)]\tLoss: 0.06354135\n",
      "Train Epoch: 255 [640/891 (71.42857143%)]\tLoss: 0.14414221\n",
      "Train Set: Average loss: 0.13425997, Accuracy: 851/891 (95.51066218%)\n",
      "\n",
      "Test set: Average loss: 0.29592680, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 255 time: 1.0054311752319336 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 256 [0/891 (0.00000000%)]\tLoss: 0.23337188\n",
      "Train Epoch: 256 [320/891 (35.71428571%)]\tLoss: 0.15466136\n",
      "Train Epoch: 256 [640/891 (71.42857143%)]\tLoss: 0.06830812\n",
      "Train Set: Average loss: 0.13981590, Accuracy: 852/891 (95.62289562%)\n",
      "\n",
      "Test set: Average loss: 0.27003159, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 256 time: 0.9864518642425537 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 257 [0/891 (0.00000000%)]\tLoss: 0.08217430\n",
      "Train Epoch: 257 [320/891 (35.71428571%)]\tLoss: 0.04395348\n",
      "Train Epoch: 257 [640/891 (71.42857143%)]\tLoss: 0.10210246\n",
      "Train Set: Average loss: 0.15566651, Accuracy: 845/891 (94.83726150%)\n",
      "\n",
      "Test set: Average loss: 0.28367041, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 257 time: 0.9973089694976807 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 258 [0/891 (0.00000000%)]\tLoss: 0.06940800\n",
      "Train Epoch: 258 [320/891 (35.71428571%)]\tLoss: 0.15698195\n",
      "Train Epoch: 258 [640/891 (71.42857143%)]\tLoss: 0.14552206\n",
      "Train Set: Average loss: 0.15226581, Accuracy: 845/891 (94.83726150%)\n",
      "\n",
      "Test set: Average loss: 0.31374741, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 258 time: 0.9876902103424072 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 259 [0/891 (0.00000000%)]\tLoss: 0.06879586\n",
      "Train Epoch: 259 [320/891 (35.71428571%)]\tLoss: 0.01890957\n",
      "Train Epoch: 259 [640/891 (71.42857143%)]\tLoss: 0.29921091\n",
      "Train Set: Average loss: 0.09648158, Accuracy: 863/891 (96.85746352%)\n",
      "\n",
      "Test set: Average loss: 0.29829368, Accuracy: 95/99 (95.95959596%)\n",
      "\n",
      "Epoch 259 time: 1.0066182613372803 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 260 [0/891 (0.00000000%)]\tLoss: 0.09286433\n",
      "Train Epoch: 260 [320/891 (35.71428571%)]\tLoss: 0.11216086\n",
      "Train Epoch: 260 [640/891 (71.42857143%)]\tLoss: 0.17874470\n",
      "Train Set: Average loss: 0.11356320, Accuracy: 856/891 (96.07182941%)\n",
      "\n",
      "Test set: Average loss: 0.29186900, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 260 time: 0.9888274669647217 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 261 [0/891 (0.00000000%)]\tLoss: 0.14577192\n",
      "Train Epoch: 261 [320/891 (35.71428571%)]\tLoss: 0.04685122\n",
      "Train Epoch: 261 [640/891 (71.42857143%)]\tLoss: 0.01170433\n",
      "Train Set: Average loss: 0.09702210, Accuracy: 859/891 (96.40852974%)\n",
      "\n",
      "Test set: Average loss: 0.29038425, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 261 time: 0.9880280494689941 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 262 [0/891 (0.00000000%)]\tLoss: 0.04886776\n",
      "Train Epoch: 262 [320/891 (35.71428571%)]\tLoss: 0.19861090\n",
      "Train Epoch: 262 [640/891 (71.42857143%)]\tLoss: 0.03523070\n",
      "Train Set: Average loss: 0.09143874, Accuracy: 866/891 (97.19416386%)\n",
      "\n",
      "Test set: Average loss: 0.28550364, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 262 time: 1.0348715782165527 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 263 [0/891 (0.00000000%)]\tLoss: 0.04037938\n",
      "Train Epoch: 263 [320/891 (35.71428571%)]\tLoss: 0.09281850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 263 [640/891 (71.42857143%)]\tLoss: 0.00396711\n",
      "Train Set: Average loss: 0.09209584, Accuracy: 865/891 (97.08193042%)\n",
      "\n",
      "Test set: Average loss: 0.28501830, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 263 time: 0.9986639022827148 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 264 [0/891 (0.00000000%)]\tLoss: 0.53958398\n",
      "Train Epoch: 264 [320/891 (35.71428571%)]\tLoss: 0.03278393\n",
      "Train Epoch: 264 [640/891 (71.42857143%)]\tLoss: 0.12490588\n",
      "Train Set: Average loss: 0.13152060, Accuracy: 851/891 (95.51066218%)\n",
      "\n",
      "Test set: Average loss: 0.30324314, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 264 time: 1.054893970489502 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 265 [0/891 (0.00000000%)]\tLoss: 0.06538659\n",
      "Train Epoch: 265 [320/891 (35.71428571%)]\tLoss: 0.23729694\n",
      "Train Epoch: 265 [640/891 (71.42857143%)]\tLoss: 0.01237386\n",
      "Train Set: Average loss: 0.10081642, Accuracy: 857/891 (96.18406285%)\n",
      "\n",
      "Test set: Average loss: 0.30559108, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 265 time: 1.0268614292144775 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 266 [0/891 (0.00000000%)]\tLoss: 0.07732588\n",
      "Train Epoch: 266 [320/891 (35.71428571%)]\tLoss: 0.01806873\n",
      "Train Epoch: 266 [640/891 (71.42857143%)]\tLoss: 0.16402900\n",
      "Train Set: Average loss: 0.11084868, Accuracy: 862/891 (96.74523008%)\n",
      "\n",
      "Test set: Average loss: 0.23148531, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 266 time: 1.0158495903015137 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 267 [0/891 (0.00000000%)]\tLoss: 0.23515850\n",
      "Train Epoch: 267 [320/891 (35.71428571%)]\tLoss: 0.13131940\n",
      "Train Epoch: 267 [640/891 (71.42857143%)]\tLoss: 0.11190540\n",
      "Train Set: Average loss: 0.12906636, Accuracy: 852/891 (95.62289562%)\n",
      "\n",
      "Test set: Average loss: 0.20831563, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 267 time: 1.013326644897461 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 268 [0/891 (0.00000000%)]\tLoss: 0.10449284\n",
      "Train Epoch: 268 [320/891 (35.71428571%)]\tLoss: 0.17290390\n",
      "Train Epoch: 268 [640/891 (71.42857143%)]\tLoss: 0.18409711\n",
      "Train Set: Average loss: 0.14753059, Accuracy: 847/891 (95.06172840%)\n",
      "\n",
      "Test set: Average loss: 0.26741509, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 268 time: 1.0622282028198242 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 269 [0/891 (0.00000000%)]\tLoss: 0.01884872\n",
      "Train Epoch: 269 [320/891 (35.71428571%)]\tLoss: 0.05656075\n",
      "Train Epoch: 269 [640/891 (71.42857143%)]\tLoss: 0.11304736\n",
      "Train Set: Average loss: 0.12919735, Accuracy: 847/891 (95.06172840%)\n",
      "\n",
      "Test set: Average loss: 0.27760425, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 269 time: 0.981682300567627 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 270 [0/891 (0.00000000%)]\tLoss: 0.10851872\n",
      "Train Epoch: 270 [320/891 (35.71428571%)]\tLoss: 0.09940898\n",
      "Train Epoch: 270 [640/891 (71.42857143%)]\tLoss: 0.18285632\n",
      "Train Set: Average loss: 0.12290494, Accuracy: 852/891 (95.62289562%)\n",
      "\n",
      "Test set: Average loss: 0.29912976, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 270 time: 1.078129768371582 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 271 [0/891 (0.00000000%)]\tLoss: 0.11659110\n",
      "Train Epoch: 271 [320/891 (35.71428571%)]\tLoss: 0.00895441\n",
      "Train Epoch: 271 [640/891 (71.42857143%)]\tLoss: 0.02697319\n",
      "Train Set: Average loss: 0.09217213, Accuracy: 861/891 (96.63299663%)\n",
      "\n",
      "Test set: Average loss: 0.33393663, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 271 time: 0.9954535961151123 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 272 [0/891 (0.00000000%)]\tLoss: 0.25907373\n",
      "Train Epoch: 272 [320/891 (35.71428571%)]\tLoss: 0.12626958\n",
      "Train Epoch: 272 [640/891 (71.42857143%)]\tLoss: 0.08668274\n",
      "Train Set: Average loss: 0.10827061, Accuracy: 850/891 (95.39842873%)\n",
      "\n",
      "Test set: Average loss: 0.30950739, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 272 time: 1.06522536277771 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 273 [0/891 (0.00000000%)]\tLoss: 0.11408645\n",
      "Train Epoch: 273 [320/891 (35.71428571%)]\tLoss: 0.04032844\n",
      "Train Epoch: 273 [640/891 (71.42857143%)]\tLoss: 0.00691175\n",
      "Train Set: Average loss: 0.16105757, Accuracy: 845/891 (94.83726150%)\n",
      "\n",
      "Test set: Average loss: 0.26556111, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 273 time: 1.0347106456756592 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 274 [0/891 (0.00000000%)]\tLoss: 0.07026702\n",
      "Train Epoch: 274 [320/891 (35.71428571%)]\tLoss: 0.05694664\n",
      "Train Epoch: 274 [640/891 (71.42857143%)]\tLoss: 0.39680564\n",
      "Train Set: Average loss: 0.09954525, Accuracy: 856/891 (96.07182941%)\n",
      "\n",
      "Test set: Average loss: 0.31547824, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 274 time: 1.06489896774292 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 275 [0/891 (0.00000000%)]\tLoss: 0.08813184\n",
      "Train Epoch: 275 [320/891 (35.71428571%)]\tLoss: 0.04411840\n",
      "Train Epoch: 275 [640/891 (71.42857143%)]\tLoss: 0.11446387\n",
      "Train Set: Average loss: 0.10814894, Accuracy: 857/891 (96.18406285%)\n",
      "\n",
      "Test set: Average loss: 0.24698071, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 275 time: 1.0394084453582764 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 276 [0/891 (0.00000000%)]\tLoss: 0.05319923\n",
      "Train Epoch: 276 [320/891 (35.71428571%)]\tLoss: 0.01304525\n",
      "Train Epoch: 276 [640/891 (71.42857143%)]\tLoss: 0.09823853\n",
      "Train Set: Average loss: 0.10381604, Accuracy: 857/891 (96.18406285%)\n",
      "\n",
      "Test set: Average loss: 0.26346318, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 276 time: 1.0124258995056152 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 277 [0/891 (0.00000000%)]\tLoss: 0.08390057\n",
      "Train Epoch: 277 [320/891 (35.71428571%)]\tLoss: 0.03638709\n",
      "Train Epoch: 277 [640/891 (71.42857143%)]\tLoss: 0.02467477\n",
      "Train Set: Average loss: 0.07683622, Accuracy: 870/891 (97.64309764%)\n",
      "\n",
      "Test set: Average loss: 0.27110699, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 277 time: 0.9920103549957275 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 278 [0/891 (0.00000000%)]\tLoss: 0.11412555\n",
      "Train Epoch: 278 [320/891 (35.71428571%)]\tLoss: 0.28505397\n",
      "Train Epoch: 278 [640/891 (71.42857143%)]\tLoss: 0.02547485\n",
      "Train Set: Average loss: 0.11109530, Accuracy: 856/891 (96.07182941%)\n",
      "\n",
      "Test set: Average loss: 0.23384998, Accuracy: 95/99 (95.95959596%)\n",
      "\n",
      "Epoch 278 time: 0.9800479412078857 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 279 [0/891 (0.00000000%)]\tLoss: 0.18021625\n",
      "Train Epoch: 279 [320/891 (35.71428571%)]\tLoss: 0.16464216\n",
      "Train Epoch: 279 [640/891 (71.42857143%)]\tLoss: 0.12210613\n",
      "Train Set: Average loss: 0.11100051, Accuracy: 858/891 (96.29629630%)\n",
      "\n",
      "Test set: Average loss: 0.25491036, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 279 time: 1.0889875888824463 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 280 [0/891 (0.00000000%)]\tLoss: 0.01779658\n",
      "Train Epoch: 280 [320/891 (35.71428571%)]\tLoss: 0.10953456\n",
      "Train Epoch: 280 [640/891 (71.42857143%)]\tLoss: 0.12089226\n",
      "Train Set: Average loss: 0.11415674, Accuracy: 855/891 (95.95959596%)\n",
      "\n",
      "Test set: Average loss: 0.28886550, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 280 time: 1.0483579635620117 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 281 [0/891 (0.00000000%)]\tLoss: 0.04629457\n",
      "Train Epoch: 281 [320/891 (35.71428571%)]\tLoss: 0.33076888\n",
      "Train Epoch: 281 [640/891 (71.42857143%)]\tLoss: 0.14844853\n",
      "Train Set: Average loss: 0.08626562, Accuracy: 860/891 (96.52076319%)\n",
      "\n",
      "Test set: Average loss: 0.30451545, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 281 time: 1.039515733718872 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 282 [0/891 (0.00000000%)]\tLoss: 0.12764066\n",
      "Train Epoch: 282 [320/891 (35.71428571%)]\tLoss: 0.01636350\n",
      "Train Epoch: 282 [640/891 (71.42857143%)]\tLoss: 0.11879998\n",
      "Train Set: Average loss: 0.10457592, Accuracy: 861/891 (96.63299663%)\n",
      "\n",
      "Test set: Average loss: 0.37170058, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 282 time: 1.02054762840271 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 283 [0/891 (0.00000000%)]\tLoss: 0.01656610\n",
      "Train Epoch: 283 [320/891 (35.71428571%)]\tLoss: 0.13957220\n",
      "Train Epoch: 283 [640/891 (71.42857143%)]\tLoss: 0.03703445\n",
      "Train Set: Average loss: 0.06982082, Accuracy: 873/891 (97.97979798%)\n",
      "\n",
      "Test set: Average loss: 0.31132342, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 283 time: 1.0244250297546387 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 284 [0/891 (0.00000000%)]\tLoss: 0.03872216\n",
      "Train Epoch: 284 [320/891 (35.71428571%)]\tLoss: 0.02379692\n",
      "Train Epoch: 284 [640/891 (71.42857143%)]\tLoss: 0.10185057\n",
      "Train Set: Average loss: 0.11299142, Accuracy: 855/891 (95.95959596%)\n",
      "\n",
      "Test set: Average loss: 0.22608919, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 284 time: 1.0163445472717285 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 285 [0/891 (0.00000000%)]\tLoss: 0.17131442\n",
      "Train Epoch: 285 [320/891 (35.71428571%)]\tLoss: 0.03710020\n",
      "Train Epoch: 285 [640/891 (71.42857143%)]\tLoss: 0.04919356\n",
      "Train Set: Average loss: 0.10429197, Accuracy: 856/891 (96.07182941%)\n",
      "\n",
      "Test set: Average loss: 0.23536374, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 285 time: 0.9862387180328369 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 286 [0/891 (0.00000000%)]\tLoss: 0.20711273\n",
      "Train Epoch: 286 [320/891 (35.71428571%)]\tLoss: 0.01340544\n",
      "Train Epoch: 286 [640/891 (71.42857143%)]\tLoss: 0.02008194\n",
      "Train Set: Average loss: 0.10810256, Accuracy: 859/891 (96.40852974%)\n",
      "\n",
      "Test set: Average loss: 0.26237497, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 286 time: 1.0044384002685547 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 287 [0/891 (0.00000000%)]\tLoss: 0.02044520\n",
      "Train Epoch: 287 [320/891 (35.71428571%)]\tLoss: 0.05429852\n",
      "Train Epoch: 287 [640/891 (71.42857143%)]\tLoss: 0.04447472\n",
      "Train Set: Average loss: 0.11479295, Accuracy: 861/891 (96.63299663%)\n",
      "\n",
      "Test set: Average loss: 0.28055765, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 287 time: 0.9930069446563721 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 288 [0/891 (0.00000000%)]\tLoss: 0.07352114\n",
      "Train Epoch: 288 [320/891 (35.71428571%)]\tLoss: 0.02559686\n",
      "Train Epoch: 288 [640/891 (71.42857143%)]\tLoss: 0.31122977\n",
      "Train Set: Average loss: 0.11790532, Accuracy: 858/891 (96.29629630%)\n",
      "\n",
      "Test set: Average loss: 0.25881673, Accuracy: 95/99 (95.95959596%)\n",
      "\n",
      "Epoch 288 time: 1.0232887268066406 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 289 [0/891 (0.00000000%)]\tLoss: 0.20979211\n",
      "Train Epoch: 289 [320/891 (35.71428571%)]\tLoss: 0.06370735\n",
      "Train Epoch: 289 [640/891 (71.42857143%)]\tLoss: 0.12643933\n",
      "Train Set: Average loss: 0.10350545, Accuracy: 854/891 (95.84736251%)\n",
      "\n",
      "Test set: Average loss: 0.27191748, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 289 time: 1.0074009895324707 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 290 [0/891 (0.00000000%)]\tLoss: 0.02459678\n",
      "Train Epoch: 290 [320/891 (35.71428571%)]\tLoss: 0.34964311\n",
      "Train Epoch: 290 [640/891 (71.42857143%)]\tLoss: 0.02096146\n",
      "Train Set: Average loss: 0.11007405, Accuracy: 860/891 (96.52076319%)\n",
      "\n",
      "Test set: Average loss: 0.31652967, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 290 time: 1.0072367191314697 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 291 [0/891 (0.00000000%)]\tLoss: 0.00870484\n",
      "Train Epoch: 291 [320/891 (35.71428571%)]\tLoss: 0.05190307\n",
      "Train Epoch: 291 [640/891 (71.42857143%)]\tLoss: 0.10042095\n",
      "Train Set: Average loss: 0.08210895, Accuracy: 865/891 (97.08193042%)\n",
      "\n",
      "Test set: Average loss: 0.28896996, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 291 time: 0.9743475914001465 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 292 [0/891 (0.00000000%)]\tLoss: 0.03524965\n",
      "Train Epoch: 292 [320/891 (35.71428571%)]\tLoss: 0.14895207\n",
      "Train Epoch: 292 [640/891 (71.42857143%)]\tLoss: 0.19762635\n",
      "Train Set: Average loss: 0.10612446, Accuracy: 857/891 (96.18406285%)\n",
      "\n",
      "Test set: Average loss: 0.28332500, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 292 time: 0.9980452060699463 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 293 [0/891 (0.00000000%)]\tLoss: 0.01590377\n",
      "Train Epoch: 293 [320/891 (35.71428571%)]\tLoss: 0.05842894\n",
      "Train Epoch: 293 [640/891 (71.42857143%)]\tLoss: 0.02331698\n",
      "Train Set: Average loss: 0.12492806, Accuracy: 853/891 (95.73512907%)\n",
      "\n",
      "Test set: Average loss: 0.26078654, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 293 time: 1.0138075351715088 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 294 [0/891 (0.00000000%)]\tLoss: 0.01581591\n",
      "Train Epoch: 294 [320/891 (35.71428571%)]\tLoss: 0.05311489\n",
      "Train Epoch: 294 [640/891 (71.42857143%)]\tLoss: 0.14169106\n",
      "Train Set: Average loss: 0.12561487, Accuracy: 853/891 (95.73512907%)\n",
      "\n",
      "Test set: Average loss: 0.23338794, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 294 time: 1.0530078411102295 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 295 [0/891 (0.00000000%)]\tLoss: 0.16238922\n",
      "Train Epoch: 295 [320/891 (35.71428571%)]\tLoss: 0.09654027\n",
      "Train Epoch: 295 [640/891 (71.42857143%)]\tLoss: 0.18588746\n",
      "Train Set: Average loss: 0.09796189, Accuracy: 861/891 (96.63299663%)\n",
      "\n",
      "Test set: Average loss: 0.26567378, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 295 time: 1.0122992992401123 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 296 [0/891 (0.00000000%)]\tLoss: 0.11435872\n",
      "Train Epoch: 296 [320/891 (35.71428571%)]\tLoss: 0.18894589\n",
      "Train Epoch: 296 [640/891 (71.42857143%)]\tLoss: 0.12460047\n",
      "Train Set: Average loss: 0.09974841, Accuracy: 862/891 (96.74523008%)\n",
      "\n",
      "Test set: Average loss: 0.28739157, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 296 time: 1.0408108234405518 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 297 [0/891 (0.00000000%)]\tLoss: 0.01074946\n",
      "Train Epoch: 297 [320/891 (35.71428571%)]\tLoss: 0.41734910\n",
      "Train Epoch: 297 [640/891 (71.42857143%)]\tLoss: 0.05335236\n",
      "Train Set: Average loss: 0.08603386, Accuracy: 860/891 (96.52076319%)\n",
      "\n",
      "Test set: Average loss: 0.28694634, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 297 time: 1.0243403911590576 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 298 [0/891 (0.00000000%)]\tLoss: 0.04357290\n",
      "Train Epoch: 298 [320/891 (35.71428571%)]\tLoss: 0.00742757\n",
      "Train Epoch: 298 [640/891 (71.42857143%)]\tLoss: 0.26017123\n",
      "Train Set: Average loss: 0.10110256, Accuracy: 864/891 (96.96969697%)\n",
      "\n",
      "Test set: Average loss: 0.21277636, Accuracy: 95/99 (95.95959596%)\n",
      "\n",
      "Epoch 298 time: 0.9978830814361572 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 299 [0/891 (0.00000000%)]\tLoss: 0.12390745\n",
      "Train Epoch: 299 [320/891 (35.71428571%)]\tLoss: 0.10837704\n",
      "Train Epoch: 299 [640/891 (71.42857143%)]\tLoss: 0.25491077\n",
      "Train Set: Average loss: 0.13250198, Accuracy: 853/891 (95.73512907%)\n",
      "\n",
      "Test set: Average loss: 0.30380031, Accuracy: 95/99 (95.95959596%)\n",
      "\n",
      "Epoch 299 time: 1.0224850177764893 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 300 [0/891 (0.00000000%)]\tLoss: 0.07552123\n",
      "Train Epoch: 300 [320/891 (35.71428571%)]\tLoss: 0.01859993\n",
      "Train Epoch: 300 [640/891 (71.42857143%)]\tLoss: 0.02588844\n",
      "Train Set: Average loss: 0.10985632, Accuracy: 857/891 (96.18406285%)\n",
      "\n",
      "Test set: Average loss: 0.34545471, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 300 time: 1.1373498439788818 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 301 [0/891 (0.00000000%)]\tLoss: 0.22263211\n",
      "Train Epoch: 301 [320/891 (35.71428571%)]\tLoss: 0.20274043\n",
      "Train Epoch: 301 [640/891 (71.42857143%)]\tLoss: 0.40473711\n",
      "Train Set: Average loss: 0.12338251, Accuracy: 850/891 (95.39842873%)\n",
      "\n",
      "Test set: Average loss: 0.36721721, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 301 time: 1.0155034065246582 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 302 [0/891 (0.00000000%)]\tLoss: 0.12234205\n",
      "Train Epoch: 302 [320/891 (35.71428571%)]\tLoss: 0.02214766\n",
      "Train Epoch: 302 [640/891 (71.42857143%)]\tLoss: 0.00284159\n",
      "Train Set: Average loss: 0.13255470, Accuracy: 855/891 (95.95959596%)\n",
      "\n",
      "Test set: Average loss: 0.36116536, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 302 time: 1.004807949066162 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 303 [0/891 (0.00000000%)]\tLoss: 0.04394627\n",
      "Train Epoch: 303 [320/891 (35.71428571%)]\tLoss: 0.00638133\n",
      "Train Epoch: 303 [640/891 (71.42857143%)]\tLoss: 0.02295631\n",
      "Train Set: Average loss: 0.08293155, Accuracy: 865/891 (97.08193042%)\n",
      "\n",
      "Test set: Average loss: 0.30532922, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 303 time: 1.022369146347046 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 304 [0/891 (0.00000000%)]\tLoss: 0.01964539\n",
      "Train Epoch: 304 [320/891 (35.71428571%)]\tLoss: 0.00561106\n",
      "Train Epoch: 304 [640/891 (71.42857143%)]\tLoss: 0.10424978\n",
      "Train Set: Average loss: 0.09543958, Accuracy: 861/891 (96.63299663%)\n",
      "\n",
      "Test set: Average loss: 0.25298876, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 304 time: 1.0326673984527588 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 305 [0/891 (0.00000000%)]\tLoss: 0.04373714\n",
      "Train Epoch: 305 [320/891 (35.71428571%)]\tLoss: 0.02148956\n",
      "Train Epoch: 305 [640/891 (71.42857143%)]\tLoss: 0.01723164\n",
      "Train Set: Average loss: 0.05439575, Accuracy: 874/891 (98.09203143%)\n",
      "\n",
      "Test set: Average loss: 0.27602778, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 305 time: 1.0361549854278564 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 306 [0/891 (0.00000000%)]\tLoss: 0.23415095\n",
      "Train Epoch: 306 [320/891 (35.71428571%)]\tLoss: 0.26396656\n",
      "Train Epoch: 306 [640/891 (71.42857143%)]\tLoss: 0.04793674\n",
      "Train Set: Average loss: 0.10925676, Accuracy: 859/891 (96.40852974%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.29468662, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 306 time: 1.0169270038604736 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 307 [0/891 (0.00000000%)]\tLoss: 0.25687432\n",
      "Train Epoch: 307 [320/891 (35.71428571%)]\tLoss: 0.12668604\n",
      "Train Epoch: 307 [640/891 (71.42857143%)]\tLoss: 0.27217847\n",
      "Train Set: Average loss: 0.11490615, Accuracy: 848/891 (95.17396184%)\n",
      "\n",
      "Test set: Average loss: 0.27755485, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 307 time: 1.0307197570800781 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 308 [0/891 (0.00000000%)]\tLoss: 0.04206210\n",
      "Train Epoch: 308 [320/891 (35.71428571%)]\tLoss: 0.08686560\n",
      "Train Epoch: 308 [640/891 (71.42857143%)]\tLoss: 0.11594230\n",
      "Train Set: Average loss: 0.07421701, Accuracy: 865/891 (97.08193042%)\n",
      "\n",
      "Test set: Average loss: 0.28594100, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 308 time: 0.9840555191040039 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 309 [0/891 (0.00000000%)]\tLoss: 0.00940126\n",
      "Train Epoch: 309 [320/891 (35.71428571%)]\tLoss: 0.04155958\n",
      "Train Epoch: 309 [640/891 (71.42857143%)]\tLoss: 0.20969743\n",
      "Train Set: Average loss: 0.11172039, Accuracy: 857/891 (96.18406285%)\n",
      "\n",
      "Test set: Average loss: 0.28428372, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 309 time: 1.0344843864440918 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 310 [0/891 (0.00000000%)]\tLoss: 0.04628724\n",
      "Train Epoch: 310 [320/891 (35.71428571%)]\tLoss: 0.16039121\n",
      "Train Epoch: 310 [640/891 (71.42857143%)]\tLoss: 0.01509267\n",
      "Train Set: Average loss: 0.13601829, Accuracy: 847/891 (95.06172840%)\n",
      "\n",
      "Test set: Average loss: 0.40435647, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 310 time: 1.1006152629852295 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 311 [0/891 (0.00000000%)]\tLoss: 0.06575555\n",
      "Train Epoch: 311 [320/891 (35.71428571%)]\tLoss: 0.01401508\n",
      "Train Epoch: 311 [640/891 (71.42857143%)]\tLoss: 0.16524535\n",
      "Train Set: Average loss: 0.10141328, Accuracy: 862/891 (96.74523008%)\n",
      "\n",
      "Test set: Average loss: 0.44885882, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 311 time: 1.0679278373718262 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 312 [0/891 (0.00000000%)]\tLoss: 0.04955745\n",
      "Train Epoch: 312 [320/891 (35.71428571%)]\tLoss: 0.25852460\n",
      "Train Epoch: 312 [640/891 (71.42857143%)]\tLoss: 0.19863051\n",
      "Train Set: Average loss: 0.10026892, Accuracy: 864/891 (96.96969697%)\n",
      "\n",
      "Test set: Average loss: 0.44225691, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 312 time: 0.9689562320709229 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 313 [0/891 (0.00000000%)]\tLoss: 0.11368769\n",
      "Train Epoch: 313 [320/891 (35.71428571%)]\tLoss: 0.05212349\n",
      "Train Epoch: 313 [640/891 (71.42857143%)]\tLoss: 0.11252752\n",
      "Train Set: Average loss: 0.10107864, Accuracy: 856/891 (96.07182941%)\n",
      "\n",
      "Test set: Average loss: 0.43539920, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 313 time: 1.0464568138122559 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 314 [0/891 (0.00000000%)]\tLoss: 0.07160461\n",
      "Train Epoch: 314 [320/891 (35.71428571%)]\tLoss: 0.20162445\n",
      "Train Epoch: 314 [640/891 (71.42857143%)]\tLoss: 0.49507958\n",
      "Train Set: Average loss: 0.13104970, Accuracy: 851/891 (95.51066218%)\n",
      "\n",
      "Test set: Average loss: 0.41912250, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 314 time: 1.029860496520996 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 315 [0/891 (0.00000000%)]\tLoss: 0.21107492\n",
      "Train Epoch: 315 [320/891 (35.71428571%)]\tLoss: 0.05371314\n",
      "Train Epoch: 315 [640/891 (71.42857143%)]\tLoss: 0.14273375\n",
      "Train Set: Average loss: 0.07065735, Accuracy: 869/891 (97.53086420%)\n",
      "\n",
      "Test set: Average loss: 0.40763075, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 315 time: 1.0037546157836914 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 316 [0/891 (0.00000000%)]\tLoss: 0.09856379\n",
      "Train Epoch: 316 [320/891 (35.71428571%)]\tLoss: 0.14964420\n",
      "Train Epoch: 316 [640/891 (71.42857143%)]\tLoss: 0.19976300\n",
      "Train Set: Average loss: 0.11356578, Accuracy: 855/891 (95.95959596%)\n",
      "\n",
      "Test set: Average loss: 0.44532703, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 316 time: 1.0144875049591064 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 317 [0/891 (0.00000000%)]\tLoss: 0.05795318\n",
      "Train Epoch: 317 [320/891 (35.71428571%)]\tLoss: 0.22368574\n",
      "Train Epoch: 317 [640/891 (71.42857143%)]\tLoss: 0.01246852\n",
      "Train Set: Average loss: 0.05015068, Accuracy: 879/891 (98.65319865%)\n",
      "\n",
      "Test set: Average loss: 0.39161199, Accuracy: 89/99 (89.89898990%)\n",
      "\n",
      "Epoch 317 time: 1.0946953296661377 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 318 [0/891 (0.00000000%)]\tLoss: 0.12220752\n",
      "Train Epoch: 318 [320/891 (35.71428571%)]\tLoss: 0.12648058\n",
      "Train Epoch: 318 [640/891 (71.42857143%)]\tLoss: 0.21787608\n",
      "Train Set: Average loss: 0.09576932, Accuracy: 862/891 (96.74523008%)\n",
      "\n",
      "Test set: Average loss: 0.43943765, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 318 time: 1.0408127307891846 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 319 [0/891 (0.00000000%)]\tLoss: 0.03567648\n",
      "Train Epoch: 319 [320/891 (35.71428571%)]\tLoss: 0.01308900\n",
      "Train Epoch: 319 [640/891 (71.42857143%)]\tLoss: 0.13191897\n",
      "Train Set: Average loss: 0.12699425, Accuracy: 857/891 (96.18406285%)\n",
      "\n",
      "Test set: Average loss: 0.30330754, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 319 time: 1.0270180702209473 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 320 [0/891 (0.00000000%)]\tLoss: 0.19683510\n",
      "Train Epoch: 320 [320/891 (35.71428571%)]\tLoss: 0.16427380\n",
      "Train Epoch: 320 [640/891 (71.42857143%)]\tLoss: 0.30262363\n",
      "Train Set: Average loss: 0.14745518, Accuracy: 845/891 (94.83726150%)\n",
      "\n",
      "Test set: Average loss: 0.39072093, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 320 time: 1.035684585571289 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 321 [0/891 (0.00000000%)]\tLoss: 0.02109349\n",
      "Train Epoch: 321 [320/891 (35.71428571%)]\tLoss: 0.06168967\n",
      "Train Epoch: 321 [640/891 (71.42857143%)]\tLoss: 0.09859926\n",
      "Train Set: Average loss: 0.09939747, Accuracy: 865/891 (97.08193042%)\n",
      "\n",
      "Test set: Average loss: 0.43258711, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 321 time: 1.0550405979156494 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 322 [0/891 (0.00000000%)]\tLoss: 0.13229775\n",
      "Train Epoch: 322 [320/891 (35.71428571%)]\tLoss: 0.21778840\n",
      "Train Epoch: 322 [640/891 (71.42857143%)]\tLoss: 0.08000916\n",
      "Train Set: Average loss: 0.13238487, Accuracy: 853/891 (95.73512907%)\n",
      "\n",
      "Test set: Average loss: 0.40993446, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 322 time: 1.0830726623535156 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 323 [0/891 (0.00000000%)]\tLoss: 0.18837345\n",
      "Train Epoch: 323 [320/891 (35.71428571%)]\tLoss: 0.02273160\n",
      "Train Epoch: 323 [640/891 (71.42857143%)]\tLoss: 0.07199967\n",
      "Train Set: Average loss: 0.11729816, Accuracy: 865/891 (97.08193042%)\n",
      "\n",
      "Test set: Average loss: 0.39869692, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 323 time: 1.0852952003479004 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 324 [0/891 (0.00000000%)]\tLoss: 0.07307142\n",
      "Train Epoch: 324 [320/891 (35.71428571%)]\tLoss: 0.00618064\n",
      "Train Epoch: 324 [640/891 (71.42857143%)]\tLoss: 0.01413095\n",
      "Train Set: Average loss: 0.10464570, Accuracy: 858/891 (96.29629630%)\n",
      "\n",
      "Test set: Average loss: 0.43730900, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 324 time: 1.068849802017212 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 325 [0/891 (0.00000000%)]\tLoss: 0.02774233\n",
      "Train Epoch: 325 [320/891 (35.71428571%)]\tLoss: 0.13477916\n",
      "Train Epoch: 325 [640/891 (71.42857143%)]\tLoss: 0.23590112\n",
      "Train Set: Average loss: 0.08868643, Accuracy: 869/891 (97.53086420%)\n",
      "\n",
      "Test set: Average loss: 0.32031942, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 325 time: 1.0497918128967285 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 326 [0/891 (0.00000000%)]\tLoss: 0.13715887\n",
      "Train Epoch: 326 [320/891 (35.71428571%)]\tLoss: 0.04240739\n",
      "Train Epoch: 326 [640/891 (71.42857143%)]\tLoss: 0.02333510\n",
      "Train Set: Average loss: 0.12853800, Accuracy: 859/891 (96.40852974%)\n",
      "\n",
      "Test set: Average loss: 0.30589075, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 326 time: 1.0369455814361572 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 327 [0/891 (0.00000000%)]\tLoss: 0.25151309\n",
      "Train Epoch: 327 [320/891 (35.71428571%)]\tLoss: 0.02832484\n",
      "Train Epoch: 327 [640/891 (71.42857143%)]\tLoss: 0.05011952\n",
      "Train Set: Average loss: 0.10548304, Accuracy: 854/891 (95.84736251%)\n",
      "\n",
      "Test set: Average loss: 0.29321172, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 327 time: 1.034484624862671 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 328 [0/891 (0.00000000%)]\tLoss: 0.05152053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 328 [320/891 (35.71428571%)]\tLoss: 0.03447306\n",
      "Train Epoch: 328 [640/891 (71.42857143%)]\tLoss: 0.04998732\n",
      "Train Set: Average loss: 0.11137620, Accuracy: 855/891 (95.95959596%)\n",
      "\n",
      "Test set: Average loss: 0.28482587, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 328 time: 1.061347246170044 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 329 [0/891 (0.00000000%)]\tLoss: 0.11488402\n",
      "Train Epoch: 329 [320/891 (35.71428571%)]\tLoss: 0.11992067\n",
      "Train Epoch: 329 [640/891 (71.42857143%)]\tLoss: 0.09703445\n",
      "Train Set: Average loss: 0.12119232, Accuracy: 856/891 (96.07182941%)\n",
      "\n",
      "Test set: Average loss: 0.27648928, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 329 time: 1.10825514793396 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 330 [0/891 (0.00000000%)]\tLoss: 0.16073829\n",
      "Train Epoch: 330 [320/891 (35.71428571%)]\tLoss: 0.13947898\n",
      "Train Epoch: 330 [640/891 (71.42857143%)]\tLoss: 0.06170523\n",
      "Train Set: Average loss: 0.12751732, Accuracy: 852/891 (95.62289562%)\n",
      "\n",
      "Test set: Average loss: 0.29549834, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 330 time: 1.131498098373413 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 331 [0/891 (0.00000000%)]\tLoss: 0.04317391\n",
      "Train Epoch: 331 [320/891 (35.71428571%)]\tLoss: 0.01447546\n",
      "Train Epoch: 331 [640/891 (71.42857143%)]\tLoss: 0.06398368\n",
      "Train Set: Average loss: 0.06612368, Accuracy: 876/891 (98.31649832%)\n",
      "\n",
      "Test set: Average loss: 0.27724586, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 331 time: 1.06333589553833 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 332 [0/891 (0.00000000%)]\tLoss: 0.10179257\n",
      "Train Epoch: 332 [320/891 (35.71428571%)]\tLoss: 0.00179797\n",
      "Train Epoch: 332 [640/891 (71.42857143%)]\tLoss: 0.07213688\n",
      "Train Set: Average loss: 0.12190555, Accuracy: 860/891 (96.52076319%)\n",
      "\n",
      "Test set: Average loss: 0.21684785, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 332 time: 1.0945839881896973 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 333 [0/891 (0.00000000%)]\tLoss: 0.00629562\n",
      "Train Epoch: 333 [320/891 (35.71428571%)]\tLoss: 0.03268087\n",
      "Train Epoch: 333 [640/891 (71.42857143%)]\tLoss: 0.08705097\n",
      "Train Set: Average loss: 0.11341681, Accuracy: 861/891 (96.63299663%)\n",
      "\n",
      "Test set: Average loss: 0.27085532, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 333 time: 1.1017556190490723 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 334 [0/891 (0.00000000%)]\tLoss: 0.05672044\n",
      "Train Epoch: 334 [320/891 (35.71428571%)]\tLoss: 0.18083638\n",
      "Train Epoch: 334 [640/891 (71.42857143%)]\tLoss: 0.12216568\n",
      "Train Set: Average loss: 0.08126418, Accuracy: 870/891 (97.64309764%)\n",
      "\n",
      "Test set: Average loss: 0.27709025, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 334 time: 1.0270135402679443 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 335 [0/891 (0.00000000%)]\tLoss: 0.06292218\n",
      "Train Epoch: 335 [320/891 (35.71428571%)]\tLoss: 0.02673733\n",
      "Train Epoch: 335 [640/891 (71.42857143%)]\tLoss: 0.02179193\n",
      "Train Set: Average loss: 0.09862815, Accuracy: 865/891 (97.08193042%)\n",
      "\n",
      "Test set: Average loss: 0.34351195, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 335 time: 1.0912525653839111 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 336 [0/891 (0.00000000%)]\tLoss: 0.12788588\n",
      "Train Epoch: 336 [320/891 (35.71428571%)]\tLoss: 0.02336824\n",
      "Train Epoch: 336 [640/891 (71.42857143%)]\tLoss: 0.13732976\n",
      "Train Set: Average loss: 0.09946559, Accuracy: 860/891 (96.52076319%)\n",
      "\n",
      "Test set: Average loss: 0.37701564, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 336 time: 1.0917160511016846 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 337 [0/891 (0.00000000%)]\tLoss: 0.01272780\n",
      "Train Epoch: 337 [320/891 (35.71428571%)]\tLoss: 0.10886487\n",
      "Train Epoch: 337 [640/891 (71.42857143%)]\tLoss: 0.04241902\n",
      "Train Set: Average loss: 0.09034979, Accuracy: 866/891 (97.19416386%)\n",
      "\n",
      "Test set: Average loss: 0.36065104, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 337 time: 1.0100481510162354 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 338 [0/891 (0.00000000%)]\tLoss: 0.24426264\n",
      "Train Epoch: 338 [320/891 (35.71428571%)]\tLoss: 0.06568193\n",
      "Train Epoch: 338 [640/891 (71.42857143%)]\tLoss: 0.00153881\n",
      "Train Set: Average loss: 0.12778871, Accuracy: 864/891 (96.96969697%)\n",
      "\n",
      "Test set: Average loss: 0.32850510, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 338 time: 1.0365614891052246 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 339 [0/891 (0.00000000%)]\tLoss: 0.15957147\n",
      "Train Epoch: 339 [320/891 (35.71428571%)]\tLoss: 0.17253631\n",
      "Train Epoch: 339 [640/891 (71.42857143%)]\tLoss: 0.01299089\n",
      "Train Set: Average loss: 0.08468566, Accuracy: 861/891 (96.63299663%)\n",
      "\n",
      "Test set: Average loss: 0.34266063, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 339 time: 1.0420618057250977 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 340 [0/891 (0.00000000%)]\tLoss: 0.01258379\n",
      "Train Epoch: 340 [320/891 (35.71428571%)]\tLoss: 0.22205037\n",
      "Train Epoch: 340 [640/891 (71.42857143%)]\tLoss: 0.15041673\n",
      "Train Set: Average loss: 0.11615111, Accuracy: 851/891 (95.51066218%)\n",
      "\n",
      "Test set: Average loss: 0.38713168, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 340 time: 1.0641114711761475 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 341 [0/891 (0.00000000%)]\tLoss: 0.04898024\n",
      "Train Epoch: 341 [320/891 (35.71428571%)]\tLoss: 0.09583491\n",
      "Train Epoch: 341 [640/891 (71.42857143%)]\tLoss: 0.15214276\n",
      "Train Set: Average loss: 0.14434166, Accuracy: 852/891 (95.62289562%)\n",
      "\n",
      "Test set: Average loss: 0.31869863, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 341 time: 1.0321240425109863 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 342 [0/891 (0.00000000%)]\tLoss: 0.08391953\n",
      "Train Epoch: 342 [320/891 (35.71428571%)]\tLoss: 0.05586922\n",
      "Train Epoch: 342 [640/891 (71.42857143%)]\tLoss: 0.26991236\n",
      "Train Set: Average loss: 0.08300704, Accuracy: 869/891 (97.53086420%)\n",
      "\n",
      "Test set: Average loss: 0.24252269, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 342 time: 1.052093267440796 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 343 [0/891 (0.00000000%)]\tLoss: 0.06265008\n",
      "Train Epoch: 343 [320/891 (35.71428571%)]\tLoss: 0.09087139\n",
      "Train Epoch: 343 [640/891 (71.42857143%)]\tLoss: 0.01551729\n",
      "Train Set: Average loss: 0.10035292, Accuracy: 864/891 (96.96969697%)\n",
      "\n",
      "Test set: Average loss: 0.29967522, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 343 time: 1.0622336864471436 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 344 [0/891 (0.00000000%)]\tLoss: 0.04679611\n",
      "Train Epoch: 344 [320/891 (35.71428571%)]\tLoss: 0.10256058\n",
      "Train Epoch: 344 [640/891 (71.42857143%)]\tLoss: 0.04980296\n",
      "Train Set: Average loss: 0.08863567, Accuracy: 861/891 (96.63299663%)\n",
      "\n",
      "Test set: Average loss: 0.28460212, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 344 time: 1.044161319732666 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 345 [0/891 (0.00000000%)]\tLoss: 0.01772869\n",
      "Train Epoch: 345 [320/891 (35.71428571%)]\tLoss: 0.03279567\n",
      "Train Epoch: 345 [640/891 (71.42857143%)]\tLoss: 0.20539773\n",
      "Train Set: Average loss: 0.08200103, Accuracy: 864/891 (96.96969697%)\n",
      "\n",
      "Test set: Average loss: 0.28742686, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 345 time: 1.0590574741363525 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 346 [0/891 (0.00000000%)]\tLoss: 0.02221650\n",
      "Train Epoch: 346 [320/891 (35.71428571%)]\tLoss: 0.05093330\n",
      "Train Epoch: 346 [640/891 (71.42857143%)]\tLoss: 0.25259575\n",
      "Train Set: Average loss: 0.10287983, Accuracy: 859/891 (96.40852974%)\n",
      "\n",
      "Test set: Average loss: 0.27458753, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 346 time: 1.013765811920166 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 347 [0/891 (0.00000000%)]\tLoss: 0.11706185\n",
      "Train Epoch: 347 [320/891 (35.71428571%)]\tLoss: 0.06385940\n",
      "Train Epoch: 347 [640/891 (71.42857143%)]\tLoss: 0.01651281\n",
      "Train Set: Average loss: 0.07667463, Accuracy: 867/891 (97.30639731%)\n",
      "\n",
      "Test set: Average loss: 0.26569232, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 347 time: 1.0925850868225098 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 348 [0/891 (0.00000000%)]\tLoss: 0.01583236\n",
      "Train Epoch: 348 [320/891 (35.71428571%)]\tLoss: 0.43072140\n",
      "Train Epoch: 348 [640/891 (71.42857143%)]\tLoss: 0.04121625\n",
      "Train Set: Average loss: 0.09893661, Accuracy: 860/891 (96.52076319%)\n",
      "\n",
      "Test set: Average loss: 0.35886668, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 348 time: 1.038773775100708 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 349 [0/891 (0.00000000%)]\tLoss: 0.01284015\n",
      "Train Epoch: 349 [320/891 (35.71428571%)]\tLoss: 0.01699495\n",
      "Train Epoch: 349 [640/891 (71.42857143%)]\tLoss: 0.24102747\n",
      "Train Set: Average loss: 0.11215214, Accuracy: 861/891 (96.63299663%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.34100519, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 349 time: 1.0206592082977295 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 350 [0/891 (0.00000000%)]\tLoss: 0.06250691\n",
      "Train Epoch: 350 [320/891 (35.71428571%)]\tLoss: 0.07428110\n",
      "Train Epoch: 350 [640/891 (71.42857143%)]\tLoss: 0.15024012\n",
      "Train Set: Average loss: 0.11940554, Accuracy: 858/891 (96.29629630%)\n",
      "\n",
      "Test set: Average loss: 0.38167501, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 350 time: 1.0373890399932861 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 351 [0/891 (0.00000000%)]\tLoss: 0.09158832\n",
      "Train Epoch: 351 [320/891 (35.71428571%)]\tLoss: 0.05791265\n",
      "Train Epoch: 351 [640/891 (71.42857143%)]\tLoss: 0.04224771\n",
      "Train Set: Average loss: 0.09795066, Accuracy: 860/891 (96.52076319%)\n",
      "\n",
      "Test set: Average loss: 0.39119593, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 351 time: 0.9954614639282227 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 352 [0/891 (0.00000000%)]\tLoss: 0.00570589\n",
      "Train Epoch: 352 [320/891 (35.71428571%)]\tLoss: 0.04374915\n",
      "Train Epoch: 352 [640/891 (71.42857143%)]\tLoss: 0.08542097\n",
      "Train Set: Average loss: 0.07691010, Accuracy: 864/891 (96.96969697%)\n",
      "\n",
      "Test set: Average loss: 0.39142437, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 352 time: 1.0215609073638916 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 353 [0/891 (0.00000000%)]\tLoss: 0.01048398\n",
      "Train Epoch: 353 [320/891 (35.71428571%)]\tLoss: 0.01450777\n",
      "Train Epoch: 353 [640/891 (71.42857143%)]\tLoss: 0.00543231\n",
      "Train Set: Average loss: 0.07878120, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.28891280, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 353 time: 1.024040699005127 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 354 [0/891 (0.00000000%)]\tLoss: 0.06973070\n",
      "Train Epoch: 354 [320/891 (35.71428571%)]\tLoss: 0.07188833\n",
      "Train Epoch: 354 [640/891 (71.42857143%)]\tLoss: 0.01146817\n",
      "Train Set: Average loss: 0.07447775, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.33883251, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 354 time: 0.9884774684906006 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 355 [0/891 (0.00000000%)]\tLoss: 0.10815221\n",
      "Train Epoch: 355 [320/891 (35.71428571%)]\tLoss: 0.00719869\n",
      "Train Epoch: 355 [640/891 (71.42857143%)]\tLoss: 0.13925809\n",
      "Train Set: Average loss: 0.10711804, Accuracy: 859/891 (96.40852974%)\n",
      "\n",
      "Test set: Average loss: 0.42354237, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 355 time: 1.0437638759613037 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 356 [0/891 (0.00000000%)]\tLoss: 0.01249278\n",
      "Train Epoch: 356 [320/891 (35.71428571%)]\tLoss: 0.01567668\n",
      "Train Epoch: 356 [640/891 (71.42857143%)]\tLoss: 0.02050877\n",
      "Train Set: Average loss: 0.09084096, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.43505638, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 356 time: 0.9906973838806152 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 357 [0/891 (0.00000000%)]\tLoss: 0.08312905\n",
      "Train Epoch: 357 [320/891 (35.71428571%)]\tLoss: 0.02788687\n",
      "Train Epoch: 357 [640/891 (71.42857143%)]\tLoss: 0.10423428\n",
      "Train Set: Average loss: 0.08566134, Accuracy: 864/891 (96.96969697%)\n",
      "\n",
      "Test set: Average loss: 0.41654874, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 357 time: 0.9893643856048584 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 358 [0/891 (0.00000000%)]\tLoss: 0.00204396\n",
      "Train Epoch: 358 [320/891 (35.71428571%)]\tLoss: 0.02877867\n",
      "Train Epoch: 358 [640/891 (71.42857143%)]\tLoss: 0.12365043\n",
      "Train Set: Average loss: 0.07250906, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.36532593, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 358 time: 1.0026483535766602 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 359 [0/891 (0.00000000%)]\tLoss: 0.32935083\n",
      "Train Epoch: 359 [320/891 (35.71428571%)]\tLoss: 0.38242811\n",
      "Train Epoch: 359 [640/891 (71.42857143%)]\tLoss: 0.14362335\n",
      "Train Set: Average loss: 0.11294220, Accuracy: 860/891 (96.52076319%)\n",
      "\n",
      "Test set: Average loss: 0.30871432, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 359 time: 1.0002140998840332 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 360 [0/891 (0.00000000%)]\tLoss: 0.05800992\n",
      "Train Epoch: 360 [320/891 (35.71428571%)]\tLoss: 0.03100204\n",
      "Train Epoch: 360 [640/891 (71.42857143%)]\tLoss: 0.15971643\n",
      "Train Set: Average loss: 0.08791378, Accuracy: 863/891 (96.85746352%)\n",
      "\n",
      "Test set: Average loss: 0.33414120, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 360 time: 1.0049378871917725 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 361 [0/891 (0.00000000%)]\tLoss: 0.00697553\n",
      "Train Epoch: 361 [320/891 (35.71428571%)]\tLoss: 0.01923019\n",
      "Train Epoch: 361 [640/891 (71.42857143%)]\tLoss: 0.05259031\n",
      "Train Set: Average loss: 0.09261478, Accuracy: 871/891 (97.75533109%)\n",
      "\n",
      "Test set: Average loss: 0.34355836, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 361 time: 0.9922542572021484 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 362 [0/891 (0.00000000%)]\tLoss: 0.10083646\n",
      "Train Epoch: 362 [320/891 (35.71428571%)]\tLoss: 0.15747422\n",
      "Train Epoch: 362 [640/891 (71.42857143%)]\tLoss: 0.00839001\n",
      "Train Set: Average loss: 0.11618656, Accuracy: 855/891 (95.95959596%)\n",
      "\n",
      "Test set: Average loss: 0.32983576, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 362 time: 0.981175422668457 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 363 [0/891 (0.00000000%)]\tLoss: 0.45435226\n",
      "Train Epoch: 363 [320/891 (35.71428571%)]\tLoss: 0.04302067\n",
      "Train Epoch: 363 [640/891 (71.42857143%)]\tLoss: 0.17743927\n",
      "Train Set: Average loss: 0.11735899, Accuracy: 857/891 (96.18406285%)\n",
      "\n",
      "Test set: Average loss: 0.34401144, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 363 time: 1.0020725727081299 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 364 [0/891 (0.00000000%)]\tLoss: 0.03907037\n",
      "Train Epoch: 364 [320/891 (35.71428571%)]\tLoss: 0.09718704\n",
      "Train Epoch: 364 [640/891 (71.42857143%)]\tLoss: 0.04150563\n",
      "Train Set: Average loss: 0.08272780, Accuracy: 859/891 (96.40852974%)\n",
      "\n",
      "Test set: Average loss: 0.35394150, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 364 time: 0.9903998374938965 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 365 [0/891 (0.00000000%)]\tLoss: 0.03011376\n",
      "Train Epoch: 365 [320/891 (35.71428571%)]\tLoss: 0.01088947\n",
      "Train Epoch: 365 [640/891 (71.42857143%)]\tLoss: 0.32569152\n",
      "Train Set: Average loss: 0.08659569, Accuracy: 867/891 (97.30639731%)\n",
      "\n",
      "Test set: Average loss: 0.29591256, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 365 time: 1.0240774154663086 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 366 [0/891 (0.00000000%)]\tLoss: 0.07434410\n",
      "Train Epoch: 366 [320/891 (35.71428571%)]\tLoss: 0.31352216\n",
      "Train Epoch: 366 [640/891 (71.42857143%)]\tLoss: 0.33856910\n",
      "Train Set: Average loss: 0.14737629, Accuracy: 854/891 (95.84736251%)\n",
      "\n",
      "Test set: Average loss: 0.28719769, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 366 time: 0.9966955184936523 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 367 [0/891 (0.00000000%)]\tLoss: 0.09362650\n",
      "Train Epoch: 367 [320/891 (35.71428571%)]\tLoss: 0.12237114\n",
      "Train Epoch: 367 [640/891 (71.42857143%)]\tLoss: 0.01753139\n",
      "Train Set: Average loss: 0.08361610, Accuracy: 864/891 (96.96969697%)\n",
      "\n",
      "Test set: Average loss: 0.39833464, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 367 time: 0.9821891784667969 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 368 [0/891 (0.00000000%)]\tLoss: 0.05721581\n",
      "Train Epoch: 368 [320/891 (35.71428571%)]\tLoss: 0.01616699\n",
      "Train Epoch: 368 [640/891 (71.42857143%)]\tLoss: 0.07029670\n",
      "Train Set: Average loss: 0.05774361, Accuracy: 867/891 (97.30639731%)\n",
      "\n",
      "Test set: Average loss: 0.39654647, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 368 time: 0.9892745018005371 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 369 [0/891 (0.00000000%)]\tLoss: 0.04899800\n",
      "Train Epoch: 369 [320/891 (35.71428571%)]\tLoss: 0.23869604\n",
      "Train Epoch: 369 [640/891 (71.42857143%)]\tLoss: 0.06469536\n",
      "Train Set: Average loss: 0.08740750, Accuracy: 865/891 (97.08193042%)\n",
      "\n",
      "Test set: Average loss: 0.31914087, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 369 time: 1.0650885105133057 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 370 [0/891 (0.00000000%)]\tLoss: 0.07678467\n",
      "Train Epoch: 370 [320/891 (35.71428571%)]\tLoss: 0.03913271\n",
      "Train Epoch: 370 [640/891 (71.42857143%)]\tLoss: 0.17222840\n",
      "Train Set: Average loss: 0.11012526, Accuracy: 864/891 (96.96969697%)\n",
      "\n",
      "Test set: Average loss: 0.30711548, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 370 time: 1.0051095485687256 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 371 [0/891 (0.00000000%)]\tLoss: 0.17983449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 371 [320/891 (35.71428571%)]\tLoss: 0.18118668\n",
      "Train Epoch: 371 [640/891 (71.42857143%)]\tLoss: 0.01994443\n",
      "Train Set: Average loss: 0.12077504, Accuracy: 857/891 (96.18406285%)\n",
      "\n",
      "Test set: Average loss: 0.35989648, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 371 time: 1.0260796546936035 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 372 [0/891 (0.00000000%)]\tLoss: 0.02818614\n",
      "Train Epoch: 372 [320/891 (35.71428571%)]\tLoss: 0.20693684\n",
      "Train Epoch: 372 [640/891 (71.42857143%)]\tLoss: 0.06528652\n",
      "Train Set: Average loss: 0.10661008, Accuracy: 863/891 (96.85746352%)\n",
      "\n",
      "Test set: Average loss: 0.35936689, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 372 time: 1.020101547241211 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 373 [0/891 (0.00000000%)]\tLoss: 0.05533528\n",
      "Train Epoch: 373 [320/891 (35.71428571%)]\tLoss: 0.26265812\n",
      "Train Epoch: 373 [640/891 (71.42857143%)]\tLoss: 0.05664265\n",
      "Train Set: Average loss: 0.09593184, Accuracy: 862/891 (96.74523008%)\n",
      "\n",
      "Test set: Average loss: 0.32889740, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 373 time: 1.0547599792480469 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 374 [0/891 (0.00000000%)]\tLoss: 0.04508913\n",
      "Train Epoch: 374 [320/891 (35.71428571%)]\tLoss: 0.14537990\n",
      "Train Epoch: 374 [640/891 (71.42857143%)]\tLoss: 0.32318789\n",
      "Train Set: Average loss: 0.11023625, Accuracy: 857/891 (96.18406285%)\n",
      "\n",
      "Test set: Average loss: 0.32808628, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 374 time: 0.9853470325469971 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.005625\n",
      "Train Epoch: 375 [0/891 (0.00000000%)]\tLoss: 0.02167743\n",
      "Train Epoch: 375 [320/891 (35.71428571%)]\tLoss: 0.03225052\n",
      "Train Epoch: 375 [640/891 (71.42857143%)]\tLoss: 0.30784851\n",
      "Train Set: Average loss: 0.07863560, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.26213467, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 375 time: 1.0116815567016602 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 376 [0/891 (0.00000000%)]\tLoss: 0.03840107\n",
      "Train Epoch: 376 [320/891 (35.71428571%)]\tLoss: 0.06935579\n",
      "Train Epoch: 376 [640/891 (71.42857143%)]\tLoss: 0.05215073\n",
      "Train Set: Average loss: 0.05566891, Accuracy: 876/891 (98.31649832%)\n",
      "\n",
      "Test set: Average loss: 0.30599665, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 376 time: 1.0265769958496094 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 377 [0/891 (0.00000000%)]\tLoss: 0.06015879\n",
      "Train Epoch: 377 [320/891 (35.71428571%)]\tLoss: 0.01356381\n",
      "Train Epoch: 377 [640/891 (71.42857143%)]\tLoss: 0.00741744\n",
      "Train Set: Average loss: 0.07271638, Accuracy: 865/891 (97.08193042%)\n",
      "\n",
      "Test set: Average loss: 0.31395296, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 377 time: 1.017888069152832 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 378 [0/891 (0.00000000%)]\tLoss: 0.01181847\n",
      "Train Epoch: 378 [320/891 (35.71428571%)]\tLoss: 0.01387078\n",
      "Train Epoch: 378 [640/891 (71.42857143%)]\tLoss: 0.01951885\n",
      "Train Set: Average loss: 0.06410698, Accuracy: 869/891 (97.53086420%)\n",
      "\n",
      "Test set: Average loss: 0.33735474, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 378 time: 1.0483613014221191 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 379 [0/891 (0.00000000%)]\tLoss: 0.09457344\n",
      "Train Epoch: 379 [320/891 (35.71428571%)]\tLoss: 0.11558384\n",
      "Train Epoch: 379 [640/891 (71.42857143%)]\tLoss: 0.11984581\n",
      "Train Set: Average loss: 0.07239448, Accuracy: 869/891 (97.53086420%)\n",
      "\n",
      "Test set: Average loss: 0.39690783, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 379 time: 0.9700679779052734 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 380 [0/891 (0.00000000%)]\tLoss: 0.04490882\n",
      "Train Epoch: 380 [320/891 (35.71428571%)]\tLoss: 0.07085973\n",
      "Train Epoch: 380 [640/891 (71.42857143%)]\tLoss: 0.01463372\n",
      "Train Set: Average loss: 0.08274875, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.37883080, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 380 time: 1.0118412971496582 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 381 [0/891 (0.00000000%)]\tLoss: 0.00644988\n",
      "Train Epoch: 381 [320/891 (35.71428571%)]\tLoss: 0.38294232\n",
      "Train Epoch: 381 [640/891 (71.42857143%)]\tLoss: 0.00822228\n",
      "Train Set: Average loss: 0.07105732, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.34574869, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 381 time: 1.031343698501587 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 382 [0/891 (0.00000000%)]\tLoss: 0.10481560\n",
      "Train Epoch: 382 [320/891 (35.71428571%)]\tLoss: 0.10913289\n",
      "Train Epoch: 382 [640/891 (71.42857143%)]\tLoss: 0.03001261\n",
      "Train Set: Average loss: 0.08218624, Accuracy: 867/891 (97.30639731%)\n",
      "\n",
      "Test set: Average loss: 0.31308346, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 382 time: 1.0300476551055908 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 383 [0/891 (0.00000000%)]\tLoss: 0.09615284\n",
      "Train Epoch: 383 [320/891 (35.71428571%)]\tLoss: 0.00986189\n",
      "Train Epoch: 383 [640/891 (71.42857143%)]\tLoss: 0.07719976\n",
      "Train Set: Average loss: 0.06877733, Accuracy: 867/891 (97.30639731%)\n",
      "\n",
      "Test set: Average loss: 0.37189233, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 383 time: 1.0264320373535156 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 384 [0/891 (0.00000000%)]\tLoss: 0.08772779\n",
      "Train Epoch: 384 [320/891 (35.71428571%)]\tLoss: 0.02134836\n",
      "Train Epoch: 384 [640/891 (71.42857143%)]\tLoss: 0.14581776\n",
      "Train Set: Average loss: 0.08147437, Accuracy: 861/891 (96.63299663%)\n",
      "\n",
      "Test set: Average loss: 0.42176289, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 384 time: 1.0374457836151123 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 385 [0/891 (0.00000000%)]\tLoss: 0.13689369\n",
      "Train Epoch: 385 [320/891 (35.71428571%)]\tLoss: 0.04607898\n",
      "Train Epoch: 385 [640/891 (71.42857143%)]\tLoss: 0.08830196\n",
      "Train Set: Average loss: 0.09177070, Accuracy: 862/891 (96.74523008%)\n",
      "\n",
      "Test set: Average loss: 0.33726684, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 385 time: 0.9968791007995605 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 386 [0/891 (0.00000000%)]\tLoss: 0.03506625\n",
      "Train Epoch: 386 [320/891 (35.71428571%)]\tLoss: 0.04416853\n",
      "Train Epoch: 386 [640/891 (71.42857143%)]\tLoss: 0.12359077\n",
      "Train Set: Average loss: 0.07894335, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.34603560, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 386 time: 1.0152223110198975 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 387 [0/891 (0.00000000%)]\tLoss: 0.07826138\n",
      "Train Epoch: 387 [320/891 (35.71428571%)]\tLoss: 0.02420330\n",
      "Train Epoch: 387 [640/891 (71.42857143%)]\tLoss: 0.08435589\n",
      "Train Set: Average loss: 0.06497760, Accuracy: 871/891 (97.75533109%)\n",
      "\n",
      "Test set: Average loss: 0.31376085, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 387 time: 1.0277025699615479 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 388 [0/891 (0.00000000%)]\tLoss: 0.25894511\n",
      "Train Epoch: 388 [320/891 (35.71428571%)]\tLoss: 0.38611627\n",
      "Train Epoch: 388 [640/891 (71.42857143%)]\tLoss: 0.00698531\n",
      "Train Set: Average loss: 0.11121537, Accuracy: 858/891 (96.29629630%)\n",
      "\n",
      "Test set: Average loss: 0.27739039, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 388 time: 1.0512139797210693 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 389 [0/891 (0.00000000%)]\tLoss: 0.02007550\n",
      "Train Epoch: 389 [320/891 (35.71428571%)]\tLoss: 0.00501460\n",
      "Train Epoch: 389 [640/891 (71.42857143%)]\tLoss: 0.02094674\n",
      "Train Set: Average loss: 0.07077565, Accuracy: 871/891 (97.75533109%)\n",
      "\n",
      "Test set: Average loss: 0.31598209, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 389 time: 1.06648588180542 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 390 [0/891 (0.00000000%)]\tLoss: 0.12596011\n",
      "Train Epoch: 390 [320/891 (35.71428571%)]\tLoss: 0.12214521\n",
      "Train Epoch: 390 [640/891 (71.42857143%)]\tLoss: 0.10875505\n",
      "Train Set: Average loss: 0.08272682, Accuracy: 858/891 (96.29629630%)\n",
      "\n",
      "Test set: Average loss: 0.30370096, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 390 time: 1.0205721855163574 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 391 [0/891 (0.00000000%)]\tLoss: 0.03649563\n",
      "Train Epoch: 391 [320/891 (35.71428571%)]\tLoss: 0.66059119\n",
      "Train Epoch: 391 [640/891 (71.42857143%)]\tLoss: 0.08475506\n",
      "Train Set: Average loss: 0.09273415, Accuracy: 865/891 (97.08193042%)\n",
      "\n",
      "Test set: Average loss: 0.30981189, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 391 time: 1.0356669425964355 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 392 [0/891 (0.00000000%)]\tLoss: 0.07524091\n",
      "Train Epoch: 392 [320/891 (35.71428571%)]\tLoss: 0.01379240\n",
      "Train Epoch: 392 [640/891 (71.42857143%)]\tLoss: 0.16381949\n",
      "Train Set: Average loss: 0.05646136, Accuracy: 875/891 (98.20426487%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.32881376, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 392 time: 0.9955010414123535 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 393 [0/891 (0.00000000%)]\tLoss: 0.07212555\n",
      "Train Epoch: 393 [320/891 (35.71428571%)]\tLoss: 0.03635949\n",
      "Train Epoch: 393 [640/891 (71.42857143%)]\tLoss: 0.04090255\n",
      "Train Set: Average loss: 0.08343378, Accuracy: 871/891 (97.75533109%)\n",
      "\n",
      "Test set: Average loss: 0.35568613, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 393 time: 1.0775530338287354 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 394 [0/891 (0.00000000%)]\tLoss: 0.11437088\n",
      "Train Epoch: 394 [320/891 (35.71428571%)]\tLoss: 0.07548177\n",
      "Train Epoch: 394 [640/891 (71.42857143%)]\tLoss: 0.09628499\n",
      "Train Set: Average loss: 0.06711591, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.39333107, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 394 time: 1.0193257331848145 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 395 [0/891 (0.00000000%)]\tLoss: 0.14598387\n",
      "Train Epoch: 395 [320/891 (35.71428571%)]\tLoss: 0.05951440\n",
      "Train Epoch: 395 [640/891 (71.42857143%)]\tLoss: 0.00900036\n",
      "Train Set: Average loss: 0.05769923, Accuracy: 872/891 (97.86756453%)\n",
      "\n",
      "Test set: Average loss: 0.28796096, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 395 time: 1.134200096130371 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 396 [0/891 (0.00000000%)]\tLoss: 0.06447887\n",
      "Train Epoch: 396 [320/891 (35.71428571%)]\tLoss: 0.00919855\n",
      "Train Epoch: 396 [640/891 (71.42857143%)]\tLoss: 0.01322240\n",
      "Train Set: Average loss: 0.06216703, Accuracy: 870/891 (97.64309764%)\n",
      "\n",
      "Test set: Average loss: 0.29312691, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 396 time: 1.0304746627807617 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 397 [0/891 (0.00000000%)]\tLoss: 0.03325498\n",
      "Train Epoch: 397 [320/891 (35.71428571%)]\tLoss: 0.07858896\n",
      "Train Epoch: 397 [640/891 (71.42857143%)]\tLoss: 0.00789094\n",
      "Train Set: Average loss: 0.04939005, Accuracy: 876/891 (98.31649832%)\n",
      "\n",
      "Test set: Average loss: 0.32000867, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 397 time: 1.0159401893615723 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 398 [0/891 (0.00000000%)]\tLoss: 0.03047347\n",
      "Train Epoch: 398 [320/891 (35.71428571%)]\tLoss: 0.08158225\n",
      "Train Epoch: 398 [640/891 (71.42857143%)]\tLoss: 0.00376886\n",
      "Train Set: Average loss: 0.06091213, Accuracy: 877/891 (98.42873176%)\n",
      "\n",
      "Test set: Average loss: 0.38863733, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 398 time: 1.0445420742034912 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 399 [0/891 (0.00000000%)]\tLoss: 0.01270932\n",
      "Train Epoch: 399 [320/891 (35.71428571%)]\tLoss: 0.18793991\n",
      "Train Epoch: 399 [640/891 (71.42857143%)]\tLoss: 0.07846367\n",
      "Train Set: Average loss: 0.08555167, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.34058530, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 399 time: 1.121908187866211 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 400 [0/891 (0.00000000%)]\tLoss: 0.00519735\n",
      "Train Epoch: 400 [320/891 (35.71428571%)]\tLoss: 0.01473379\n",
      "Train Epoch: 400 [640/891 (71.42857143%)]\tLoss: 0.03036469\n",
      "Train Set: Average loss: 0.05246654, Accuracy: 875/891 (98.20426487%)\n",
      "\n",
      "Test set: Average loss: 0.32590874, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 400 time: 1.0002455711364746 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 401 [0/891 (0.00000000%)]\tLoss: 0.06943852\n",
      "Train Epoch: 401 [320/891 (35.71428571%)]\tLoss: 0.00695670\n",
      "Train Epoch: 401 [640/891 (71.42857143%)]\tLoss: 0.09106737\n",
      "Train Set: Average loss: 0.07521509, Accuracy: 871/891 (97.75533109%)\n",
      "\n",
      "Test set: Average loss: 0.30100830, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 401 time: 1.0314104557037354 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 402 [0/891 (0.00000000%)]\tLoss: 0.09964049\n",
      "Train Epoch: 402 [320/891 (35.71428571%)]\tLoss: 0.03563511\n",
      "Train Epoch: 402 [640/891 (71.42857143%)]\tLoss: 0.06608444\n",
      "Train Set: Average loss: 0.06236554, Accuracy: 871/891 (97.75533109%)\n",
      "\n",
      "Test set: Average loss: 0.28692093, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 402 time: 1.082019567489624 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 403 [0/891 (0.00000000%)]\tLoss: 0.10714519\n",
      "Train Epoch: 403 [320/891 (35.71428571%)]\tLoss: 0.04559988\n",
      "Train Epoch: 403 [640/891 (71.42857143%)]\tLoss: 0.02895612\n",
      "Train Set: Average loss: 0.07129701, Accuracy: 866/891 (97.19416386%)\n",
      "\n",
      "Test set: Average loss: 0.26603115, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 403 time: 1.150505542755127 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 404 [0/891 (0.00000000%)]\tLoss: 0.06842691\n",
      "Train Epoch: 404 [320/891 (35.71428571%)]\tLoss: 0.00420797\n",
      "Train Epoch: 404 [640/891 (71.42857143%)]\tLoss: 0.07678530\n",
      "Train Set: Average loss: 0.07694933, Accuracy: 866/891 (97.19416386%)\n",
      "\n",
      "Test set: Average loss: 0.30858913, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 404 time: 1.0576260089874268 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 405 [0/891 (0.00000000%)]\tLoss: 0.06895864\n",
      "Train Epoch: 405 [320/891 (35.71428571%)]\tLoss: 0.01527214\n",
      "Train Epoch: 405 [640/891 (71.42857143%)]\tLoss: 0.04902774\n",
      "Train Set: Average loss: 0.06712561, Accuracy: 870/891 (97.64309764%)\n",
      "\n",
      "Test set: Average loss: 0.44035935, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 405 time: 0.9782042503356934 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 406 [0/891 (0.00000000%)]\tLoss: 0.00152582\n",
      "Train Epoch: 406 [320/891 (35.71428571%)]\tLoss: 0.20459551\n",
      "Train Epoch: 406 [640/891 (71.42857143%)]\tLoss: 0.00823754\n",
      "Train Set: Average loss: 0.09264985, Accuracy: 862/891 (96.74523008%)\n",
      "\n",
      "Test set: Average loss: 0.42639863, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 406 time: 1.0919606685638428 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 407 [0/891 (0.00000000%)]\tLoss: 0.01064515\n",
      "Train Epoch: 407 [320/891 (35.71428571%)]\tLoss: 0.00906146\n",
      "Train Epoch: 407 [640/891 (71.42857143%)]\tLoss: 0.03646183\n",
      "Train Set: Average loss: 0.05053814, Accuracy: 877/891 (98.42873176%)\n",
      "\n",
      "Test set: Average loss: 0.42812679, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 407 time: 1.0527935028076172 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 408 [0/891 (0.00000000%)]\tLoss: 0.01462114\n",
      "Train Epoch: 408 [320/891 (35.71428571%)]\tLoss: 0.02264047\n",
      "Train Epoch: 408 [640/891 (71.42857143%)]\tLoss: 0.16687369\n",
      "Train Set: Average loss: 0.07541698, Accuracy: 861/891 (96.63299663%)\n",
      "\n",
      "Test set: Average loss: 0.31407426, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 408 time: 0.9834282398223877 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 409 [0/891 (0.00000000%)]\tLoss: 0.01651275\n",
      "Train Epoch: 409 [320/891 (35.71428571%)]\tLoss: 0.00164282\n",
      "Train Epoch: 409 [640/891 (71.42857143%)]\tLoss: 0.03495443\n",
      "Train Set: Average loss: 0.07566012, Accuracy: 865/891 (97.08193042%)\n",
      "\n",
      "Test set: Average loss: 0.36842876, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 409 time: 0.9887204170227051 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 410 [0/891 (0.00000000%)]\tLoss: 0.04232931\n",
      "Train Epoch: 410 [320/891 (35.71428571%)]\tLoss: 0.06037462\n",
      "Train Epoch: 410 [640/891 (71.42857143%)]\tLoss: 0.04408777\n",
      "Train Set: Average loss: 0.06710368, Accuracy: 867/891 (97.30639731%)\n",
      "\n",
      "Test set: Average loss: 0.43136668, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 410 time: 1.1218459606170654 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 411 [0/891 (0.00000000%)]\tLoss: 0.06937307\n",
      "Train Epoch: 411 [320/891 (35.71428571%)]\tLoss: 0.01865089\n",
      "Train Epoch: 411 [640/891 (71.42857143%)]\tLoss: 0.02428818\n",
      "Train Set: Average loss: 0.06859634, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.39189926, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 411 time: 1.031512975692749 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 412 [0/891 (0.00000000%)]\tLoss: 0.16224623\n",
      "Train Epoch: 412 [320/891 (35.71428571%)]\tLoss: 0.13087058\n",
      "Train Epoch: 412 [640/891 (71.42857143%)]\tLoss: 0.06148028\n",
      "Train Set: Average loss: 0.05019861, Accuracy: 873/891 (97.97979798%)\n",
      "\n",
      "Test set: Average loss: 0.40370419, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 412 time: 1.0349485874176025 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 413 [0/891 (0.00000000%)]\tLoss: 0.00917077\n",
      "Train Epoch: 413 [320/891 (35.71428571%)]\tLoss: 0.01244903\n",
      "Train Epoch: 413 [640/891 (71.42857143%)]\tLoss: 0.20556384\n",
      "Train Set: Average loss: 0.06793377, Accuracy: 874/891 (98.09203143%)\n",
      "\n",
      "Test set: Average loss: 0.32686012, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 413 time: 0.9953413009643555 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 414 [0/891 (0.00000000%)]\tLoss: 0.01069164\n",
      "Train Epoch: 414 [320/891 (35.71428571%)]\tLoss: 0.04252106\n",
      "Train Epoch: 414 [640/891 (71.42857143%)]\tLoss: 0.01273787\n",
      "Train Set: Average loss: 0.07429097, Accuracy: 870/891 (97.64309764%)\n",
      "\n",
      "Test set: Average loss: 0.30032420, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 414 time: 0.9864890575408936 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 415 [0/891 (0.00000000%)]\tLoss: 0.02030593\n",
      "Train Epoch: 415 [320/891 (35.71428571%)]\tLoss: 0.08513558\n",
      "Train Epoch: 415 [640/891 (71.42857143%)]\tLoss: 0.06540900\n",
      "Train Set: Average loss: 0.04844939, Accuracy: 876/891 (98.31649832%)\n",
      "\n",
      "Test set: Average loss: 0.33530730, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 415 time: 0.9852800369262695 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 416 [0/891 (0.00000000%)]\tLoss: 0.15373027\n",
      "Train Epoch: 416 [320/891 (35.71428571%)]\tLoss: 0.01373369\n",
      "Train Epoch: 416 [640/891 (71.42857143%)]\tLoss: 0.03270197\n",
      "Train Set: Average loss: 0.05385369, Accuracy: 872/891 (97.86756453%)\n",
      "\n",
      "Test set: Average loss: 0.35151100, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 416 time: 1.0284528732299805 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 417 [0/891 (0.00000000%)]\tLoss: 0.05212480\n",
      "Train Epoch: 417 [320/891 (35.71428571%)]\tLoss: 0.05165190\n",
      "Train Epoch: 417 [640/891 (71.42857143%)]\tLoss: 0.03478843\n",
      "Train Set: Average loss: 0.05789219, Accuracy: 873/891 (97.97979798%)\n",
      "\n",
      "Test set: Average loss: 0.39865047, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 417 time: 0.989962100982666 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 418 [0/891 (0.00000000%)]\tLoss: 0.08997041\n",
      "Train Epoch: 418 [320/891 (35.71428571%)]\tLoss: 0.00733507\n",
      "Train Epoch: 418 [640/891 (71.42857143%)]\tLoss: 0.01523674\n",
      "Train Set: Average loss: 0.07836213, Accuracy: 869/891 (97.53086420%)\n",
      "\n",
      "Test set: Average loss: 0.31185114, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 418 time: 1.0124642848968506 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 419 [0/891 (0.00000000%)]\tLoss: 0.14278942\n",
      "Train Epoch: 419 [320/891 (35.71428571%)]\tLoss: 0.00709873\n",
      "Train Epoch: 419 [640/891 (71.42857143%)]\tLoss: 0.00435948\n",
      "Train Set: Average loss: 0.06122088, Accuracy: 875/891 (98.20426487%)\n",
      "\n",
      "Test set: Average loss: 0.33376601, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 419 time: 1.0890576839447021 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 420 [0/891 (0.00000000%)]\tLoss: 0.01094270\n",
      "Train Epoch: 420 [320/891 (35.71428571%)]\tLoss: 0.02107424\n",
      "Train Epoch: 420 [640/891 (71.42857143%)]\tLoss: 0.09888643\n",
      "Train Set: Average loss: 0.06230745, Accuracy: 876/891 (98.31649832%)\n",
      "\n",
      "Test set: Average loss: 0.31353074, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 420 time: 1.1517326831817627 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 421 [0/891 (0.00000000%)]\tLoss: 0.04839700\n",
      "Train Epoch: 421 [320/891 (35.71428571%)]\tLoss: 0.02920389\n",
      "Train Epoch: 421 [640/891 (71.42857143%)]\tLoss: 0.01149458\n",
      "Train Set: Average loss: 0.06247050, Accuracy: 871/891 (97.75533109%)\n",
      "\n",
      "Test set: Average loss: 0.31107118, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 421 time: 1.0443692207336426 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 422 [0/891 (0.00000000%)]\tLoss: 0.01074487\n",
      "Train Epoch: 422 [320/891 (35.71428571%)]\tLoss: 0.05625808\n",
      "Train Epoch: 422 [640/891 (71.42857143%)]\tLoss: 0.00556248\n",
      "Train Set: Average loss: 0.07885800, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.34846840, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 422 time: 1.0002338886260986 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 423 [0/891 (0.00000000%)]\tLoss: 0.15313727\n",
      "Train Epoch: 423 [320/891 (35.71428571%)]\tLoss: 0.02026981\n",
      "Train Epoch: 423 [640/891 (71.42857143%)]\tLoss: 0.16399646\n",
      "Train Set: Average loss: 0.05366171, Accuracy: 876/891 (98.31649832%)\n",
      "\n",
      "Test set: Average loss: 0.34574112, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 423 time: 1.0063791275024414 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 424 [0/891 (0.00000000%)]\tLoss: 0.04282612\n",
      "Train Epoch: 424 [320/891 (35.71428571%)]\tLoss: 0.01269847\n",
      "Train Epoch: 424 [640/891 (71.42857143%)]\tLoss: 0.06741029\n",
      "Train Set: Average loss: 0.06466444, Accuracy: 873/891 (97.97979798%)\n",
      "\n",
      "Test set: Average loss: 0.36832299, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 424 time: 1.0383901596069336 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 425 [0/891 (0.00000000%)]\tLoss: 0.14759326\n",
      "Train Epoch: 425 [320/891 (35.71428571%)]\tLoss: 0.04971510\n",
      "Train Epoch: 425 [640/891 (71.42857143%)]\tLoss: 0.02234608\n",
      "Train Set: Average loss: 0.07414754, Accuracy: 871/891 (97.75533109%)\n",
      "\n",
      "Test set: Average loss: 0.34356096, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 425 time: 1.0173680782318115 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 426 [0/891 (0.00000000%)]\tLoss: 0.07676899\n",
      "Train Epoch: 426 [320/891 (35.71428571%)]\tLoss: 0.17720044\n",
      "Train Epoch: 426 [640/891 (71.42857143%)]\tLoss: 0.00342631\n",
      "Train Set: Average loss: 0.05925789, Accuracy: 872/891 (97.86756453%)\n",
      "\n",
      "Test set: Average loss: 0.34010841, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 426 time: 1.0999326705932617 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 427 [0/891 (0.00000000%)]\tLoss: 0.00765848\n",
      "Train Epoch: 427 [320/891 (35.71428571%)]\tLoss: 0.05590415\n",
      "Train Epoch: 427 [640/891 (71.42857143%)]\tLoss: 0.09507263\n",
      "Train Set: Average loss: 0.05139558, Accuracy: 874/891 (98.09203143%)\n",
      "\n",
      "Test set: Average loss: 0.32817018, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 427 time: 1.1000964641571045 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 428 [0/891 (0.00000000%)]\tLoss: 0.14567471\n",
      "Train Epoch: 428 [320/891 (35.71428571%)]\tLoss: 0.20069343\n",
      "Train Epoch: 428 [640/891 (71.42857143%)]\tLoss: 0.26157647\n",
      "Train Set: Average loss: 0.05678824, Accuracy: 873/891 (97.97979798%)\n",
      "\n",
      "Test set: Average loss: 0.35190655, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 428 time: 1.1653132438659668 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 429 [0/891 (0.00000000%)]\tLoss: 0.00457770\n",
      "Train Epoch: 429 [320/891 (35.71428571%)]\tLoss: 0.00654149\n",
      "Train Epoch: 429 [640/891 (71.42857143%)]\tLoss: 0.00666362\n",
      "Train Set: Average loss: 0.07060513, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.44119660, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 429 time: 1.1145341396331787 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 430 [0/891 (0.00000000%)]\tLoss: 0.00797248\n",
      "Train Epoch: 430 [320/891 (35.71428571%)]\tLoss: 0.14714545\n",
      "Train Epoch: 430 [640/891 (71.42857143%)]\tLoss: 0.00109959\n",
      "Train Set: Average loss: 0.05434995, Accuracy: 875/891 (98.20426487%)\n",
      "\n",
      "Test set: Average loss: 0.42295624, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 430 time: 1.073136568069458 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 431 [0/891 (0.00000000%)]\tLoss: 0.00257552\n",
      "Train Epoch: 431 [320/891 (35.71428571%)]\tLoss: 0.10236871\n",
      "Train Epoch: 431 [640/891 (71.42857143%)]\tLoss: 0.08740842\n",
      "Train Set: Average loss: 0.05856878, Accuracy: 873/891 (97.97979798%)\n",
      "\n",
      "Test set: Average loss: 0.44644760, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 431 time: 1.0310113430023193 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 432 [0/891 (0.00000000%)]\tLoss: 0.02604663\n",
      "Train Epoch: 432 [320/891 (35.71428571%)]\tLoss: 0.02912080\n",
      "Train Epoch: 432 [640/891 (71.42857143%)]\tLoss: 0.06702018\n",
      "Train Set: Average loss: 0.07678275, Accuracy: 869/891 (97.53086420%)\n",
      "\n",
      "Test set: Average loss: 0.41144182, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 432 time: 1.0160515308380127 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 433 [0/891 (0.00000000%)]\tLoss: 0.04892486\n",
      "Train Epoch: 433 [320/891 (35.71428571%)]\tLoss: 0.04630780\n",
      "Train Epoch: 433 [640/891 (71.42857143%)]\tLoss: 0.03840816\n",
      "Train Set: Average loss: 0.04258845, Accuracy: 875/891 (98.20426487%)\n",
      "\n",
      "Test set: Average loss: 0.39420673, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 433 time: 1.0469465255737305 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 434 [0/891 (0.00000000%)]\tLoss: 0.01248813\n",
      "Train Epoch: 434 [320/891 (35.71428571%)]\tLoss: 0.22571033\n",
      "Train Epoch: 434 [640/891 (71.42857143%)]\tLoss: 0.11091739\n",
      "Train Set: Average loss: 0.05106287, Accuracy: 877/891 (98.42873176%)\n",
      "\n",
      "Test set: Average loss: 0.39990845, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 434 time: 0.9781608581542969 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 435 [0/891 (0.00000000%)]\tLoss: 0.02499580\n",
      "Train Epoch: 435 [320/891 (35.71428571%)]\tLoss: 0.11416745\n",
      "Train Epoch: 435 [640/891 (71.42857143%)]\tLoss: 0.00790912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: Average loss: 0.05007930, Accuracy: 879/891 (98.65319865%)\n",
      "\n",
      "Test set: Average loss: 0.36127676, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 435 time: 1.0663492679595947 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 436 [0/891 (0.00000000%)]\tLoss: 0.01602972\n",
      "Train Epoch: 436 [320/891 (35.71428571%)]\tLoss: 0.14107138\n",
      "Train Epoch: 436 [640/891 (71.42857143%)]\tLoss: 0.06315672\n",
      "Train Set: Average loss: 0.05228824, Accuracy: 872/891 (97.86756453%)\n",
      "\n",
      "Test set: Average loss: 0.38234356, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 436 time: 1.0459775924682617 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 437 [0/891 (0.00000000%)]\tLoss: 0.08044720\n",
      "Train Epoch: 437 [320/891 (35.71428571%)]\tLoss: 0.04625314\n",
      "Train Epoch: 437 [640/891 (71.42857143%)]\tLoss: 0.00819767\n",
      "Train Set: Average loss: 0.04655275, Accuracy: 880/891 (98.76543210%)\n",
      "\n",
      "Test set: Average loss: 0.39746429, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 437 time: 1.0386168956756592 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 438 [0/891 (0.00000000%)]\tLoss: 0.01653564\n",
      "Train Epoch: 438 [320/891 (35.71428571%)]\tLoss: 0.07190102\n",
      "Train Epoch: 438 [640/891 (71.42857143%)]\tLoss: 0.01322967\n",
      "Train Set: Average loss: 0.05949210, Accuracy: 875/891 (98.20426487%)\n",
      "\n",
      "Test set: Average loss: 0.33884070, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 438 time: 1.0137865543365479 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 439 [0/891 (0.00000000%)]\tLoss: 0.01768517\n",
      "Train Epoch: 439 [320/891 (35.71428571%)]\tLoss: 0.15469742\n",
      "Train Epoch: 439 [640/891 (71.42857143%)]\tLoss: 0.10428777\n",
      "Train Set: Average loss: 0.06498645, Accuracy: 872/891 (97.86756453%)\n",
      "\n",
      "Test set: Average loss: 0.31763356, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 439 time: 1.0547637939453125 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 440 [0/891 (0.00000000%)]\tLoss: 0.00353813\n",
      "Train Epoch: 440 [320/891 (35.71428571%)]\tLoss: 0.06384873\n",
      "Train Epoch: 440 [640/891 (71.42857143%)]\tLoss: 0.01361090\n",
      "Train Set: Average loss: 0.04795263, Accuracy: 877/891 (98.42873176%)\n",
      "\n",
      "Test set: Average loss: 0.32057860, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 440 time: 1.0439965724945068 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 441 [0/891 (0.00000000%)]\tLoss: 0.25384712\n",
      "Train Epoch: 441 [320/891 (35.71428571%)]\tLoss: 0.00881684\n",
      "Train Epoch: 441 [640/891 (71.42857143%)]\tLoss: 0.22034979\n",
      "Train Set: Average loss: 0.06806292, Accuracy: 872/891 (97.86756453%)\n",
      "\n",
      "Test set: Average loss: 0.40082951, Accuracy: 90/99 (90.90909091%)\n",
      "\n",
      "Epoch 441 time: 1.084177017211914 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 442 [0/891 (0.00000000%)]\tLoss: 0.19367999\n",
      "Train Epoch: 442 [320/891 (35.71428571%)]\tLoss: 0.19049698\n",
      "Train Epoch: 442 [640/891 (71.42857143%)]\tLoss: 0.05367541\n",
      "Train Set: Average loss: 0.11739030, Accuracy: 852/891 (95.62289562%)\n",
      "\n",
      "Test set: Average loss: 0.33905632, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 442 time: 1.0454840660095215 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 443 [0/891 (0.00000000%)]\tLoss: 0.01549488\n",
      "Train Epoch: 443 [320/891 (35.71428571%)]\tLoss: 0.04271024\n",
      "Train Epoch: 443 [640/891 (71.42857143%)]\tLoss: 0.00664496\n",
      "Train Set: Average loss: 0.07091707, Accuracy: 870/891 (97.64309764%)\n",
      "\n",
      "Test set: Average loss: 0.34411558, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 443 time: 1.0474128723144531 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 444 [0/891 (0.00000000%)]\tLoss: 0.02567899\n",
      "Train Epoch: 444 [320/891 (35.71428571%)]\tLoss: 0.19803047\n",
      "Train Epoch: 444 [640/891 (71.42857143%)]\tLoss: 0.17609203\n",
      "Train Set: Average loss: 0.04693178, Accuracy: 881/891 (98.87766554%)\n",
      "\n",
      "Test set: Average loss: 0.30833000, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 444 time: 1.049006700515747 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 445 [0/891 (0.00000000%)]\tLoss: 0.02160811\n",
      "Train Epoch: 445 [320/891 (35.71428571%)]\tLoss: 0.14872575\n",
      "Train Epoch: 445 [640/891 (71.42857143%)]\tLoss: 0.02352017\n",
      "Train Set: Average loss: 0.06111532, Accuracy: 870/891 (97.64309764%)\n",
      "\n",
      "Test set: Average loss: 0.28628074, Accuracy: 95/99 (95.95959596%)\n",
      "\n",
      "Epoch 445 time: 1.017698049545288 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 446 [0/891 (0.00000000%)]\tLoss: 0.02172470\n",
      "Train Epoch: 446 [320/891 (35.71428571%)]\tLoss: 0.24924326\n",
      "Train Epoch: 446 [640/891 (71.42857143%)]\tLoss: 0.13752574\n",
      "Train Set: Average loss: 0.07028941, Accuracy: 867/891 (97.30639731%)\n",
      "\n",
      "Test set: Average loss: 0.27471845, Accuracy: 95/99 (95.95959596%)\n",
      "\n",
      "Epoch 446 time: 0.9641375541687012 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 447 [0/891 (0.00000000%)]\tLoss: 0.09960967\n",
      "Train Epoch: 447 [320/891 (35.71428571%)]\tLoss: 0.00938827\n",
      "Train Epoch: 447 [640/891 (71.42857143%)]\tLoss: 0.04749185\n",
      "Train Set: Average loss: 0.05660206, Accuracy: 872/891 (97.86756453%)\n",
      "\n",
      "Test set: Average loss: 0.30512236, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 447 time: 1.0065324306488037 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 448 [0/891 (0.00000000%)]\tLoss: 0.01355422\n",
      "Train Epoch: 448 [320/891 (35.71428571%)]\tLoss: 0.03443998\n",
      "Train Epoch: 448 [640/891 (71.42857143%)]\tLoss: 0.00279754\n",
      "Train Set: Average loss: 0.05394112, Accuracy: 873/891 (97.97979798%)\n",
      "\n",
      "Test set: Average loss: 0.30512095, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 448 time: 1.043743371963501 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 449 [0/891 (0.00000000%)]\tLoss: 0.03808349\n",
      "Train Epoch: 449 [320/891 (35.71428571%)]\tLoss: 0.09891361\n",
      "Train Epoch: 449 [640/891 (71.42857143%)]\tLoss: 0.00376087\n",
      "Train Set: Average loss: 0.03712416, Accuracy: 879/891 (98.65319865%)\n",
      "\n",
      "Test set: Average loss: 0.28451330, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 449 time: 1.092350721359253 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 450 [0/891 (0.00000000%)]\tLoss: 0.03334373\n",
      "Train Epoch: 450 [320/891 (35.71428571%)]\tLoss: 0.08658922\n",
      "Train Epoch: 450 [640/891 (71.42857143%)]\tLoss: 0.00112182\n",
      "Train Set: Average loss: 0.05222911, Accuracy: 881/891 (98.87766554%)\n",
      "\n",
      "Test set: Average loss: 0.26550372, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 450 time: 1.0155205726623535 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 451 [0/891 (0.00000000%)]\tLoss: 0.14283592\n",
      "Train Epoch: 451 [320/891 (35.71428571%)]\tLoss: 0.02061039\n",
      "Train Epoch: 451 [640/891 (71.42857143%)]\tLoss: 0.04747695\n",
      "Train Set: Average loss: 0.04173622, Accuracy: 879/891 (98.65319865%)\n",
      "\n",
      "Test set: Average loss: 0.27130447, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 451 time: 1.0504662990570068 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 452 [0/891 (0.00000000%)]\tLoss: 0.12786126\n",
      "Train Epoch: 452 [320/891 (35.71428571%)]\tLoss: 0.21402287\n",
      "Train Epoch: 452 [640/891 (71.42857143%)]\tLoss: 0.07373556\n",
      "Train Set: Average loss: 0.05689560, Accuracy: 872/891 (97.86756453%)\n",
      "\n",
      "Test set: Average loss: 0.27478395, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 452 time: 0.9883396625518799 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 453 [0/891 (0.00000000%)]\tLoss: 0.00719339\n",
      "Train Epoch: 453 [320/891 (35.71428571%)]\tLoss: 0.04162568\n",
      "Train Epoch: 453 [640/891 (71.42857143%)]\tLoss: 0.12396848\n",
      "Train Set: Average loss: 0.04763793, Accuracy: 877/891 (98.42873176%)\n",
      "\n",
      "Test set: Average loss: 0.30772949, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 453 time: 1.0212855339050293 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 454 [0/891 (0.00000000%)]\tLoss: 0.07778311\n",
      "Train Epoch: 454 [320/891 (35.71428571%)]\tLoss: 0.09949380\n",
      "Train Epoch: 454 [640/891 (71.42857143%)]\tLoss: 0.01039004\n",
      "Train Set: Average loss: 0.06476534, Accuracy: 871/891 (97.75533109%)\n",
      "\n",
      "Test set: Average loss: 0.29148786, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 454 time: 1.0349223613739014 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 455 [0/891 (0.00000000%)]\tLoss: 0.02046949\n",
      "Train Epoch: 455 [320/891 (35.71428571%)]\tLoss: 0.30330420\n",
      "Train Epoch: 455 [640/891 (71.42857143%)]\tLoss: 0.25223130\n",
      "Train Set: Average loss: 0.06517510, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.31308924, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 455 time: 1.0107231140136719 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 456 [0/891 (0.00000000%)]\tLoss: 0.01546675\n",
      "Train Epoch: 456 [320/891 (35.71428571%)]\tLoss: 0.00257909\n",
      "Train Epoch: 456 [640/891 (71.42857143%)]\tLoss: 0.04266304\n",
      "Train Set: Average loss: 0.05623469, Accuracy: 872/891 (97.86756453%)\n",
      "\n",
      "Test set: Average loss: 0.32302567, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 456 time: 0.9974775314331055 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 457 [0/891 (0.00000000%)]\tLoss: 0.01505655\n",
      "Train Epoch: 457 [320/891 (35.71428571%)]\tLoss: 0.03122908\n",
      "Train Epoch: 457 [640/891 (71.42857143%)]\tLoss: 0.02901852\n",
      "Train Set: Average loss: 0.05691626, Accuracy: 878/891 (98.54096521%)\n",
      "\n",
      "Test set: Average loss: 0.30009834, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 457 time: 1.04537034034729 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 458 [0/891 (0.00000000%)]\tLoss: 0.32815742\n",
      "Train Epoch: 458 [320/891 (35.71428571%)]\tLoss: 0.01491541\n",
      "Train Epoch: 458 [640/891 (71.42857143%)]\tLoss: 0.01182866\n",
      "Train Set: Average loss: 0.05351354, Accuracy: 876/891 (98.31649832%)\n",
      "\n",
      "Test set: Average loss: 0.31664357, Accuracy: 95/99 (95.95959596%)\n",
      "\n",
      "Epoch 458 time: 1.0679798126220703 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 459 [0/891 (0.00000000%)]\tLoss: 0.15849072\n",
      "Train Epoch: 459 [320/891 (35.71428571%)]\tLoss: 0.20207977\n",
      "Train Epoch: 459 [640/891 (71.42857143%)]\tLoss: 0.05798757\n",
      "Train Set: Average loss: 0.07140787, Accuracy: 872/891 (97.86756453%)\n",
      "\n",
      "Test set: Average loss: 0.31947217, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 459 time: 1.0885140895843506 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 460 [0/891 (0.00000000%)]\tLoss: 0.06067401\n",
      "Train Epoch: 460 [320/891 (35.71428571%)]\tLoss: 0.00488669\n",
      "Train Epoch: 460 [640/891 (71.42857143%)]\tLoss: 0.03444552\n",
      "Train Set: Average loss: 0.07631553, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.33212196, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 460 time: 1.0894660949707031 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 461 [0/891 (0.00000000%)]\tLoss: 0.04714370\n",
      "Train Epoch: 461 [320/891 (35.71428571%)]\tLoss: 0.09218234\n",
      "Train Epoch: 461 [640/891 (71.42857143%)]\tLoss: 0.01004541\n",
      "Train Set: Average loss: 0.05562093, Accuracy: 876/891 (98.31649832%)\n",
      "\n",
      "Test set: Average loss: 0.34608332, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 461 time: 1.0196611881256104 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 462 [0/891 (0.00000000%)]\tLoss: 0.04249525\n",
      "Train Epoch: 462 [320/891 (35.71428571%)]\tLoss: 0.02837050\n",
      "Train Epoch: 462 [640/891 (71.42857143%)]\tLoss: 0.00589675\n",
      "Train Set: Average loss: 0.04657189, Accuracy: 876/891 (98.31649832%)\n",
      "\n",
      "Test set: Average loss: 0.34295323, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 462 time: 1.0666680335998535 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 463 [0/891 (0.00000000%)]\tLoss: 0.06557089\n",
      "Train Epoch: 463 [320/891 (35.71428571%)]\tLoss: 0.05506039\n",
      "Train Epoch: 463 [640/891 (71.42857143%)]\tLoss: 0.00673580\n",
      "Train Set: Average loss: 0.04545365, Accuracy: 876/891 (98.31649832%)\n",
      "\n",
      "Test set: Average loss: 0.27443745, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 463 time: 1.0420150756835938 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 464 [0/891 (0.00000000%)]\tLoss: 0.01063490\n",
      "Train Epoch: 464 [320/891 (35.71428571%)]\tLoss: 0.06170422\n",
      "Train Epoch: 464 [640/891 (71.42857143%)]\tLoss: 0.02585584\n",
      "Train Set: Average loss: 0.05034540, Accuracy: 874/891 (98.09203143%)\n",
      "\n",
      "Test set: Average loss: 0.28609206, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 464 time: 1.1051664352416992 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 465 [0/891 (0.00000000%)]\tLoss: 0.02221185\n",
      "Train Epoch: 465 [320/891 (35.71428571%)]\tLoss: 0.03532052\n",
      "Train Epoch: 465 [640/891 (71.42857143%)]\tLoss: 0.06807506\n",
      "Train Set: Average loss: 0.05332892, Accuracy: 879/891 (98.65319865%)\n",
      "\n",
      "Test set: Average loss: 0.27828750, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 465 time: 1.0921781063079834 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 466 [0/891 (0.00000000%)]\tLoss: 0.09184182\n",
      "Train Epoch: 466 [320/891 (35.71428571%)]\tLoss: 0.00049847\n",
      "Train Epoch: 466 [640/891 (71.42857143%)]\tLoss: 0.04744571\n",
      "Train Set: Average loss: 0.07358899, Accuracy: 871/891 (97.75533109%)\n",
      "\n",
      "Test set: Average loss: 0.26669947, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 466 time: 1.034433364868164 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 467 [0/891 (0.00000000%)]\tLoss: 0.00333035\n",
      "Train Epoch: 467 [320/891 (35.71428571%)]\tLoss: 0.03509277\n",
      "Train Epoch: 467 [640/891 (71.42857143%)]\tLoss: 0.05199993\n",
      "Train Set: Average loss: 0.07273585, Accuracy: 869/891 (97.53086420%)\n",
      "\n",
      "Test set: Average loss: 0.27542243, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 467 time: 1.100684642791748 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 468 [0/891 (0.00000000%)]\tLoss: 0.00608641\n",
      "Train Epoch: 468 [320/891 (35.71428571%)]\tLoss: 0.00539583\n",
      "Train Epoch: 468 [640/891 (71.42857143%)]\tLoss: 0.25343287\n",
      "Train Set: Average loss: 0.05468192, Accuracy: 874/891 (98.09203143%)\n",
      "\n",
      "Test set: Average loss: 0.33792977, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 468 time: 1.0558290481567383 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 469 [0/891 (0.00000000%)]\tLoss: 0.02091509\n",
      "Train Epoch: 469 [320/891 (35.71428571%)]\tLoss: 0.10130566\n",
      "Train Epoch: 469 [640/891 (71.42857143%)]\tLoss: 0.01326537\n",
      "Train Set: Average loss: 0.05655683, Accuracy: 879/891 (98.65319865%)\n",
      "\n",
      "Test set: Average loss: 0.23226160, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 469 time: 1.0377800464630127 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 470 [0/891 (0.00000000%)]\tLoss: 0.07004374\n",
      "Train Epoch: 470 [320/891 (35.71428571%)]\tLoss: 0.02290124\n",
      "Train Epoch: 470 [640/891 (71.42857143%)]\tLoss: 0.00288528\n",
      "Train Set: Average loss: 0.04807617, Accuracy: 879/891 (98.65319865%)\n",
      "\n",
      "Test set: Average loss: 0.22613304, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 470 time: 1.0523004531860352 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 471 [0/891 (0.00000000%)]\tLoss: 0.12550181\n",
      "Train Epoch: 471 [320/891 (35.71428571%)]\tLoss: 0.00387144\n",
      "Train Epoch: 471 [640/891 (71.42857143%)]\tLoss: 0.12281430\n",
      "Train Set: Average loss: 0.06969086, Accuracy: 866/891 (97.19416386%)\n",
      "\n",
      "Test set: Average loss: 0.25731632, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 471 time: 1.0573842525482178 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 472 [0/891 (0.00000000%)]\tLoss: 0.11146855\n",
      "Train Epoch: 472 [320/891 (35.71428571%)]\tLoss: 0.15251106\n",
      "Train Epoch: 472 [640/891 (71.42857143%)]\tLoss: 0.14521760\n",
      "Train Set: Average loss: 0.06689578, Accuracy: 870/891 (97.64309764%)\n",
      "\n",
      "Test set: Average loss: 0.32169829, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 472 time: 1.0461056232452393 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 473 [0/891 (0.00000000%)]\tLoss: 0.02967960\n",
      "Train Epoch: 473 [320/891 (35.71428571%)]\tLoss: 0.01367271\n",
      "Train Epoch: 473 [640/891 (71.42857143%)]\tLoss: 0.03968477\n",
      "Train Set: Average loss: 0.04706718, Accuracy: 875/891 (98.20426487%)\n",
      "\n",
      "Test set: Average loss: 0.30927943, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 473 time: 1.011225700378418 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 474 [0/891 (0.00000000%)]\tLoss: 0.03139430\n",
      "Train Epoch: 474 [320/891 (35.71428571%)]\tLoss: 0.02364767\n",
      "Train Epoch: 474 [640/891 (71.42857143%)]\tLoss: 0.00846815\n",
      "Train Set: Average loss: 0.06364815, Accuracy: 870/891 (97.64309764%)\n",
      "\n",
      "Test set: Average loss: 0.25530131, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 474 time: 1.0287318229675293 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 475 [0/891 (0.00000000%)]\tLoss: 0.04390037\n",
      "Train Epoch: 475 [320/891 (35.71428571%)]\tLoss: 0.01456523\n",
      "Train Epoch: 475 [640/891 (71.42857143%)]\tLoss: 0.00999331\n",
      "Train Set: Average loss: 0.04506170, Accuracy: 875/891 (98.20426487%)\n",
      "\n",
      "Test set: Average loss: 0.31045303, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 475 time: 1.010204792022705 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 476 [0/891 (0.00000000%)]\tLoss: 0.00346899\n",
      "Train Epoch: 476 [320/891 (35.71428571%)]\tLoss: 0.02625683\n",
      "Train Epoch: 476 [640/891 (71.42857143%)]\tLoss: 0.01737899\n",
      "Train Set: Average loss: 0.03777562, Accuracy: 878/891 (98.54096521%)\n",
      "\n",
      "Test set: Average loss: 0.31194436, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 476 time: 1.003692865371704 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 477 [0/891 (0.00000000%)]\tLoss: 0.03766376\n",
      "Train Epoch: 477 [320/891 (35.71428571%)]\tLoss: 0.02245659\n",
      "Train Epoch: 477 [640/891 (71.42857143%)]\tLoss: 0.02610379\n",
      "Train Set: Average loss: 0.05612769, Accuracy: 871/891 (97.75533109%)\n",
      "\n",
      "Test set: Average loss: 0.29968566, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 477 time: 1.0065879821777344 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 478 [0/891 (0.00000000%)]\tLoss: 0.01307434\n",
      "Train Epoch: 478 [320/891 (35.71428571%)]\tLoss: 0.03000414\n",
      "Train Epoch: 478 [640/891 (71.42857143%)]\tLoss: 0.30577999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: Average loss: 0.07991931, Accuracy: 872/891 (97.86756453%)\n",
      "\n",
      "Test set: Average loss: 0.32787529, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 478 time: 1.040968894958496 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 479 [0/891 (0.00000000%)]\tLoss: 0.07583642\n",
      "Train Epoch: 479 [320/891 (35.71428571%)]\tLoss: 0.02227831\n",
      "Train Epoch: 479 [640/891 (71.42857143%)]\tLoss: 0.00229257\n",
      "Train Set: Average loss: 0.05767822, Accuracy: 870/891 (97.64309764%)\n",
      "\n",
      "Test set: Average loss: 0.35785527, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 479 time: 0.9630050659179688 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 480 [0/891 (0.00000000%)]\tLoss: 0.00046021\n",
      "Train Epoch: 480 [320/891 (35.71428571%)]\tLoss: 0.00586146\n",
      "Train Epoch: 480 [640/891 (71.42857143%)]\tLoss: 0.13269317\n",
      "Train Set: Average loss: 0.04072284, Accuracy: 881/891 (98.87766554%)\n",
      "\n",
      "Test set: Average loss: 0.35312392, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 480 time: 1.026679515838623 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 481 [0/891 (0.00000000%)]\tLoss: 0.00529617\n",
      "Train Epoch: 481 [320/891 (35.71428571%)]\tLoss: 0.02188325\n",
      "Train Epoch: 481 [640/891 (71.42857143%)]\tLoss: 0.00800395\n",
      "Train Set: Average loss: 0.05392738, Accuracy: 875/891 (98.20426487%)\n",
      "\n",
      "Test set: Average loss: 0.32200449, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 481 time: 1.0269200801849365 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 482 [0/891 (0.00000000%)]\tLoss: 0.00185966\n",
      "Train Epoch: 482 [320/891 (35.71428571%)]\tLoss: 0.11429602\n",
      "Train Epoch: 482 [640/891 (71.42857143%)]\tLoss: 0.08170128\n",
      "Train Set: Average loss: 0.08604984, Accuracy: 864/891 (96.96969697%)\n",
      "\n",
      "Test set: Average loss: 0.27497475, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 482 time: 1.0481958389282227 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 483 [0/891 (0.00000000%)]\tLoss: 0.01484114\n",
      "Train Epoch: 483 [320/891 (35.71428571%)]\tLoss: 0.00263447\n",
      "Train Epoch: 483 [640/891 (71.42857143%)]\tLoss: 0.04511976\n",
      "Train Set: Average loss: 0.05840945, Accuracy: 874/891 (98.09203143%)\n",
      "\n",
      "Test set: Average loss: 0.23108440, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 483 time: 1.0450341701507568 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 484 [0/891 (0.00000000%)]\tLoss: 0.00079465\n",
      "Train Epoch: 484 [320/891 (35.71428571%)]\tLoss: 0.26648414\n",
      "Train Epoch: 484 [640/891 (71.42857143%)]\tLoss: 0.00411636\n",
      "Train Set: Average loss: 0.04539528, Accuracy: 878/891 (98.54096521%)\n",
      "\n",
      "Test set: Average loss: 0.26372335, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 484 time: 1.0628790855407715 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 485 [0/891 (0.00000000%)]\tLoss: 0.12415630\n",
      "Train Epoch: 485 [320/891 (35.71428571%)]\tLoss: 0.12312824\n",
      "Train Epoch: 485 [640/891 (71.42857143%)]\tLoss: 0.00474852\n",
      "Train Set: Average loss: 0.05955811, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.22483153, Accuracy: 95/99 (95.95959596%)\n",
      "\n",
      "Epoch 485 time: 1.0305876731872559 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 486 [0/891 (0.00000000%)]\tLoss: 0.00184911\n",
      "Train Epoch: 486 [320/891 (35.71428571%)]\tLoss: 0.11306071\n",
      "Train Epoch: 486 [640/891 (71.42857143%)]\tLoss: 0.06935680\n",
      "Train Set: Average loss: 0.06726112, Accuracy: 871/891 (97.75533109%)\n",
      "\n",
      "Test set: Average loss: 0.26204814, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 486 time: 1.0641295909881592 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 487 [0/891 (0.00000000%)]\tLoss: 0.00788170\n",
      "Train Epoch: 487 [320/891 (35.71428571%)]\tLoss: 0.01500273\n",
      "Train Epoch: 487 [640/891 (71.42857143%)]\tLoss: 0.02009141\n",
      "Train Set: Average loss: 0.03569614, Accuracy: 882/891 (98.98989899%)\n",
      "\n",
      "Test set: Average loss: 0.27627024, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 487 time: 1.0154705047607422 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 488 [0/891 (0.00000000%)]\tLoss: 0.00211197\n",
      "Train Epoch: 488 [320/891 (35.71428571%)]\tLoss: 0.01323986\n",
      "Train Epoch: 488 [640/891 (71.42857143%)]\tLoss: 0.23935980\n",
      "Train Set: Average loss: 0.06403302, Accuracy: 877/891 (98.42873176%)\n",
      "\n",
      "Test set: Average loss: 0.27161363, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 488 time: 1.1287047863006592 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 489 [0/891 (0.00000000%)]\tLoss: 0.07660896\n",
      "Train Epoch: 489 [320/891 (35.71428571%)]\tLoss: 0.01390737\n",
      "Train Epoch: 489 [640/891 (71.42857143%)]\tLoss: 0.15586984\n",
      "Train Set: Average loss: 0.06745983, Accuracy: 867/891 (97.30639731%)\n",
      "\n",
      "Test set: Average loss: 0.26111433, Accuracy: 95/99 (95.95959596%)\n",
      "\n",
      "Epoch 489 time: 1.038344144821167 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 490 [0/891 (0.00000000%)]\tLoss: 0.07459748\n",
      "Train Epoch: 490 [320/891 (35.71428571%)]\tLoss: 0.45322555\n",
      "Train Epoch: 490 [640/891 (71.42857143%)]\tLoss: 0.09889096\n",
      "Train Set: Average loss: 0.08176156, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.23673006, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 490 time: 1.0141944885253906 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 491 [0/891 (0.00000000%)]\tLoss: 0.03286123\n",
      "Train Epoch: 491 [320/891 (35.71428571%)]\tLoss: 0.23035944\n",
      "Train Epoch: 491 [640/891 (71.42857143%)]\tLoss: 0.01958632\n",
      "Train Set: Average loss: 0.07255227, Accuracy: 867/891 (97.30639731%)\n",
      "\n",
      "Test set: Average loss: 0.25419960, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 491 time: 0.9926998615264893 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 492 [0/891 (0.00000000%)]\tLoss: 0.00210941\n",
      "Train Epoch: 492 [320/891 (35.71428571%)]\tLoss: 0.13506621\n",
      "Train Epoch: 492 [640/891 (71.42857143%)]\tLoss: 0.17944002\n",
      "Train Set: Average loss: 0.06515898, Accuracy: 870/891 (97.64309764%)\n",
      "\n",
      "Test set: Average loss: 0.27377204, Accuracy: 95/99 (95.95959596%)\n",
      "\n",
      "Epoch 492 time: 1.0585806369781494 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 493 [0/891 (0.00000000%)]\tLoss: 0.00785625\n",
      "Train Epoch: 493 [320/891 (35.71428571%)]\tLoss: 0.01319551\n",
      "Train Epoch: 493 [640/891 (71.42857143%)]\tLoss: 0.00298673\n",
      "Train Set: Average loss: 0.07462855, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.29250399, Accuracy: 94/99 (94.94949495%)\n",
      "\n",
      "Epoch 493 time: 1.0190660953521729 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 494 [0/891 (0.00000000%)]\tLoss: 0.01111084\n",
      "Train Epoch: 494 [320/891 (35.71428571%)]\tLoss: 0.01426452\n",
      "Train Epoch: 494 [640/891 (71.42857143%)]\tLoss: 0.02714461\n",
      "Train Set: Average loss: 0.06488805, Accuracy: 873/891 (97.97979798%)\n",
      "\n",
      "Test set: Average loss: 0.33826868, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 494 time: 1.030012607574463 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 495 [0/891 (0.00000000%)]\tLoss: 0.00245577\n",
      "Train Epoch: 495 [320/891 (35.71428571%)]\tLoss: 0.17543972\n",
      "Train Epoch: 495 [640/891 (71.42857143%)]\tLoss: 0.17934066\n",
      "Train Set: Average loss: 0.12355138, Accuracy: 861/891 (96.63299663%)\n",
      "\n",
      "Test set: Average loss: 0.40262421, Accuracy: 91/99 (91.91919192%)\n",
      "\n",
      "Epoch 495 time: 1.0044543743133545 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 496 [0/891 (0.00000000%)]\tLoss: 0.12150770\n",
      "Train Epoch: 496 [320/891 (35.71428571%)]\tLoss: 0.02792293\n",
      "Train Epoch: 496 [640/891 (71.42857143%)]\tLoss: 0.47231913\n",
      "Train Set: Average loss: 0.07089705, Accuracy: 871/891 (97.75533109%)\n",
      "\n",
      "Test set: Average loss: 0.33891539, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 496 time: 1.0247595310211182 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 497 [0/891 (0.00000000%)]\tLoss: 0.27949703\n",
      "Train Epoch: 497 [320/891 (35.71428571%)]\tLoss: 0.29345161\n",
      "Train Epoch: 497 [640/891 (71.42857143%)]\tLoss: 0.03213584\n",
      "Train Set: Average loss: 0.07918062, Accuracy: 865/891 (97.08193042%)\n",
      "\n",
      "Test set: Average loss: 0.33663276, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 497 time: 0.9726560115814209 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 498 [0/891 (0.00000000%)]\tLoss: 0.07765925\n",
      "Train Epoch: 498 [320/891 (35.71428571%)]\tLoss: 0.00743461\n",
      "Train Epoch: 498 [640/891 (71.42857143%)]\tLoss: 0.13636839\n",
      "Train Set: Average loss: 0.05485489, Accuracy: 871/891 (97.75533109%)\n",
      "\n",
      "Test set: Average loss: 0.28180120, Accuracy: 92/99 (92.92929293%)\n",
      "\n",
      "Epoch 498 time: 1.0433933734893799 seconds\n",
      "\n",
      "\n",
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 499 [0/891 (0.00000000%)]\tLoss: 0.02111250\n",
      "Train Epoch: 499 [320/891 (35.71428571%)]\tLoss: 0.30579704\n",
      "Train Epoch: 499 [640/891 (71.42857143%)]\tLoss: 0.13634545\n",
      "Train Set: Average loss: 0.07713768, Accuracy: 866/891 (97.19416386%)\n",
      "\n",
      "Test set: Average loss: 0.29710361, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 499 time: 1.015697956085205 seconds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.00421875\n",
      "Train Epoch: 500 [0/891 (0.00000000%)]\tLoss: 0.10804039\n",
      "Train Epoch: 500 [320/891 (35.71428571%)]\tLoss: 0.04200649\n",
      "Train Epoch: 500 [640/891 (71.42857143%)]\tLoss: 0.01683164\n",
      "Train Set: Average loss: 0.07231457, Accuracy: 868/891 (97.41863075%)\n",
      "\n",
      "Test set: Average loss: 0.26329374, Accuracy: 93/99 (93.93939394%)\n",
      "\n",
      "Epoch 500 time: 1.0447742938995361 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "training_history = pd.DataFrame()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    curt = time.time()\n",
    "    torch_utils.clear_cuda()\n",
    "    \n",
    "    print(\"Learning Rate: {}\".format(torch_utils.get_lr(optimizer)))\n",
    "    \n",
    "    train_loss, train_acc = train_classifier(model,optimizer,criterion,1+epoch,train_loader,\n",
    "                                 device=device, print_interval=10)\n",
    "    valid_loss, valid_acc = test_classifier(model, nn.NLLLoss(reduction=\"sum\"), device, valid_loader)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    training_history.loc[epoch, \"train_loss\"] = train_loss\n",
    "    training_history.loc[epoch, \"valid_loss\"] = valid_loss\n",
    "    training_history.loc[epoch, \"train_accuracy\"] = train_acc\n",
    "    training_history.loc[epoch, \"valid_accuracy\"] = valid_acc\n",
    "    print(\"Epoch {} time: {} seconds\\n\\n\".format(epoch+1, time.time()-curt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_history = pd.read_csv(\"./history_450_pytorch.csv\")\n",
    "# model.load_state_dict(torch.load(\"./Trained_450.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f91d44ac710>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_history[\"train_loss\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f91d41823d0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_history[\"valid_loss\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f91d41561d0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_history[\"train_accuracy\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f91d40ed850>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_history[\"valid_accuracy\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0c485ee1e6492e851ccf6c42748d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_pred = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader):\n",
    "        torch_utils.clear_cuda()\n",
    "        data = (data[0].to(device), data[1].to(device))\n",
    "        y_pred = model(data).cpu()\n",
    "        all_pred.append(torch.exp(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(torch.cat(all_pred).numpy(), columns=le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(0, \"id\", raw_test.id)\n",
    "df.to_csv(\"torch_Submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Acer_Capillipes</th>\n",
       "      <th>Acer_Circinatum</th>\n",
       "      <th>Acer_Mono</th>\n",
       "      <th>Acer_Opalus</th>\n",
       "      <th>Acer_Palmatum</th>\n",
       "      <th>Acer_Pictum</th>\n",
       "      <th>Acer_Platanoids</th>\n",
       "      <th>Acer_Rubrum</th>\n",
       "      <th>Acer_Rufinerve</th>\n",
       "      <th>...</th>\n",
       "      <th>Salix_Fragilis</th>\n",
       "      <th>Salix_Intergra</th>\n",
       "      <th>Sorbus_Aria</th>\n",
       "      <th>Tilia_Oliveri</th>\n",
       "      <th>Tilia_Platyphyllos</th>\n",
       "      <th>Tilia_Tomentosa</th>\n",
       "      <th>Ulmus_Bergmanniana</th>\n",
       "      <th>Viburnum_Tinus</th>\n",
       "      <th>Viburnum_x_Rhytidophylloides</th>\n",
       "      <th>Zelkova_Serrata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1.248743e-15</td>\n",
       "      <td>9.810202e-18</td>\n",
       "      <td>4.599580e-18</td>\n",
       "      <td>1.010958e-07</td>\n",
       "      <td>4.172064e-20</td>\n",
       "      <td>1.004897e-18</td>\n",
       "      <td>2.090520e-12</td>\n",
       "      <td>5.928289e-15</td>\n",
       "      <td>3.789376e-18</td>\n",
       "      <td>...</td>\n",
       "      <td>1.149523e-22</td>\n",
       "      <td>1.752809e-12</td>\n",
       "      <td>2.024685e-14</td>\n",
       "      <td>6.078964e-19</td>\n",
       "      <td>2.279168e-15</td>\n",
       "      <td>2.300145e-14</td>\n",
       "      <td>1.363965e-20</td>\n",
       "      <td>2.493423e-18</td>\n",
       "      <td>3.546188e-22</td>\n",
       "      <td>2.119055e-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>3.518905e-19</td>\n",
       "      <td>3.201246e-22</td>\n",
       "      <td>1.858941e-10</td>\n",
       "      <td>7.723427e-11</td>\n",
       "      <td>2.887956e-26</td>\n",
       "      <td>8.376755e-17</td>\n",
       "      <td>3.082452e-14</td>\n",
       "      <td>4.098586e-20</td>\n",
       "      <td>5.426534e-20</td>\n",
       "      <td>...</td>\n",
       "      <td>1.471518e-24</td>\n",
       "      <td>2.557486e-17</td>\n",
       "      <td>1.382564e-16</td>\n",
       "      <td>3.582674e-19</td>\n",
       "      <td>1.217389e-16</td>\n",
       "      <td>2.601203e-16</td>\n",
       "      <td>3.923697e-16</td>\n",
       "      <td>3.652019e-08</td>\n",
       "      <td>1.253739e-10</td>\n",
       "      <td>9.649373e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>1.145346e-18</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.012437e-14</td>\n",
       "      <td>1.022495e-19</td>\n",
       "      <td>5.602308e-08</td>\n",
       "      <td>4.328240e-10</td>\n",
       "      <td>8.720607e-20</td>\n",
       "      <td>1.814246e-20</td>\n",
       "      <td>8.132862e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>2.650399e-19</td>\n",
       "      <td>1.028219e-23</td>\n",
       "      <td>3.179655e-22</td>\n",
       "      <td>3.656958e-21</td>\n",
       "      <td>2.774431e-32</td>\n",
       "      <td>5.015776e-20</td>\n",
       "      <td>4.891713e-13</td>\n",
       "      <td>5.080368e-33</td>\n",
       "      <td>9.073593e-22</td>\n",
       "      <td>1.505692e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>3.132703e-13</td>\n",
       "      <td>6.637863e-07</td>\n",
       "      <td>2.315737e-10</td>\n",
       "      <td>7.062288e-15</td>\n",
       "      <td>5.052179e-15</td>\n",
       "      <td>7.254198e-12</td>\n",
       "      <td>3.359301e-11</td>\n",
       "      <td>8.988323e-12</td>\n",
       "      <td>1.460755e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>6.011214e-19</td>\n",
       "      <td>9.509936e-25</td>\n",
       "      <td>6.994370e-13</td>\n",
       "      <td>2.615978e-14</td>\n",
       "      <td>6.189193e-20</td>\n",
       "      <td>2.133502e-11</td>\n",
       "      <td>2.741445e-01</td>\n",
       "      <td>3.577809e-21</td>\n",
       "      <td>7.954836e-15</td>\n",
       "      <td>1.177624e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>5.819731e-21</td>\n",
       "      <td>1.196014e-20</td>\n",
       "      <td>2.644065e-21</td>\n",
       "      <td>3.896793e-21</td>\n",
       "      <td>7.613541e-26</td>\n",
       "      <td>2.503133e-27</td>\n",
       "      <td>4.728310e-19</td>\n",
       "      <td>3.203161e-24</td>\n",
       "      <td>7.173682e-11</td>\n",
       "      <td>...</td>\n",
       "      <td>3.410424e-16</td>\n",
       "      <td>7.022825e-37</td>\n",
       "      <td>1.053906e-12</td>\n",
       "      <td>1.080892e-21</td>\n",
       "      <td>2.446107e-13</td>\n",
       "      <td>1.045349e-12</td>\n",
       "      <td>2.081169e-06</td>\n",
       "      <td>2.840028e-18</td>\n",
       "      <td>2.549460e-13</td>\n",
       "      <td>3.253340e-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Acer_Capillipes  Acer_Circinatum     Acer_Mono   Acer_Opalus  \\\n",
       "0   4     1.248743e-15     9.810202e-18  4.599580e-18  1.010958e-07   \n",
       "1   7     3.518905e-19     3.201246e-22  1.858941e-10  7.723427e-11   \n",
       "2   9     1.145346e-18     1.000000e+00  1.012437e-14  1.022495e-19   \n",
       "3  12     3.132703e-13     6.637863e-07  2.315737e-10  7.062288e-15   \n",
       "4  13     5.819731e-21     1.196014e-20  2.644065e-21  3.896793e-21   \n",
       "\n",
       "   Acer_Palmatum   Acer_Pictum  Acer_Platanoids   Acer_Rubrum  Acer_Rufinerve  \\\n",
       "0   4.172064e-20  1.004897e-18     2.090520e-12  5.928289e-15    3.789376e-18   \n",
       "1   2.887956e-26  8.376755e-17     3.082452e-14  4.098586e-20    5.426534e-20   \n",
       "2   5.602308e-08  4.328240e-10     8.720607e-20  1.814246e-20    8.132862e-15   \n",
       "3   5.052179e-15  7.254198e-12     3.359301e-11  8.988323e-12    1.460755e-09   \n",
       "4   7.613541e-26  2.503133e-27     4.728310e-19  3.203161e-24    7.173682e-11   \n",
       "\n",
       "   ...  Salix_Fragilis  Salix_Intergra   Sorbus_Aria  Tilia_Oliveri  \\\n",
       "0  ...    1.149523e-22    1.752809e-12  2.024685e-14   6.078964e-19   \n",
       "1  ...    1.471518e-24    2.557486e-17  1.382564e-16   3.582674e-19   \n",
       "2  ...    2.650399e-19    1.028219e-23  3.179655e-22   3.656958e-21   \n",
       "3  ...    6.011214e-19    9.509936e-25  6.994370e-13   2.615978e-14   \n",
       "4  ...    3.410424e-16    7.022825e-37  1.053906e-12   1.080892e-21   \n",
       "\n",
       "   Tilia_Platyphyllos  Tilia_Tomentosa  Ulmus_Bergmanniana  Viburnum_Tinus  \\\n",
       "0        2.279168e-15     2.300145e-14        1.363965e-20    2.493423e-18   \n",
       "1        1.217389e-16     2.601203e-16        3.923697e-16    3.652019e-08   \n",
       "2        2.774431e-32     5.015776e-20        4.891713e-13    5.080368e-33   \n",
       "3        6.189193e-20     2.133502e-11        2.741445e-01    3.577809e-21   \n",
       "4        2.446107e-13     1.045349e-12        2.081169e-06    2.840028e-18   \n",
       "\n",
       "   Viburnum_x_Rhytidophylloides  Zelkova_Serrata  \n",
       "0                  3.546188e-22     2.119055e-20  \n",
       "1                  1.253739e-10     9.649373e-12  \n",
       "2                  9.073593e-22     1.505692e-10  \n",
       "3                  7.954836e-15     1.177624e-05  \n",
       "4                  2.549460e-13     3.253340e-18  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
