{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ALL IMPORTS FOR A NEW NOTEBOOK\n",
    "\n",
    "import os, sys, random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import itertools as it\n",
    "import scipy\n",
    "import glob\n",
    "import matplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.optim import Optimizer\n",
    "import torchvision.transforms.transforms as txf\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn import model_selection as ms\n",
    "\n",
    "import torch_utils\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import time\n",
    "\n",
    "font = {'size'   : 20}\n",
    "\n",
    "matplotlib.rc('font', **font)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "SEED = 947\n",
    "torch_utils.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize=\"spacy\")\n",
    "LABEL = data.LabelField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.46 s, sys: 51.5 ms, total: 1.52 s\n",
      "Wall time: 563 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=True)\n",
    "train_data, valid_data = train_data.split(split_ratio=0.8, random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4362, 1090, 500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['What', 'are', 'Bellworts', '?'], 'label': 'DESC:def'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(\n",
    "    train_data,\n",
    "    max_size=MAX_VOCAB_SIZE,\n",
    "    vectors=\"glove.6B.200d\",\n",
    "    unk_init=torch.Tensor.normal_\n",
    ")\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'HUM:ind': 0,\n",
       "             'LOC:other': 1,\n",
       "             'DESC:def': 2,\n",
       "             'NUM:count': 3,\n",
       "             'DESC:desc': 4,\n",
       "             'DESC:manner': 5,\n",
       "             'ENTY:other': 6,\n",
       "             'NUM:date': 7,\n",
       "             'DESC:reason': 8,\n",
       "             'ENTY:cremat': 9,\n",
       "             'HUM:gr': 10,\n",
       "             'LOC:country': 11,\n",
       "             'LOC:city': 12,\n",
       "             'ENTY:animal': 13,\n",
       "             'ENTY:food': 14,\n",
       "             'ENTY:dismed': 15,\n",
       "             'ENTY:termeq': 16,\n",
       "             'NUM:period': 17,\n",
       "             'ABBR:exp': 18,\n",
       "             'NUM:money': 19,\n",
       "             'LOC:state': 20,\n",
       "             'ENTY:event': 21,\n",
       "             'ENTY:sport': 22,\n",
       "             'NUM:other': 23,\n",
       "             'HUM:desc': 24,\n",
       "             'ENTY:product': 25,\n",
       "             'ENTY:color': 26,\n",
       "             'ENTY:substance': 27,\n",
       "             'ENTY:techmeth': 28,\n",
       "             'NUM:dist': 29,\n",
       "             'ENTY:word': 30,\n",
       "             'ENTY:veh': 31,\n",
       "             'NUM:perc': 32,\n",
       "             'HUM:title': 33,\n",
       "             'LOC:mount': 34,\n",
       "             'ABBR:abb': 35,\n",
       "             'ENTY:body': 36,\n",
       "             'ENTY:lang': 37,\n",
       "             'ENTY:plant': 38,\n",
       "             'NUM:volsize': 39,\n",
       "             'NUM:weight': 40,\n",
       "             'ENTY:letter': 41,\n",
       "             'ENTY:symbol': 42,\n",
       "             'ENTY:instru': 43,\n",
       "             'NUM:speed': 44,\n",
       "             'NUM:code': 45,\n",
       "             'NUM:ord': 46,\n",
       "             'ENTY:currency': 47,\n",
       "             'ENTY:religion': 48,\n",
       "             'NUM:temp': 49})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentimental2DCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super(Sentimental2DCNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, \n",
    "                out_channels=n_filters, \n",
    "                kernel_size=(fs, embed_size)\n",
    "            ) for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # sent_len x batch\n",
    "        text = text.permute(1,0)\n",
    "        # batch x sent_len\n",
    "        embedded = self.embedding(text)\n",
    "        # batch x sent_len x embedding_size\n",
    "        embedded = embedded.unsqueeze(dim=1)\n",
    "        # batch x 1 x sent_len x embedding_size\n",
    "        conved = [F.leaky_relu(conv(embedded)).squeeze(dim=3) for conv in self.convs]\n",
    "        # batch x n_filters x sent_len-filter_size[n]+1\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(dim=2) for conv in conved]\n",
    "        # batch x n_filters\n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "        # batch x n_filters*len(filter_sizes)\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    max_preds = preds.argmax(dim=1, keepdim=True)\n",
    "    correct = max_preds.squeeze(dim=1).eq(y)\n",
    "    return correct.sum()/torch.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    l,a = 0,0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch.text)\n",
    "        loss = criterion(preds, batch.label)\n",
    "        accr = categorical_accuracy(preds, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        l+=loss.item()\n",
    "        a+=accr.item()\n",
    "    return l/(len(iterator)), a/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    l,a = 0,0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            preds = model(batch.text)\n",
    "            loss = criterion(preds, batch.label)\n",
    "            accr = categorical_accuracy(preds, batch.label)\n",
    "            l+=loss.item()\n",
    "            a+=accr.item()\n",
    "    return l/(len(iterator)), a/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.45\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "PRETRAINED_EMBEDDINGS = TEXT.vocab.vectors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sentimental2DCNN(INPUT_DIM,EMBEDDING_DIM,N_FILTERS,FILTER_SIZES,OUTPUT_DIM,DROPOUT,PAD_IDX)\n",
    "model.embedding.weight.data.copy_(PRETRAINED_EMBEDDINGS)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model = model.to(device)\n",
    "torch_utils.clear_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adamax(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EPOCH 1 Completed, Time Taken: 0:00:00.541681\n",
      "\tTrain Loss \t2.64627338\n",
      "\tTrain Accuracy \t36.8973214%\n",
      "\tValid Loss \t2.18850329\n",
      "\tValid Accuracy \t44.1287878%\n",
      "LR: 0.002\n",
      "Found better solution (inf --> 2.188503).  Saving model ...\n",
      "\n",
      "\n",
      "EPOCH 2 Completed, Time Taken: 0:00:00.418259\n",
      "\tTrain Loss \t1.94859985\n",
      "\tTrain Accuracy \t50.1383929%\n",
      "\tValid Loss \t1.96846618\n",
      "\tValid Accuracy \t49.2503156%\n",
      "LR: 0.002\n",
      "Found better solution (2.188503 --> 1.968466).  Saving model ...\n",
      "\n",
      "\n",
      "EPOCH 3 Completed, Time Taken: 0:00:00.422086\n",
      "\tTrain Loss \t1.64698013\n",
      "\tTrain Accuracy \t59.1607143%\n",
      "\tValid Loss \t1.80062058\n",
      "\tValid Accuracy \t55.2346382%\n",
      "LR: 0.002\n",
      "Found better solution (1.968466 --> 1.800621).  Saving model ...\n",
      "\n",
      "\n",
      "EPOCH 4 Completed, Time Taken: 0:00:00.418046\n",
      "\tTrain Loss \t1.41229984\n",
      "\tTrain Accuracy \t66.7767857%\n",
      "\tValid Loss \t1.6665246\n",
      "\tValid Accuracy \t57.6704545%\n",
      "LR: 0.002\n",
      "Found better solution (1.800621 --> 1.666525).  Saving model ...\n",
      "\n",
      "\n",
      "EPOCH 5 Completed, Time Taken: 0:00:00.385416\n",
      "\tTrain Loss \t1.23387858\n",
      "\tTrain Accuracy \t72.4598214%\n",
      "\tValid Loss \t1.55915194\n",
      "\tValid Accuracy \t58.7068604%\n",
      "LR: 0.002\n",
      "Found better solution (1.666525 --> 1.559152).  Saving model ...\n",
      "\n",
      "\n",
      "EPOCH 6 Completed, Time Taken: 0:00:00.411885\n",
      "\tTrain Loss \t1.04304922\n",
      "\tTrain Accuracy \t76.8705358%\n",
      "\tValid Loss \t1.46818252\n",
      "\tValid Accuracy \t62.7814604%\n",
      "LR: 0.002\n",
      "Found better solution (1.559152 --> 1.468183).  Saving model ...\n",
      "\n",
      "\n",
      "EPOCH 7 Completed, Time Taken: 0:00:00.407055\n",
      "\tTrain Loss \t0.886082246\n",
      "\tTrain Accuracy \t82.3035714%\n",
      "\tValid Loss \t1.36780292\n",
      "\tValid Accuracy \t64.3439604%\n",
      "LR: 0.002\n",
      "Found better solution (1.468183 --> 1.367803).  Saving model ...\n",
      "\n",
      "\n",
      "EPOCH 8 Completed, Time Taken: 0:00:00.443686\n",
      "\tTrain Loss \t0.765261211\n",
      "\tTrain Accuracy \t84.6964286%\n",
      "\tValid Loss \t1.3234462\n",
      "\tValid Accuracy \t66.1510944%\n",
      "LR: 0.002\n",
      "Found better solution (1.367803 --> 1.323446).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 200\n",
    "ea = torch_utils.EarlyStopping(verbose=True, patience=25)\n",
    "\n",
    "sch = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.5, patience=3)\n",
    "history = pd.DataFrame()\n",
    "\n",
    "for e in range(N_EPOCHS):\n",
    "    st = time.time()\n",
    "    tl, ta = train(model, train_iterator, optimizer, criterion)\n",
    "    vl, va = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    history = torch_utils.print_epoch_stat(e, time.time()-st, history, tl, ta, vl, va)\n",
    "    print(\"LR: {}\".format(torch_utils.get_lr(optimizer)))\n",
    "    ea(vl, model)\n",
    "    sch.step(vl)\n",
    "    if ea.early_stop:\n",
    "        print(\"STOPPING EARLY!!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, a = evaluate(model, test_iterator, criterion)\n",
    "print(l, 100.0*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"checkpoint.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, a = evaluate(model, test_iterator, criterion)\n",
    "print(l, 100.0*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = history[\"train_loss\"].plot()\n",
    "history[\"valid_loss\"].plot(ax=ax)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = history[\"train_accuracy\"].plot()\n",
    "history[\"valid_accuracy\"].plot(ax=ax)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
